#' Generic Keras Sequential API Model Fitting Implementation
#'
#' @description
#' This function is the internal engine for fitting models generated by
#' `create_keras_sequential_spec()`. It is not intended to be called directly
#' by the user.
#'
#' @details
#' This function performs the following key steps:
#' \enumerate{
#'   \item \strong{Argument & Data Preparation:} It resolves arguments passed
#'     from `parsnip` (handling `rlang_zap` objects for unspecified arguments)
#'     and prepares the `x` and `y` data for Keras. It automatically determines
#'     the `input_shape` from `x` and, for classification, the `num_classes`
#'     from `y`.
#'   \item \strong{Dynamic Model Construction:} It builds the Keras model by
#'     sequentially processing the `layer_blocks` list.
#'     \itemize{
#'       \item The first block function \strong{must initialize the model}, typically
#'         by calling `keras3::keras_model_sequential()`.
#'       \item It checks for `num_{block_name}` arguments to repeat a block
#'         multiple times, creating a deeper stack of layers.
#'     }
#'   \item \strong{Model Compilation:} It compiles the final Keras model. The
#'     compilation arguments (optimizer, loss, metrics) can be customized by
#'     passing arguments prefixed with `compile_` (e.g., `compile_loss = "mae"`).
#'   \item \strong{Model Fitting:} It calls `keras3::fit()` to train the model on
#'     the prepared data.
#' }
#'
#' @param x A data frame or matrix of predictors.
#' @param y A vector of outcomes.
#' @param layer_blocks A named list of layer block functions. This is passed
#'   internally from the `parsnip` model specification.
#' @param epochs An integer for the number of training iterations.
#' @param learn_rate A double for the learning rate, used to configure the
#'   default Adam optimizer.
#' @param batch_size An integer for the number of samples per gradient update.
#'   This is a tunable parameter and is passed to `keras3::fit()`.
#' @param validation_split The proportion of the training data to use for
#'   the validation set.
#' @param verbose An integer for the verbosity of the fitting process (0, 1, or 2).
#' @param ... Additional arguments passed down from the model specification. These
#'   can include:
#'   \itemize{
#'     \item \strong{Layer Parameters:} Arguments for the layer blocks, prefixed
#'       with the block name (e.g., `dense_units = 64`).
#'     \item \strong{Architecture Parameters:} Arguments to control the number of
#'       times a block is repeated, in the format `num_{block_name}` (e.g.,
#'       `num_dense = 2`).
#'     \item \strong{Compile Parameters:} Arguments to customize model compilation,
#'       prefixed with `compile_` (e.g., `compile_loss = "mae"`,
#'       `compile_optimizer = "sgd"`).
#'     \item \strong{Fit Parameters:} Arguments to customize model fitting,
#'       prefixed with `fit_` (e.g., `fit_callbacks = list(...)`,
#'       `fit_class_weight = list(...)`).
#'   }
#'
#' @return A list containing the fitted model and other metadata. This list is
#'   stored in the `fit` slot of the `parsnip` model fit object. The list
#'   contains the following elements:
#'   \itemize{
#'     \item `fit`: The raw, fitted Keras model object.
#'     \item `history`: The Keras training history object.
#'     \item `lvl`: A character vector of the outcome factor levels (for
#'       classification) or `NULL` (for regression).
#'   }
#' @keywords internal
#' @export
generic_sequential_fit <- function(
  x,
  y,
  layer_blocks,
  epochs = 10,
  batch_size = 32,
  learn_rate = 0.01,
  validation_split = 0.2,
  verbose = 0,
  ...
) {
  # --- 0. Resolve arguments ---
  # Parsnip passes "zapped" arguments for user-unspecified args.
  # This helper replaces them with the function's defaults.
  resolve_default <- function(x, default) {
    if (inherits(x, "rlang_zap")) default else x
  }
  fmls <- rlang::fn_fmls(sys.function())
  epochs <- resolve_default(epochs, fmls$epochs)
  batch_size <- resolve_default(batch_size, fmls$batch_size)
  learn_rate <- resolve_default(learn_rate, fmls$learn_rate)
  validation_split <- resolve_default(validation_split, fmls$validation_split)
  verbose <- resolve_default(verbose, fmls$verbose)

  # --- 1. Data & Input Shape Preparation ---
  all_args <- list(...)
  # Handle both standard tabular data (matrix) and list-columns of arrays
  # (for images/sequences) that come from recipes.
  if (is.data.frame(x) && ncol(x) == 1 && is.list(x[[1]])) {
    # Assumes a single predictor column containing a list of arrays.
    # We stack them into a single higher-dimensional array.
    x_proc <- do.call(abind::abind, c(x[[1]], list(along = 0)))
  } else {
    x_proc <- as.matrix(x)
  }

  # Determine the correct input shape for the Keras model.
  input_shape <- if (length(dim(x_proc)) > 2) dim(x_proc)[-1] else ncol(x_proc)

  # Determine default compile arguments based on mode
  is_classification <- is.factor(y)
  if (is_classification) {
    class_levels <- levels(y)
    num_classes <- length(class_levels)
    y_mat <- keras3::to_categorical(
      as.numeric(y) - 1,
      num_classes = num_classes
    )
    default_loss <- if (num_classes > 2) {
      "categorical_crossentropy"
    } else {
      "binary_crossentropy"
    }
    default_metrics <- "accuracy"
  } else {
    class_levels <- NULL
    y_mat <- as.matrix(y)
    default_loss <- "mean_squared_error"
    default_metrics <- "mean_absolute_error"
  }

  # --- 2. Dynamic Model Architecture Construction ---
  # The model is initialized as NULL. The first layer_block is expected to
  # create the model (e.g., by defining an input layer). Subsequent blocks
  # will receive and modify the model object. The order is critical.
  model <- NULL

  for (block_name in names(layer_blocks)) {
    block_fn <- layer_blocks[[block_name]]
    block_fmls <- rlang::fn_fmls(block_fn)

    num_repeats_arg <- paste0("num_", block_name)
    num_repeats_val <- all_args[[num_repeats_arg]]
    num_repeats <- num_repeats_val %||% 1

    # Get the arguments for this specific block from `...`
    block_arg_names <- names(block_fmls)[-1] # Exclude 'model'
    user_args <- list()
    for (arg_name in block_arg_names) {
      full_arg_name <- paste(block_name, arg_name, sep = "_")
      arg_val <- all_args[[full_arg_name]]
      # Only use the argument if it was actually provided by the user
      if (!is.null(arg_val) && !inherits(arg_val, "rlang_zap")) {
        user_args[[arg_name]] <- arg_val
      }
    }

    # Combine user-provided args with the block's defaults
    block_args <- utils::modifyList(as.list(block_fmls[-1]), user_args)

    # If the block function can accept these, provide them. This is useful for
    # the user-defined input and output layers.
    if ("input_shape" %in% names(block_fmls)) {
      block_args$input_shape <- input_shape
    }
    if (is_classification && "num_classes" %in% names(block_fmls)) {
      block_args$num_classes <- num_classes
    }

    # Add the block(s) to the model
    for (i in seq_len(num_repeats)) {
      # The first argument to the block function is the model itself
      # On the first iteration, `model` will be NULL.
      call_args <- c(list(model), block_args)
      model <- rlang::exec(block_fn, !!!call_args)
    }
  }

  # --- 3. Model Compilation ---
  # Collect all arguments starting with "compile_" from `...`
  compile_arg_names <- names(all_args)[startsWith(names(all_args), "compile_")]
  user_compile_args <- all_args[compile_arg_names]
  names(user_compile_args) <- sub("^compile_", "", names(user_compile_args))

  # --- 3a. Resolve and Finalize Compile Arguments ---
  final_compile_args <- list()

  # Determine the final optimizer object, ensuring `learn_rate` is applied.
  optimizer_arg <- resolve_default(user_compile_args$optimizer, NULL)
  if (!is.null(optimizer_arg)) {
    if (is.character(optimizer_arg)) {
      # Resolve string to object, passing the learn_rate
      final_compile_args$optimizer <- get_keras_object(
        optimizer_arg,
        "optimizer",
        learning_rate = learn_rate
      )
    } else {
      # User passed a pre-constructed optimizer object, use it as is.
      # We assume they have configured the learning rate within it.
      final_compile_args$optimizer <- optimizer_arg
    }
  } else {
    # No optimizer provided, use the default (Adam) with the learn_rate.
    final_compile_args$optimizer <- keras3::optimizer_adam(
      learning_rate = learn_rate
    )
  }

  # Resolve loss: use user-provided, otherwise default. Resolve string if needed.
  loss_arg <- resolve_default(user_compile_args$loss, default_loss)
  if (is.character(loss_arg)) {
    final_compile_args$loss <- get_keras_object(loss_arg, "loss")
  } else {
    final_compile_args$loss <- loss_arg
  }

  # Resolve metrics: userâ€supplied or default
  metrics_arg <- resolve_default(user_compile_args$metrics, default_metrics)
  # Keras' `compile()` can handle a single string or a list/vector of strings.
  # This correctly passes along either the default string or a user-provided vector.
  final_compile_args$metrics <- metrics_arg

  # Add any other user-provided compile arguments (e.g., `weighted_metrics`)
  other_args <- user_compile_args[
    !names(user_compile_args) %in% c("optimizer", "loss", "metrics")
  ]
  final_compile_args <- c(final_compile_args, other_args)

  # --- 3b. Compile the Model ---
  rlang::exec(keras3::compile, model, !!!final_compile_args)

  # --- 4. Model Fitting ---
  # Collect all arguments starting with "fit_" from `...`
  fit_arg_names <- names(all_args)[startsWith(names(all_args), "fit_")]
  user_fit_args <- all_args[fit_arg_names]
  names(user_fit_args) <- sub("^fit_", "", names(user_fit_args))

  # Combine with core fitting arguments
  final_fit_args <- c(
    list(
      x = x_proc,
      y = y_mat,
      epochs = epochs,
      batch_size = batch_size,
      validation_split = validation_split,
      verbose = verbose
    ),
    user_fit_args
  )

  # Fit the model using the constructed arguments
  history <- rlang::exec(keras3::fit, model, !!!final_fit_args)

  # --- 5. Return value ---
  # Per parsnip extension guidelines, the fit function should return a list
  # containing the raw model object in an element named `fit`. For
  # classification, it should also include an element `lvl` with the factor levels.
  list(
    fit = model, # The raw Keras model object
    lvl = class_levels # Factor levels for classification, NULL for regression
  )
}
