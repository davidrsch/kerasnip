---
title: "Building Functional Models with kerasnip"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Building Functional Models with kerasnip}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette demonstrates how to use the `create_keras_functional_spec()` function to build complex, non-linear Keras models that integrate seamlessly with the `tidymodels` ecosystem.

## When to Use the Functional API

While `create_keras_sequential_spec()` is perfect for models that are a simple, linear stack of layers, many advanced architectures are not linear. The Keras Functional API is designed for these cases. You should use `create_keras_functional_spec()` when your model has:

*   Multiple input or multiple output layers.
*   Shared layers between different branches.
*   Residual connections (e.g., ResNets), where a layer's input is added to its output.
*   Any other non-linear topology.

`kerasnip` makes it easy to define these architectures by automatically connecting a graph of layer blocks.

## The Core Concept: Building a Graph

`kerasnip` builds the model's graph by inspecting the `layer_blocks` you provide. The connection logic is simple but powerful:

1.  The **names of the list elements** in `layer_blocks` define the names of the nodes in your graph (e.g., `main_input`, `dense_path`, `output`).
2.  The **names of the arguments** in each block function specify its inputs. A block function like `my_block <- function(input_a, input_b, ...)` declares that it needs input from the nodes named `input_a` and `input_b`.

There are two special requirements:

*   **Input Block**: The first block in the list is treated as the main input node. Its function should not take other blocks as input.
*   **Output Block**: Exactly one block must be named `"output"`. The tensor returned by this block is used as the final output of the Keras model.

Let's see this in action.

## Example 1: A Fork-Join Regression Model

We will build a model that forks the input, passes it through two separate dense layer paths, and then joins the results with a concatenation layer before producing a final prediction.

### Step 1: Load Libraries

First, we load the necessary packages.

```{r setup}
library(kerasnip)
library(tidymodels)
library(keras3)

# Silence the startup messages from remove_keras_spec
options(kerasnip.show_removal_messages = FALSE)
```

### Step 2: Define Layer Blocks

These are the building blocks of our model. Each function represents a node in the graph.

```{r define-blocks-functional}
# The input node. `input_shape` is supplied automatically by the engine.
input_block <- function(input_shape) {
  layer_input(shape = input_shape)
}

# A generic block for a dense path. `units` will be a tunable parameter.
path_block <- function(tensor, units = 16) {
  tensor |> layer_dense(units = units, activation = "relu")
}

# A block to join two tensors.
concat_block <- function(input_a, input_b) {
  layer_concatenate(list(input_a, input_b))
}

# The final output block for regression.
output_block_reg <- function(tensor) {
  layer_dense(tensor, units = 1)
}
```

### Step 3: Create the Model Specification

Now we assemble the blocks into a graph. We use the `inp_spec()` helper to connect the blocks. This avoids writing verbose anonymous functions like `function(main_input, units) path_block(main_input, units)`. `inp_spec()` automatically creates a wrapper that renames the arguments of our blocks to match the node names from the `layer_blocks` list.

```{r create-spec-functional}
model_name <- "forked_reg_spec"
# Clean up the spec when the vignette is done knitting
on.exit(remove_keras_spec(model_name), add = TRUE)

create_keras_functional_spec(
  model_name = model_name,
  layer_blocks = list(
    # Node names are defined by the list names
    main_input = input_block,

    # `inp_spec()` renames the first argument of `path_block` ('tensor')
    # to 'main_input' to match the node name.
    path_a = inp_spec(path_block, "main_input"),
    path_b = inp_spec(path_block, "main_input"),

    # For multiple inputs, `inp_spec()` takes a named vector to map
    # new argument names to the original block's argument names.
    concatenated = inp_spec(concat_block, c(path_a = "input_a", path_b = "input_b")),

    # The output block takes the concatenated tensor as its input.
    output = inp_spec(output_block_reg, "concatenated")
  ),
  mode = "regression"
)
```

### Step 4: Use and Fit the Model

The new function `forked_reg_spec()` is now available. Its arguments (`path_a_units`, `path_b_units`) were discovered automatically from our block definitions.

```{r fit-functional}
# We can override the default `units` from `path_block` for each path.
spec <- forked_reg_spec(
  path_a_units = 16,
  path_b_units = 8,
  fit_epochs = 10,
  fit_verbose = 0 # Suppress fitting output in vignette
) |>
  set_engine("keras")

print(spec)

# Fit the model on the mtcars dataset
rec <- recipe(mpg ~ ., data = mtcars)
wf <- workflow() |> 
  add_recipe(rec) |>
  add_model(spec)


fit_obj <- fit(wf, data = mtcars)

predict(fit_obj, new_data = mtcars[1:5, ])
```

## Example 2: Tuning a Functional Model's Depth

A key feature of `kerasnip` is the ability to tune the *depth* of the network by repeating a block multiple times. A block can be repeated if it has **exactly one input tensor** from another block in the graph.

Let's create a simple functional model and tune both its width (`units`) and its depth (`num_...`).

### Step 1: Define Blocks and Create Spec

This model is architecturally sequential, but we build it with the functional API to demonstrate the repetition feature.

```{r create-tunable-functional-spec}
dense_block <- function(tensor, units = 16) {
  tensor |> layer_dense(units = units, activation = "relu")
}
output_block_class <- function(tensor, num_classes) {
  tensor |> layer_dense(units = num_classes, activation = "softmax")
}

model_name_tune <- "tunable_func_mlp"
on.exit(remove_keras_spec(model_name_tune), add = TRUE)

create_keras_functional_spec(
  model_name = model_name_tune,
  layer_blocks = list(
    main_input = input_block,
    # This block has a single input ('main_input'), so it can be repeated.
    dense_path = inp_spec(dense_block, "main_input"),
    output = inp_spec(output_block_class, "dense_path")
  ),
  mode = "classification"
)
```

### Step 2: Set up and Run Tuning

We will tune `dense_path_units` (the width) and `num_dense_path` (the depth). The `num_dense_path` argument was created automatically because `dense_path` is a repeatable block.

```{r tune-functional, cache=TRUE}
tune_spec <- tunable_func_mlp(
  dense_path_units = tune(),
  num_dense_path = tune(),
  fit_epochs = 5,
  fit_verbose = 0
) |>
  set_engine("keras")

rec <- recipe(Species ~ ., data = iris)
tune_wf <- workflow() |> 
  add_recipe(rec) |>
  add_model(tune_spec)

folds <- vfold_cv(iris, v = 2)

# Define the tuning grid
params <- extract_parameter_set_dials(tune_wf) |>
  update(
    dense_path_units = hidden_units(c(8, 32)),
    num_dense_path = num_terms(c(1, 3)) # Test models with 1, 2, or 3 hidden layers
  )

grid <- grid_regular(params, levels = 2)
grid

control <- control_grid(save_pred = FALSE, verbose = FALSE)

tune_res <- tune_grid(
  tune_wf,
  resamples = folds,
  grid = grid,
  control = control
)

show_best(tune_res, metric = "accuracy")
```

The results show that `tidymodels` successfully trained and evaluated models with different numbers of hidden layers, demonstrating that we can tune the very architecture of the network.

## Conclusion

The `create_keras_functional_spec()` function provides a powerful and intuitive way to define, fit, and tune complex Keras models within the `tidymodels` framework. By defining the model as a graph of connected blocks, you can represent nearly any architecture while `kerasnip` handles the boilerplate of integrating it with `parsnip`, `dials`, and `tune`.