---
title: "Getting Started with kerasnip"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with kerasnip}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## The Core Idea: From Keras Layers to Tidymodels Specs

The `keras` package allows for building deep learning models layer-by-layer, which is a powerful and flexible approach. However, the `tidymodels` ecosystem is designed around declarative model specifications, where you define what model you want and which of its parameters you want to tune, rather than building it imperatively.

`kerasnip` bridges this gap with a simple but powerful concept: layer blocks. You define the components of your neural network (e.g., an input block, a dense block, a dropout block) as simple R functions. `kerasnip` then uses these blocks as building materials to create a brand new parsnip model specification function for you.

This new function behaves just like any other parsnip model (e.g., `rand_forest()` or `linear_reg()`), making it easy to integrate into workflows and tune with tune.

Weâ€™ll start by loading kerasnip:

```{r load-kerasnip}
library(dplyr)
```

## Example 1: Building and Fitting a Basic MLP

Let's start by building a simple Multi-Layer Perceptron (MLP) for a regression task using the `mtcars` dataset.

### Step 1: Define the Layer Blocks

We need three blocks:

-   1\. An input block to initialize the model and define the input shape. `kerasnip` will automatically pass the input_shape argument during fitting.

-   2\. A dense block for our hidden layers. We'll give it a units argument so we can control the number of neurons.

-   3\. An output block for the final prediction. For regression, this is typically a single neuron with a linear activation.

    ```{r define-blocks}
    # 1. The input block must initialize the model. 
    # input_shape is passed automatically by the fit engine. 
    mlp_input_block <- function(model, input_shape) {
      keras_model_sequential(input_shape = input_shape) 
    }

    # 2. A block for hidden layers. units will become a tunable parameter. 
    mlp_dense_block <- function(model, units = 32) {
      model |>
        layer_dense(units = units, activation = "relu") 
    }

    # 3. The output block for a regression model. 
    mlp_output_block <- function(model) {
      model |>
        layer_dense(units = 1) 
    } 
    ```

### Step 2: Create the Model Specification

Now, we use `create_keras_spec()` to generate a new model function, which we'll call `basic_mlp()`. We provide our layer blocks in the order they should be assembled.

```{r create-spec}
create_keras_spec(
  model_name = "basic_mlp",
  layer_blocks = list(
    input = mlp_input_block,
    dense = mlp_dense_block,
    output = mlp_output_block
  ),
  mode = "regression" 
) 
```

This function call has a side-effect: a new function `basic_mlp()` is now available in our environment! Notice its arguments: `kerasnip`automatically created `num_dense` (to control the number of dense layers) and `dense_units` (from the units argument in our `mlp_dense_block`).

### Step 3: Use the Spec in a Workflow

We can now use `basic_mlp()` like any other parsnip model. Let's define a model with two hidden layers, each with 64 units, and train it for 50 epochs.

```{r use-spec}
spec <- basic_mlp(
  num_dense = 2,
  dense_units = 64,
  epochs = 50,
  learn_rate = 0.01 
) |>
  set_engine("keras")

print(spec) 
```

We'll use a simple recipe to normalize the predictors and combine it with our model spec in a workflow.

```{r fit-model}
# Suppress verbose Keras output for the vignette 
options(keras.fit_verbose = 0) 
 
rec <- recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_numeric_predictors())

wf <- workflow() |>
  add_recipe(rec) |>
  add_model(spec)

set.seed(123) 
fit_obj <- fit(wf, data = mtcars) 
```

### Step 4: Make Predictions

Predictions work just as you'd expect in `tidymodels`.

```{r predict}
predictions <- predict(fit_obj, new_data = mtcars[1:5, ]) 
print(predictions)
```

## Example 2: Tuning the Model Architecture

The real power of `kerasnip` comes from its ability to tune not just *hyperparameters* (like learning rate or dropout), but the *architecture* of the network itself.

Let's create a more complex *tunable* specification where we let `tune` find the optimal number of dense layers, the number of units in those layers, and the rate for a final dropout layer.

### Step 1: Define Blocks and Create a New Spec

First, we'll define an additional block for dropout and then create a new model specification, `tunable_mlp`, that includes it.

```{r define-tunable-blocks}
tunable_dropout_block <- function(model, rate = 0.2) {
  model |>
    layer_dropout(rate = rate)
}

create_keras_spec(
  model_name = "tunable_mlp",
  layer_blocks = list(
    input = mlp_input_block,
    dense = mlp_dense_block,
    dropout = tunable_dropout_block,
    output = mlp_output_block
  ),
  mode = "regression"
)
```

### Step 2: Define a Tunable Specification

We use our new `tunable_mlp()` function, passing `tune()` to the arguments we want to optimize. We will have one dropout layer before the output.

```{r tune-spec}
tune_spec <- tunable_mlp(
  num_dense = tune(),
  dense_units = tune(),
  num_dropout = 1,
  dropout_rate = tune(),
  epochs = 20 # Use fewer epochs for faster tuning 
) |>
  set_engine("keras")

print(tune_spec)
```

### Step 3: Set up the Tuning Grid

We create a `workflow` as before. Then, we can use helper functions from `dials` to define the search space for our parameters.

```{r setup-tuning}
tune_wf <- workflow() |>
  add_recipe(rec) |>
  add_model(tune_spec)

# Define the tuning grid. 
# `num_terms()` is the dials function for `num_*` parameters.
# `hidden_units()` is the dials function for `*_units` parameters.
params <- extract_parameter_set_dials(tune_wf) |>
  update(
    num_dense = dials::num_terms(c(1, 3)),
    dense_units = dials::hidden_units(c(8, 64)),
    dropout_rate = dials::dropout(c(0.1, 0.5))
  )
grid <- grid_regular(params, levels = 2) 
print(grid) 
```

### Step 4: Run the Tuning

We use `tune_grid()` with resamples to evaluate each combination of architectural parameters.

```{r run-tuning, cache=TRUE}
set.seed(456) 
folds <- vfold_cv(mtcars, v = 3) 
 
# The control argument is used to prevent saving predictions, which 
# can be large for Keras models. 
tune_res <- tune_grid(
  tune_wf,
  resamples = folds,
  grid = grid,
  control = control_grid(save_pred = FALSE) 
) 
```

### Step 5: Analyze the Results

We can now see which architecture performed best.

```{r show-best}
show_best(tune_res, metric = "rmse")
```

The results show that `tune` has successfully tested different network depths (`num_dense`), widths (`dense_units`), and dropout rates to find the best-performing combination. This demonstrates how `kerasnip` seamlessly integrates complex architectural tuning into the standard `tidymodels` workflow.

## Advanced Customization

`kerasnip` provides a clean API for passing arguments directly to Keras's `compile()` and `fit()` methods.

-   **Compile Arguments**: Pass any argument to `keras3::compile()` by prefixing it with `compile_`. For example, to change the loss function you would use `compile_loss = "mae"`.
-   **Fit Arguments**: Pass any argument to `keras3::fit()` by prefixing it with `fit_`. For example, to set a validation split and add a callback, you would use `fit_validation_split = 0.2` and `fit_callbacks = list(...)`.

Here is an example of using these arguments to specify a different loss function, a validation split, and an early stopping callback.

```{r advanced-customization}

adv_spec <- basic_mlp(
  num_dense = 2,
  dense_units = 32,
  epochs = 100,
  # Arguments for keras3::compile()
  compile_loss = "mae",
  # Arguments for keras3::fit()
  fit_validation_split = 0.2,
  fit_callbacks = list(
    keras3::callback_early_stopping(patience = 5)
  )
) |>
  set_engine("keras")

print(adv_spec)
```

This system gives you full control over the Keras training process while keeping the model specification function signature clean and focused on the *tunable* parameters.