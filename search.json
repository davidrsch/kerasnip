[{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement daviddrsch@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kerasnip","title":"Contributing to kerasnip","text":"outlines propose change kerasnip. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kerasnip","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to kerasnip","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to kerasnip","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use air package apply styles. can format code automatically commenting /style PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kerasnip","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kerasnip authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://davidrsch.github.io/kerasnip/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with kerasnip","title":"Getting help with kerasnip","text":"Thanks using kerasnip. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"when-to-use-the-functional-api","dir":"Articles","previous_headings":"","what":"When to Use the Functional API","title":"Building Functional Models with kerasnip","text":"create_keras_sequential_spec() perfect models simple, linear stack layers, many advanced architectures linear. Keras Functional API designed cases. use create_keras_functional_spec() model : Multiple input multiple output layers. Shared layers different branches. Residual connections (e.g., ResNets), layer’s input added output. non-linear topology. kerasnip makes easy define architectures automatically connecting graph layer blocks.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"the-core-concept-building-a-graph","dir":"Articles","previous_headings":"","what":"The Core Concept: Building a Graph","title":"Building Functional Models with kerasnip","text":"kerasnip builds model’s graph inspecting layer_blocks provide. connection logic simple powerful: names list elements layer_blocks define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. two special requirements: Input Block: first block list treated main input node. function take blocks input. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. Let’s see action.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"example-1-a-two-input-regression-model","dir":"Articles","previous_headings":"","what":"Example 1: A Two-Input Regression Model","title":"Building Functional Models with kerasnip","text":"model take two distinct inputs, process separately, concatenate outputs final regression layer. clearly demonstrates functional API’s ability handle multiple inputs, possible sequential API.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-1-load-libraries","dir":"Articles","previous_headings":"Example 1: A Two-Input Regression Model","what":"Step 1: Load Libraries","title":"Building Functional Models with kerasnip","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom        1.0.9     ✔ recipes      1.3.1 ## ✔ dials        1.4.1     ✔ rsample      1.3.1 ## ✔ dplyr        1.1.4     ✔ tibble       3.3.0 ## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 ## ✔ infer        1.0.9     ✔ tune         1.3.0 ## ✔ modeldata    1.5.0     ✔ workflows    1.2.0 ## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 ## ✔ purrr        1.1.0     ✔ yardstick    1.3.2 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter()  masks stats::filter() ## ✖ dplyr::lag()     masks stats::lag() ## ✖ recipes::step()  masks stats::step() library(keras3) ##  ## Attaching package: 'keras3' ## The following object is masked from 'package:yardstick': ##  ##     get_weights # Silence the startup messages from remove_keras_spec options(kerasnip.show_removal_messages = FALSE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-2-define-layer-blocks","dir":"Articles","previous_headings":"Example 1: A Two-Input Regression Model","what":"Step 2: Define Layer Blocks","title":"Building Functional Models with kerasnip","text":"building blocks model. function represents node graph.","code":"# Input blocks for two distinct inputs input_block_1 <- function(input_shape) {   layer_input(shape = input_shape, name = \"input_1\") }  input_block_2 <- function(input_shape) {   layer_input(shape = input_shape, name = \"input_2\") }  # Dense paths for each input dense_path_1 <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") }  dense_path_2 <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") }  # A block to join two tensors concat_block <- function(input_a, input_b) {   layer_concatenate(list(input_a, input_b)) }  # The final output block for regression output_block_1 <- function(tensor) {   layer_dense(tensor, units = 1, name = \"output_1\") }  output_block_2 <- function(tensor) {   layer_dense(tensor, units = 1, name = \"output_2\") }"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-3-create-the-model-specification","dir":"Articles","previous_headings":"Example 1: A Two-Input Regression Model","what":"Step 3: Create the Model Specification","title":"Building Functional Models with kerasnip","text":"Now assemble blocks graph. inp_spec() helper simplifies connecting blocks, eliminating need verbose anonymous functions. inp_spec() automatically creates wrapper renames arguments blocks match node names defined layer_blocks list.","code":"model_name <- \"two_output_reg_spec\" # Changed model name # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(model_name), add = TRUE)  create_keras_functional_spec(   model_name = model_name,   layer_blocks = list(     input_1 = input_block_1,     input_2 = input_block_2,     processed_1 = inp_spec(dense_path_1, \"input_1\"),     processed_2 = inp_spec(dense_path_2, \"input_2\"),     concatenated = inp_spec(concat_block, c(processed_1 = \"input_a\", processed_2 = \"input_b\")),     output_1 = inp_spec(output_block_1, \"concatenated\"), # New output block 1     output_2 = inp_spec(output_block_2, \"concatenated\")  # New output block 2   ),   mode = \"regression\" # Still regression, but will have two columns in y )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-4-use-and-fit-the-model","dir":"Articles","previous_headings":"Example 1: A Two-Input Regression Model","what":"Step 4: Use and Fit the Model","title":"Building Functional Models with kerasnip","text":"new function two_input_reg_spec() now available. arguments (processed_1_units, processed_2_units) discovered automatically block definitions.","code":"# We can override the default `units` for each path. spec <- two_output_reg_spec( # Changed spec name   processed_1_units = 16,   processed_2_units = 8,   fit_epochs = 10,   fit_verbose = 0 # Suppress fitting output in vignette ) |>   set_engine(\"keras\")  print(spec) ## two output reg spec Model Specification (regression) ##  ## Main Arguments: ##   num_input_1 = structure(list(), class = \"rlang_zap\") ##   num_input_2 = structure(list(), class = \"rlang_zap\") ##   num_processed_1 = structure(list(), class = \"rlang_zap\") ##   num_processed_2 = structure(list(), class = \"rlang_zap\") ##   num_concatenated = structure(list(), class = \"rlang_zap\") ##   num_output_1 = structure(list(), class = \"rlang_zap\") ##   num_output_2 = structure(list(), class = \"rlang_zap\") ##   processed_1_units = 16 ##   processed_2_units = 8 ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 10 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = 0 ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras # Prepare dummy data with two inputs and two outputs set.seed(123) x_data_1 <- matrix(runif(100 * 5), ncol = 5) x_data_2 <- matrix(runif(100 * 3), ncol = 3) y_data_1 <- runif(100) y_data_2 <- runif(100) # New second output  # For tidymodels, inputs and outputs need to be in a data frame, potentially as lists of matrices train_df <- tibble::tibble(   input_1 = lapply(seq_len(nrow(x_data_1)), function(i) x_data_1[i, , drop = FALSE]),   input_2 = lapply(seq_len(nrow(x_data_2)), function(i) x_data_2[i, , drop = FALSE]),   output_1 = y_data_1, # Named output 1   output_2 = y_data_2  # Named output 2 )  rec <- recipe(output_1 + output_2 ~ input_1 + input_2, data = train_df) # Recipe for two outputs wf <- workflow() |>    add_recipe(rec) |>    add_model(spec)  fit_obj <- fit(wf, data = train_df)  # Predict on new data new_data_df <- tibble::tibble(   input_1 = lapply(seq_len(5), function(i) matrix(runif(5), ncol = 5)),   input_2 = lapply(seq_len(5), function(i) matrix(runif(3), ncol = 3)) ) predict(fit_obj, new_data = new_data_df) ## 1/1 - 0s - 46ms/step ## # A tibble: 5 × 2 ##   .pred_output_1 .pred_output_2 ##      <dbl[,1,1]>    <dbl[,1,1]> ## 1        0.761 …        0.693 … ## 2        0.488 …        0.560 … ## 3        0.403 …        0.489 … ## 4        0.480 …        0.593 … ## 5        0.612 …        0.681 …"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"a-common-debugging-workflow-compile_keras_grid","dir":"Articles","previous_headings":"","what":"A common debugging workflow: compile_keras_grid()","title":"Building Functional Models with kerasnip","text":"original Keras guide, common workflow incrementally add layers call summary() inspect architecture. kerasnip, model defined declaratively, can’t inspect layer--layer way. However, kerasnip provides powerful equivalent: compile_keras_grid(). function checks layer_blocks define valid Keras model returns compiled model structure, without running full training cycle. perfect debugging architecture. Let’s see action two_input_reg_spec model:","code":"# Create a spec instance spec <- two_output_reg_spec( # Changed spec name   processed_1_units = 16,   processed_2_units = 8 )  # Prepare dummy data with two inputs and two outputs x_dummy_1 <- matrix(runif(10 * 5), ncol = 5) x_dummy_2 <- matrix(runif(10 * 3), ncol = 3) y_dummy_1 <- runif(10) y_dummy_2 <- runif(10) # New second output  # For tidymodels, inputs and outputs need to be in a data frame, potentially as lists of matrices x_dummy_df <- tibble::tibble(   input_1 = lapply(seq_len(nrow(x_dummy_1)), function(i) x_dummy_1[i, , drop = FALSE]),   input_2 = lapply(seq_len(nrow(x_dummy_2)), function(i) x_dummy_2[i, , drop = FALSE]) ) y_dummy_df <- tibble::tibble(output_1 = y_dummy_1, output_2 = y_dummy_2) # Named outputs  # Use compile_keras_grid to get the model compilation_results <- compile_keras_grid(   spec = spec,   grid = tibble::tibble(),    x = x_dummy_df,   y = y_dummy_df )  # Print the summary compilation_results |>    select(compiled_model) |>    pull() |>    pluck(1) |>    summary() ## Model: \"functional_1\" ## ┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓ ## ┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃ ## ┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩ ## │ input_1 (InputLayer)  │ (None, 1, 5)      │           0 │ -                  │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ input_2 (InputLayer)  │ (None, 1, 3)      │           0 │ -                  │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ dense_2 (Dense)       │ (None, 1, 16)     │          96 │ input_1[0][0]      │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ dense_3 (Dense)       │ (None, 1, 8)      │          32 │ input_2[0][0]      │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ concatenate_1         │ (None, 1, 24)     │           0 │ dense_2[0][0],     │ ## │ (Concatenate)         │                   │             │ dense_3[0][0]      │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ output_1 (Dense)      │ (None, 1, 1)      │          25 │ concatenate_1[0][… │ ## ├───────────────────────┼───────────────────┼─────────────┼────────────────────┤ ## │ output_2 (Dense)      │ (None, 1, 1)      │          25 │ concatenate_1[0][… │ ## └───────────────────────┴───────────────────┴─────────────┴────────────────────┘ ##  Total params: 178 (712.00 B) ##  Trainable params: 178 (712.00 B) ##  Non-trainable params: 0 (0.00 B) compilation_results |>    select(compiled_model) |>    pull() |>    pluck(1) |>    plot(show_shapes = TRUE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"when-to-use-the-functional-api-1","dir":"Articles","previous_headings":"","what":"When to use the functional API","title":"Building Functional Models with kerasnip","text":"general, functional API higher-level, easier safer, number features subclassed models support. However, model subclassing provides greater flexibility building models easily expressible directed acyclic graphs layers. example, implement Tree-RNN functional API subclass Model directly.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"functional-api-strengths","dir":"Articles","previous_headings":"When to use the functional API","what":"Functional API strengths","title":"Building Functional Models with kerasnip","text":"Less verbose: super$initialize(), call = function(...), self$..., etc. Model validation graph definition: functional API, input specification (shape dtype) created advance using layer_input(). time layer called, validates input specification matches assumptions, raising helpful error message . functional model plottable inspectable: can plot model graph, can easily access intermediate nodes graph. functional model can serialized cloned: data structure rather code, functional model safely serializable. can saved single file, allowing recreate exact model without needing original code.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"functional-api-weakness","dir":"Articles","previous_headings":"When to use the functional API","what":"Functional API weakness","title":"Building Functional Models with kerasnip","text":"support dynamic architectures: functional API treats models DAGs layers. true deep learning architectures, – example, recursive networks Tree RNNs follow assumption implemented functional API.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Building Functional Models with kerasnip","text":"create_keras_functional_spec() function provides powerful intuitive way define, fit, tune complex Keras models within tidymodels framework. defining model graph connected blocks, can represent nearly architecture kerasnip handles boilerplate integrating parsnip, dials, tune.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-core-idea-from-keras-layers-to-tidymodels-specs","dir":"Articles","previous_headings":"","what":"The Core Idea: From Keras Layers to Tidymodels Specs","title":"Getting Started with kerasnip","text":"keras3 package allows building deep learning models layer--layer, powerful flexible approach. However, tidymodels ecosystem designed around declarative model specifications, define model want parameters want tune, rather building imperatively. kerasnip bridges gap simple powerful concept: layer blocks. define components neural network (e.g., input block, dense block, dropout block) simple R functions. kerasnip uses blocks building materials create brand new parsnip model specification function . new function behaves just like parsnip model (e.g., rand_forest() linear_reg()), making easy integrate tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kerasnip","text":"can install development version kerasnip GitHub. also need keras3. ’ll start loading kerasnip, tidymodels keras3:","code":"install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\") pak::pak(\"rstudio/keras3\")  # Install the backend keras3::install_keras() library(kerasnip) library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── #> ✔ broom        1.0.9     ✔ recipes      1.3.1 #> ✔ dials        1.4.1     ✔ rsample      1.3.1 #> ✔ dplyr        1.1.4     ✔ tibble       3.3.0 #> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 #> ✔ infer        1.0.9     ✔ tune         1.3.0 #> ✔ modeldata    1.5.0     ✔ workflows    1.2.0 #> ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 #> ✔ purrr        1.1.0     ✔ yardstick    1.3.2 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() library(keras3) #>  #> Attaching package: 'keras3' #> The following object is masked from 'package:yardstick': #>  #>     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"a-kerasnip-mnist-example","dir":"Articles","previous_headings":"","what":"A kerasnip MNIST Example","title":"Getting Started with kerasnip","text":"Let’s replicate classic Keras introductory example, training simple MLP MNIST dataset, using kerasnip workflow. demonstrate translate standard Keras model reusable, modular parsnip specification. ’re familiar Keras, ’ll recognize structure; , perfect place start. ’ll begin learning basics simple task: recognizing handwritten digits MNIST dataset. MNIST dataset contains 28×28 pixel grayscale images handwritten digits, like : image comes label indicating digit represents. example, labels images might 5, 0, 4, 1.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"preparing-the-data","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Preparing the Data","title":"Getting Started with kerasnip","text":"step identical Keras model. load MNIST dataset, reshape predictors, convert outcome factor tidymodels.","code":"mnist <- dataset_mnist() #> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz #>        0/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step11010048/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step x_train <- mnist$train$x y_train <- mnist$train$y x_test <- mnist$test$x y_test <- mnist$test$y  # Reshape x_train <- array_reshape(x_train, c(nrow(x_train), 784)) x_test <- array_reshape(x_test, c(nrow(x_test), 784)) # Rescale x_train <- x_train / 255 x_test <- x_test / 255  # Convert outcomes to factors for tidymodels # kerasnip will handle y convertion internally using keras3::to_categorical() y_train_factor <- factor(y_train) y_test_factor <- factor(y_test)  # For tidymodels, it's best to work with data frames # Use I() to keep the matrix structure of x within the data frame train_df <- data.frame(x = I(x_train), y = y_train_factor) test_df <- data.frame(x = I(x_test), y = y_test_factor)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-standard-keras-approach-for-comparison","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"The Standard Keras Approach (for comparison)","title":"Getting Started with kerasnip","text":"diving kerasnip workflow, let’s quickly look model built using standard keras3 code. help highlight different approach kerasnip enables. code imperative: define layer add model step--step. Now, let’s see kerasnip approaches defining reusable components declarative, tidymodels-friendly workflow.","code":"# The standard Keras3 approach model <- keras_model_sequential(input_shape = 784) |>   layer_dense(units = 256, activation = \"relu\") |>   layer_dropout(rate = 0.4) |>   layer_dense(units = 128, activation = \"relu\") |>   layer_dropout(rate = 0.3) |>   layer_dense(units = 10, activation = \"softmax\")  summary(model)  model |>   compile(     loss = \"categorical_crossentropy\",     optimizer = optimizer_rmsprop(),     metrics = \"accuracy\"   )  # The model would then be trained with model |> fit(...)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"defining-the-model-with-reusable-blocks","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Defining the Model with Reusable Blocks","title":"Getting Started with kerasnip","text":"original Keras example interleaves layer_dense() layer_dropout(). kerasnip, can encapsulate pattern single, reusable block. makes overall architecture cleaner modular. Now, use create_keras_sequential_spec() generate parsnip model function.","code":"# An input block to initialize the model. # The 'model' argument is supplied implicitly by the kerasnip backend. mlp_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  # A reusable \"module\" that combines a dense layer and a dropout layer. # All arguments that should be tunable need a default value. dense_dropout_block <- function(model, units = 128, rate = 0.1) {   model |>     layer_dense(units = units, activation = \"relu\") |>     layer_dropout(rate = rate) }  # The output block for classification. mlp_output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") } create_keras_sequential_spec(   model_name = \"mnist_mlp\",   layer_blocks = list(     input = mlp_input_block,     hidden_1 = dense_dropout_block,     hidden_2 = dense_dropout_block,     output = mlp_output_block   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"building-and-fitting-the-model","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Building and Fitting the Model","title":"Getting Started with kerasnip","text":"can now use new mnist_mlp() function. Notice arguments, hidden_1_units hidden_1_rate, automatically generated kerasnip. names created combining name layer block (e.g., hidden_1) arguments block’s function (e.g., units, rate). replicate keras3 example, ’ll use hidden blocks provide parameters.","code":"mlp_spec <- mnist_mlp(   hidden_1_units = 256,   hidden_1_rate = 0.4,   hidden_2_rate = 0.3,   hidden_2_units =  128,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Fit the model mlp_fit <- fit(mlp_spec, y ~ x, data = train_df) mlp_fit |>    extract_keras_model() |>    summary() #> Model: \"sequential\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense (Dense)                     │ (None, 256)              │       200,960 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout (Dropout)                 │ (None, 256)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_1 (Dense)                   │ (None, 128)              │        32,896 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_1 (Dropout)               │ (None, 128)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_2 (Dense)                   │ (None, 10)               │         1,290 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 470,294 (1.79 MB) #>  Trainable params: 235,146 (918.54 KB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 235,148 (918.55 KB) mlp_fit |>    extract_keras_model() |>    plot(show_shapes = TRUE) mlp_fit |>    extract_keras_history() |>    plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"evaluating-model-performance","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Evaluating Model Performance","title":"Getting Started with kerasnip","text":"keras_evaluate() function provides straightforward way assess model’s performance test set, using underlying keras3::evaluate() method. returns loss metrics specified model compilation step.","code":"mlp_fit |> keras_evaluate(x_test, y_test) #> 313/313 - 0s - 1ms/step - accuracy: 0.9831 - loss: 0.0765 #> $accuracy #> [1] 0.9831 #>  #> $loss #> [1] 0.07649575"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"making-predictions","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Making Predictions","title":"Getting Started with kerasnip","text":"model trained, can use standard tidymodels predict() function generate predictions new data. default, predict() parsnip classification model returns predicted class labels. get underlying probabilities class, can set type = \"prob\". returns tibble probability column 10 classes (0-9). can compare predicted class actual class images see model performing.","code":"# Predict the class for the first 5 images in the test set  class_preds <- mlp_fit |>   predict(new_data = head(select(test_df, x))) #> 1/1 - 0s - 39ms/step class_preds #> # A tibble: 6 × 1 #>   .pred_class #>   <fct>       #> 1 7           #> 2 2           #> 3 1           #> 4 0           #> 5 4           #> 6 1 # Predict probabilities for the first 5 images prob_preds <- mlp_fit |> predict(new_data = head(select(test_df, x)), type = \"prob\") #> 1/1 - 0s - 21ms/step prob_preds #> # A tibble: 6 × 10 #>     .pred_0   .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6  .pred_7 #>       <dbl>     <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>    <dbl> #> 1 3.49 e-20 2.26 e-17 1.14e-12 1.31e-11 2.12 e-22 1.07e-16 5.62e-29 1   e+ 0 #> 2 2.29 e-22 1.67 e-11 1   e+ 0 4.94e-14 8.79 e-30 6.47e-22 2.66e-21 1.45e-18 #> 3 6.19 e-12 1.000e+ 0 5.95e-10 4.24e-11 8.51 e- 8 8.13e-10 2.47e-10 1.08e- 6 #> 4 1.000e+ 0 9.21 e-14 6.33e- 9 4.35e-11 8.92 e-12 3.46e- 9 3.24e- 7 2.85e- 9 #> 5 3.38 e-12 6.46 e-13 1.28e-11 3.18e-13 1.000e+ 0 1.24e-11 2.44e-12 1.09e- 8 #> 6 8.65 e-13 1.000e+ 0 3.14e-11 5.17e-11 2.49 e- 7 1.16e-10 2.32e-12 5.71e- 6 #> # ℹ 2 more variables: .pred_8 <dbl>, .pred_9 <dbl> # Combine predictions with actuals for comparison comparison <- bind_cols(   class_preds,   prob_preds ) |>   bind_cols(     head(test_df[, \"y\", drop = FALSE])   ) comparison #> # A tibble: 6 × 12 #>   .pred_class   .pred_0   .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6 #>   <fct>           <dbl>     <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl> #> 1 7           3.49 e-20 2.26 e-17 1.14e-12 1.31e-11 2.12 e-22 1.07e-16 5.62e-29 #> 2 2           2.29 e-22 1.67 e-11 1   e+ 0 4.94e-14 8.79 e-30 6.47e-22 2.66e-21 #> 3 1           6.19 e-12 1.000e+ 0 5.95e-10 4.24e-11 8.51 e- 8 8.13e-10 2.47e-10 #> 4 0           1.000e+ 0 9.21 e-14 6.33e- 9 4.35e-11 8.92 e-12 3.46e- 9 3.24e- 7 #> 5 4           3.38 e-12 6.46 e-13 1.28e-11 3.18e-13 1.000e+ 0 1.24e-11 2.44e-12 #> 6 1           8.65 e-13 1.000e+ 0 3.14e-11 5.17e-11 2.49 e- 7 1.16e-10 2.32e-12 #> # ℹ 4 more variables: .pred_7 <dbl>, .pred_8 <dbl>, .pred_9 <dbl>, y <fct>"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"example-2-tuning-the-model-architecture","dir":"Articles","previous_headings":"","what":"Example 2: Tuning the Model Architecture","title":"Getting Started with kerasnip","text":"Now ’ll showcase main strength kerasnip: tuning network architecture . can treat number layers, parameters layers, hyperparameters optimized tune. Using mnist_mlp spec just created, let’s define tunable model. Next, define search space tunable parameters using dials. Finally, can inspect results find architecture performed best. First, summary table: Now ’ve identified best-performing hyperparameters, final step create train final model. use select_best() get top parameters, finalize_workflow() update workflow , fit() one last time full training dataset. can now inspect final, tuned model.  result shows tune tested various network depths, widths, dropout rates, successfully finding best-performing combination within search space. using kerasnip, able integrate complex architectural tuning directly standard tidymodels workflow.","code":"# Define a tunable specification # We set num_hidden_2 = 0 to disable the second hidden block for this tuning example tune_spec <- mnist_mlp(   num_hidden_1 = tune(),   hidden_1_units = tune(),   hidden_1_rate = tune(),   num_hidden_2 = 0,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Create a workflow tune_wf <- workflow(y ~ x, tune_spec) # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     num_hidden_1 = dials::num_terms(c(1, 3)),     hidden_1_units = dials::hidden_units(c(64, 256)),     hidden_1_rate = dials::dropout(c(0.2, 0.4))   ) grid <- grid_regular(params, levels = 3) grid #> # A tibble: 27 × 3 #>    num_hidden_1 hidden_1_units hidden_1_rate #>           <int>          <int>         <dbl> #>  1            1             64           0.2 #>  2            2             64           0.2 #>  3            3             64           0.2 #>  4            1            160           0.2 #>  5            2            160           0.2 #>  6            3            160           0.2 #>  7            1            256           0.2 #>  8            2            256           0.2 #>  9            3            256           0.2 #> 10            1             64           0.3 #> # ℹ 17 more rows # Using only the first 100 rows for speed. The real call should be # folds <- vfold_cv(train_df, v = 3) folds <- vfold_cv(train_df[1:100,], v = 3)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   metrics = metric_set(accuracy),   control = control_grid(save_pred = FALSE, save_workflow = TRUE) ) #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 1s - 264ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 33ms/step # Show the summary table of the best models show_best(tune_res, metric = \"accuracy\") #> # A tibble: 5 × 9 #>   num_hidden_1 hidden_1_units hidden_1_rate .metric  .estimator  mean     n #>          <int>          <int>         <dbl> <chr>    <chr>      <dbl> <int> #> 1            3            160           0.2 accuracy multiclass 0.800     3 #> 2            2            256           0.3 accuracy multiclass 0.790     3 #> 3            2            160           0.3 accuracy multiclass 0.780     3 #> 4            3            256           0.2 accuracy multiclass 0.780     3 #> 5            1            256           0.2 accuracy multiclass 0.770     3 #> # ℹ 2 more variables: std_err <dbl>, .config <chr> # Select the best hyperparameters best_hps <- select_best(tune_res, metric = \"accuracy\")  # Finalize the workflow with the best hyperparameters final_wf <- finalize_workflow(tune_wf, best_hps)  # Fit the final model on the full training data final_fit <- fit(final_wf, data = train_df) # Print the model summary final_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>    summary() #> Model: \"sequential_82\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense_246 (Dense)                 │ (None, 160)              │       125,600 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_164 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_247 (Dense)                 │ (None, 160)              │        25,760 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_165 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_248 (Dense)                 │ (None, 160)              │        25,760 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_166 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_249 (Dense)                 │ (None, 10)               │         1,610 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 357,462 (1.36 MB) #>  Trainable params: 178,730 (698.16 KB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 178,732 (698.18 KB)  # Plot the training history final_fit |>    extract_fit_parsnip() |>   extract_keras_history() |>   plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"The Sequential Model with kerasnip","text":"vignette provides comprehensive guide using kerasnip define sequential Keras models within tidymodels ecosystem. kerasnip bridges gap imperative, layer--layer construction Keras models declarative, specification-based approach tidymodels. , focus create_keras_sequential_spec(), ideal models layers form plain stack, layer exactly one input tensor one output tensor.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"The Sequential Model with kerasnip","text":"’ll start loading necessary packages:","code":"library(kerasnip) library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── #> ✔ broom        1.0.9     ✔ recipes      1.3.1 #> ✔ dials        1.4.1     ✔ rsample      1.3.1 #> ✔ dplyr        1.1.4     ✔ tibble       3.3.0 #> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 #> ✔ infer        1.0.9     ✔ tune         1.3.0 #> ✔ modeldata    1.5.0     ✔ workflows    1.2.0 #> ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 #> ✔ purrr        1.1.0     ✔ yardstick    1.3.2 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() library(keras3) #>  #> Attaching package: 'keras3' #> The following object is masked from 'package:yardstick': #>  #>     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"when-to-use-create_keras_sequential_spec","dir":"Articles","previous_headings":"","what":"When to use create_keras_sequential_spec()","title":"The Sequential Model with kerasnip","text":"Sequential model Keras appropriate plain stack layers layer exactly one input tensor one output tensor. kerasnip’s create_keras_sequential_spec() function designed define models tidymodels-compatible way. Instead building model layer--layer imperatively, define named, ordered list R functions called layer_blocks. layer_block function takes Keras model object first argument returns modified model. kerasnip uses blocks construct full Keras Sequential model. models complex, non-linear topologies (e.g., multiple inputs/outputs, residual connections, multi-branch models), use create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"creating-a-kerasnip-sequential-model-specification","dir":"Articles","previous_headings":"","what":"Creating a kerasnip Sequential Model Specification","title":"The Sequential Model with kerasnip","text":"Let’s define simple sequential model three dense layers. First, define layer_blocks: Now, use create_keras_sequential_spec() generate parsnip model specification function. ’ll name model my_simple_mlp.","code":"# The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  # A reusable block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32, activation = \"relu\") {   model |> layer_dense(units = units, activation = activation) }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes, activation = \"softmax\") {   model |> layer_dense(units = num_classes, activation = activation) } create_keras_sequential_spec(   model_name = \"my_simple_mlp\",   layer_blocks = list(     input = input_block,     hidden_1 = hidden_block,     hidden_2 = hidden_block,     output = output_block   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"a-common-debugging-workflow-compile_keras_grid","dir":"Articles","previous_headings":"","what":"A common debugging workflow: compile_keras_grid()","title":"The Sequential Model with kerasnip","text":"original Keras guide, common workflow incrementally add layers call summary() inspect architecture. kerasnip, model defined declaratively, can’t inspect layer--layer way. However, kerasnip provides powerful equivalent: compile_keras_grid(). function checks layer_blocks define valid Keras model returns compiled model structure, without running full training cycle. perfect debugging architecture. Let’s see action CNN architecture:","code":"# Define CNN layer blocks cnn_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } cnn_conv_block <- function(model, filters = 32, kernel_size = 3, activation = \"relu\") {   model |> layer_conv_2d(filters = filters, kernel_size = kernel_size, activation = activation) } cnn_pool_block <- function(model, pool_size = 2) {   model |> layer_max_pooling_2d(pool_size = pool_size) } cnn_flatten_block <- function(model) {   model |> layer_flatten() } cnn_output_block <- function(model, num_classes, activation = \"softmax\") {   model |> layer_dense(units = num_classes, activation = activation) }  # Create the kerasnip spec function create_keras_sequential_spec(   model_name = \"my_cnn\",   layer_blocks = list(     input = cnn_input_block,     conv1 = cnn_conv_block,     pool1 = cnn_pool_block,     flatten = cnn_flatten_block,     output = cnn_output_block   ),   mode = \"classification\" )  # Create a spec instance for a 28x28x1 image cnn_spec <- my_cnn(   conv1_filters = 32, conv1_kernel_size = 5,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\" )  # Prepare dummy data with the correct shape. # We create a list of 28x28x1 arrays. x_dummy_list <- lapply(1:10, function(i) array(runif(28*28*1), dim = c(28, 28, 1))) x_dummy_df <- tibble::tibble(x = x_dummy_list) y_dummy <- factor(sample(0:9, 10, replace = TRUE), levels = 0:9) y_dummy_df <- tibble::tibble(y = y_dummy)   # Use compile_keras_grid to get the model summary compilation_results <- compile_keras_grid(   spec = cnn_spec,   grid = tibble::tibble(),    x = x_dummy_df,   y = y_dummy_df )  # Print the summary compilation_results |>    select(compiled_model) |>    pull() |>    pluck(1) |>    summary() #> Model: \"sequential\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ conv2d (Conv2D)                   │ (None, 24, 24, 32)       │           832 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ max_pooling2d (MaxPooling2D)      │ (None, 12, 12, 32)       │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ flatten (Flatten)                 │ (None, 4608)             │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense (Dense)                     │ (None, 10)               │        46,090 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 46,922 (183.29 KB) #>  Trainable params: 46,922 (183.29 KB) #>  Non-trainable params: 0 (0.00 B) compilation_results |>    select(compiled_model) |>    pull() |>    pluck(1) |>    plot(show_shapes = TRUE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"vignette demonstrates complete tidymodels workflow regression task using Keras functional model defined kerasnip. use Ames Housing dataset predict house prices. key feature example use multi-input Keras model, numerical categorical features processed separate input branches. kerasnip allows define complex Keras architectures, including multiple inputs, integrate seamlessly tidymodels ecosystem robust modeling tuning.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) library(keras3) library(dplyr)       # For data manipulation library(ggplot2)     # For plotting library(future)      # For parallel processing library(finetune)    # For racing"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"’ll use Ames Housing dataset, available modeldata package. split data training testing sets.","code":"# Select relevant columns and remove rows with missing values ames_df <- ames |>   select(Sale_Price, Gr_Liv_Area, Year_Built, Neighborhood, Bldg_Type, Overall_Cond, Total_Bsmt_SF, contains(\"SF\")) |>   na.omit()  # Split data into training and testing sets set.seed(123) ames_split <- initial_split(ames_df, prop = 0.8, strata = Sale_Price) ames_train <- training(ames_split) ames_test <- testing(ames_split)  # Create cross-validation folds for tuning ames_folds <- vfold_cv(ames_train, v = 5, strata = Sale_Price)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"recipe-for-preprocessing","dir":"Articles","previous_headings":"","what":"Recipe for Preprocessing","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"create recipes object preprocess data. recipe : * Predict Sale_Price using variables. * Normalize numerical predictors. * Create dummy variables categorical predictors. * Collapse group predictors single matrix column using step_collapse(). final step crucial multi-input Keras model, kerasnip functional API expects list matrices multiple inputs, matrix corresponds distinct input layer.","code":"ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) |>   step_normalize(all_numeric_predictors()) |>   step_collapse(all_numeric_predictors(), new_col = \"numerical_input\") |>    step_dummy(Neighborhood) |>    step_collapse(starts_with(\"Neighborhood\"), new_col = \"neighborhood_input\") |>    step_dummy(Bldg_Type) |>    step_collapse(starts_with(\"Bldg_Type\"), new_col = \"bldg_input\") |>   step_dummy(Overall_Cond) |>    step_collapse(starts_with(\"Overall_Cond\"), new_col = \"condition_input\")  # You can prep and bake the recipe to see the processed data # prep(ames_recipe) |> bake(new_data = ames_train)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"define-keras-functional-model-with-kerasnip","dir":"Articles","previous_headings":"","what":"Define Keras Functional Model with kerasnip","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"Now, define Keras functional model using kerasnip’s layer blocks. model four distinct input layers: one numerical features three categorical features. branches processed separately concatenated final output layer.","code":"# Define layer blocks for multi-input functional model  # Input blocks for numerical and categorical features input_numerical <- function(input_shape) {   layer_input(shape = input_shape, name = \"numerical_input\") }  input_neighborhood <- function(input_shape) {   layer_input(shape = input_shape, name = \"neighborhood_input\") }  input_bldg <- function(input_shape) {   layer_input(shape = input_shape, name = \"bldg_input\") }  input_condition <- function(input_shape) {   layer_input(shape = input_shape, name = \"condition_input\") }  # Processing blocks for each input type dense_numerical <- function(tensor, units = 32, activation = \"relu\") {   tensor |>     layer_dense(units = units, activation = activation) }  dense_categorical <- function(tensor, units = 16, activation = \"relu\") {   tensor |>     layer_dense(units = units, activation = activation) }  # Concatenation block concatenate_features <- function(numeric, neighborhood, bldg, condition) {   layer_concatenate(list(numeric, neighborhood, bldg, condition)) }  # Output block for regression output_regression <- function(tensor) {   layer_dense(tensor, units = 1, name = \"output\") }  # Create the kerasnip model specification function create_keras_functional_spec(   model_name = \"ames_functional_mlp\",   layer_blocks = list(     numerical_input = input_numerical,     neighborhood_input = input_neighborhood,     bldg_input = input_bldg,     condition_input = input_condition,     processed_numerical = inp_spec(dense_numerical, \"numerical_input\"),     processed_neighborhood = inp_spec(dense_categorical, \"neighborhood_input\"),     processed_bldg = inp_spec(dense_categorical, \"bldg_input\"),     processed_condition = inp_spec(dense_categorical, \"condition_input\"),     combined_features = inp_spec(       concatenate_features,       c(         processed_numerical = \"numeric\",         processed_neighborhood = \"neighborhood\",         processed_bldg = \"bldg\",         processed_condition = \"condition\"       )     ),     output = inp_spec(output_regression, \"combined_features\")   ),   mode = \"regression\" )  # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(\"ames_functional_mlp\"), add = TRUE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"model-specification","dir":"Articles","previous_headings":"","what":"Model Specification","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"’ll define ames_functional_mlp model specification set hyperparameters tune(). Note arguments prefixed corresponding block names (e.g., processed_numerical_units).","code":"# Define the tunable model specification functional_mlp_spec <- ames_functional_mlp(   # Tunable parameters for numerical branch   processed_numerical_units = tune(),   # Tunable parameters for categorical branch   processed_neighborhood_units = tune(),   processed_bldg_units = tune(),   processed_condition_units = tune(),   # Fixed compilation and fitting parameters   compile_loss = \"mean_squared_error\",   compile_optimizer = \"adam\",   compile_metrics = c(\"mean_absolute_error\"),   fit_epochs = 50,   fit_batch_size = 32,   fit_validation_split = 0.2,   fit_callbacks = list(callback_early_stopping(monitor = \"val_loss\", patience = 5)) ) |>   set_engine(\"keras\")  print(functional_mlp_spec)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"create-workflow","dir":"Articles","previous_headings":"","what":"Create Workflow","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"workflow combines recipe model specification.","code":"ames_wf <- workflow() |>   add_recipe(ames_recipe) |>   add_model(functional_mlp_spec)  print(ames_wf)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"define-tuning-grid","dir":"Articles","previous_headings":"","what":"Define Tuning Grid","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"create regular grid hyperparameters.","code":"# Define the tuning grid params <- extract_parameter_set_dials(ames_wf) |>   update(     processed_numerical_units = hidden_units(range = c(32, 128)),     processed_neighborhood_units = hidden_units(range = c(16, 64)),     processed_bldg_units = hidden_units(range = c(16, 64)),     processed_condition_units = hidden_units(range = c(16, 64))   ) functional_mlp_grid <- grid_regular(params, levels = 3)  print(functional_mlp_grid)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"tune-model","dir":"Articles","previous_headings":"","what":"Tune Model","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"Now, ’ll use tune_race_anova() perform cross-validation find best hyperparameters.","code":"# Note: Parallel processing with `plan(multisession)` is currently not working # with Keras models due to backend conflicts. # plan(multisession)   set.seed(123) ames_tune_results <- tune_race_anova(   ames_wf,   resamples = ames_folds,   grid = functional_mlp_grid,   metrics = metric_set(rmse, mae, rsq), # Evaluate regression metrics   control = control_race(save_pred = TRUE, save_workflow = TRUE) )"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"inspect-tuning-results","dir":"Articles","previous_headings":"","what":"Inspect Tuning Results","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"can inspect tuning results see hyperparameter combinations performed best.","code":"# Show the best performing models based on RMSE show_best(ames_tune_results, metric = \"rmse\", n = 5)  # Autoplot the results # autoplot(ames_tune_results) # Currently does not work due to a label issue.  # Select the best hyperparameters best_functional_mlp_params <- select_best(ames_tune_results, metric = \"rmse\") print(best_functional_mlp_params)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"finalize-workflow-and-fit-model","dir":"Articles","previous_headings":"","what":"Finalize Workflow and Fit Model","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"best hyperparameters, finalize workflow fit model entire training dataset.","code":"# Finalize the workflow with the best hyperparameters final_ames_wf <- finalize_workflow(ames_wf, best_functional_mlp_params)  # Fit the final model on the full training data final_ames_fit <- fit(final_ames_wf, data = ames_train)  print(final_ames_fit)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"inspect-final-model","dir":"Articles","previous_headings":"Finalize Workflow and Fit Model","what":"Inspect Final Model","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"can extract underlying Keras model training history inspection.","code":"# Extract the Keras model summary final_ames_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>   summary() # Plot the Keras model final_ames_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>   plot() # Plot the training history final_ames_fit |>   extract_fit_parsnip() |>   extract_keras_history() |>   plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_functional.html","id":"make-predictions-and-evaluate","dir":"Articles","previous_headings":"","what":"Make Predictions and Evaluate","title":"Tidymodels Workflow with Functional Keras Models (Multi-Input)","text":"Finally, make predictions test set evaluate model’s performance.","code":"# Make predictions on the test set ames_test_pred <- predict(final_ames_fit, new_data = ames_test)  # Combine predictions with actuals ames_results <- tibble::tibble(   Sale_Price = ames_test$Sale_Price,   .pred = ames_test_pred$.pred )  print(head(ames_results))  # Evaluate performance using yardstick metrics metrics_results <- metric_set(rmse, mae, rsq)(ames_results, truth = Sale_Price, estimate = .pred)  print(metrics_results)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tidymodels Workflow with Sequential Keras Models","text":"vignette demonstrates complete tidymodels workflow classification task using Keras sequential model defined kerasnip. use Palmer Penguins dataset predict penguin species based physical measurements. kerasnip package allows define Keras models using modular “layer block” approach, integrates seamlessly parsnip tune packages model specification hyperparameter tuning.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Tidymodels Workflow with Sequential Keras Models","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) library(keras3) library(dplyr)          # For data manipulation library(ggplot2)        # For plotting library(future)         # For parallel processing library(finetune)       # For racing"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data Preparation","title":"Tidymodels Workflow with Sequential Keras Models","text":"’ll use penguins dataset modeldata package. clean removing rows missing values ensuring species column factor.","code":"# Remove rows with missing values penguins_df <- penguins |>   na.omit() |>   # Convert species to factor for classification   mutate(species = factor(species))  # Split data into training and testing sets set.seed(123) penguin_split <- initial_split(penguins_df, prop = 0.8, strata = species) penguin_train <- training(penguin_split)  penguin_test <- testing(penguin_split)  # Create cross-validation folds for tuning penguin_folds <- vfold_cv(penguin_train, v = 5, strata = species)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"recipe-for-preprocessing","dir":"Articles","previous_headings":"","what":"Recipe for Preprocessing","title":"Tidymodels Workflow with Sequential Keras Models","text":"create recipes object preprocess data. recipe : * Predict species using variables. * Normalize numeric predictors. * Create dummy variables categorical predictors.","code":"penguin_recipe <- recipe(species ~ ., data = penguin_train) |>   step_normalize(all_numeric_predictors()) |>   step_dummy(all_nominal_predictors())  # You can prep and bake the recipe to see the processed data # prep(penguin_recipe) |> bake(new_data = penguin_train)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"define-keras-sequential-model-with-kerasnip","dir":"Articles","previous_headings":"","what":"Define Keras Sequential Model with kerasnip","title":"Tidymodels Workflow with Sequential Keras Models","text":"Now, define Keras sequential model using kerasnip’s layer blocks. ’ll create simple Multi-Layer Perceptron (MLP) two hidden layers. sequential Keras model tabular data, preprocessed input features typically combined single input layer. recipes package handles preprocessing, transforming predictors single matrix serves input Keras model.","code":"# Define layer blocks input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  hidden_block <- function(model, units = 32, activation = \"relu\", rate = 0.2) {   model |>     layer_dense(units = units, activation = activation) |>     layer_dropout(rate = rate) }  output_block <- function(model, num_classes, activation = \"softmax\") {   model |>     layer_dense(units = num_classes, activation = activation) }  # Create the kerasnip model specification function create_keras_sequential_spec(   model_name = \"penguin_mlp\",   layer_blocks = list(     input = input_block,     hidden_1 = hidden_block,     hidden_2 = hidden_block,     output = output_block   ),   mode = \"classification\" )  # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(\"penguin_mlp\"), add = TRUE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"model-specification","dir":"Articles","previous_headings":"","what":"Model Specification","title":"Tidymodels Workflow with Sequential Keras Models","text":"’ll define penguin_mlp model specification set hyperparameters tune(), indicating optimized. also set fixed parameters compilation fitting.","code":"# Define the tunable model specification mlp_spec <- penguin_mlp(   # Tunable parameters for hidden layers   hidden_1_units = tune(),   hidden_1_rate = tune(),   hidden_2_units = tune(),   hidden_2_rate = tune(),   # Fixed compilation and fitting parameters   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\",   compile_metrics = c(\"accuracy\"),   fit_epochs = 20,   fit_batch_size = 32,   fit_validation_split = 0.2,   fit_callbacks = list(callback_early_stopping(monitor = \"val_loss\", patience = 5)) ) |>   set_engine(\"keras\")  print(mlp_spec)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"create-workflow","dir":"Articles","previous_headings":"","what":"Create Workflow","title":"Tidymodels Workflow with Sequential Keras Models","text":"workflow combines recipe model specification.","code":"penguin_wf <- workflow() |>   add_recipe(penguin_recipe) |>   add_model(mlp_spec)  print(penguin_wf)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"define-tuning-grid","dir":"Articles","previous_headings":"","what":"Define Tuning Grid","title":"Tidymodels Workflow with Sequential Keras Models","text":"create regular grid hyperparameters.","code":"# Define the tuning grid params <- extract_parameter_set_dials(penguin_wf) |>   update(     hidden_1_units = hidden_units(range = c(32, 128)),     hidden_1_rate = dropout(range = c(0.1, 0.4)),     hidden_2_units = hidden_units(range = c(16, 64)),     hidden_2_rate = dropout(range = c(0.1, 0.4))   ) mlp_grid <- grid_regular(params, levels = 3)  print(mlp_grid)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"tune-model","dir":"Articles","previous_headings":"","what":"Tune Model","title":"Tidymodels Workflow with Sequential Keras Models","text":"Now, ’ll use tune_race_anova() perform cross-validation find best hyperparameters.","code":"# Note: Parallel processing with `plan(multisession)` is currently not working # with Keras models due to backend conflicts. # plan(multisession) set.seed(123)  penguin_tune_results <- tune_race_anova(   penguin_wf,   resamples = penguin_folds,   grid = mlp_grid,   metrics = metric_set(accuracy, roc_auc, f_meas), # Evaluate multiple metrics   control = control_race(save_pred = TRUE, save_workflow = TRUE) )"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"inspect-tuning-results","dir":"Articles","previous_headings":"","what":"Inspect Tuning Results","title":"Tidymodels Workflow with Sequential Keras Models","text":"can inspect tuning results see hyperparameter combinations performed best.","code":"# Show the best performing models based on accuracy show_best(penguin_tune_results, metric = \"accuracy\", n = 5)  # Autoplot the results # autoplot(penguin_tune_results) # Currently does not work due to a label issue.  # Select the best hyperparameters best_mlp_params <- select_best(penguin_tune_results, metric = \"accuracy\") print(best_mlp_params)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"finalize-workflow-and-fit-model","dir":"Articles","previous_headings":"","what":"Finalize Workflow and Fit Model","title":"Tidymodels Workflow with Sequential Keras Models","text":"best hyperparameters, finalize workflow fit model entire training dataset.","code":"# Finalize the workflow with the best hyperparameters final_penguin_wf <- finalize_workflow(penguin_wf, best_mlp_params)  # Fit the final model on the full training data final_penguin_fit <- fit(final_penguin_wf, data = penguin_train)  print(final_penguin_fit)"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"inspect-final-model","dir":"Articles","previous_headings":"Finalize Workflow and Fit Model","what":"Inspect Final Model","title":"Tidymodels Workflow with Sequential Keras Models","text":"can extract underlying Keras model training history inspection.","code":"# Extract the Keras model summary final_penguin_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>   summary() # Plot the Keras model final_penguin_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>   plot() # Plot the training history final_penguin_fit |>   extract_fit_parsnip() |>   extract_keras_history() |>   plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/workflows_sequential.html","id":"make-predictions-and-evaluate","dir":"Articles","previous_headings":"","what":"Make Predictions and Evaluate","title":"Tidymodels Workflow with Sequential Keras Models","text":"Finally, make predictions test set evaluate model’s performance.","code":"# Make predictions on the test set penguin_test_pred <- predict(final_penguin_fit, new_data = penguin_test) penguin_test_prob <- predict(final_penguin_fit, new_data = penguin_test, type = \"prob\")  # Combine predictions with actuals penguin_results <- penguin_test |>   select(species) |>   bind_cols( penguin_test_pred, penguin_test_prob)  print(head(penguin_results))  # Evaluate performance using yardstick metrics metrics_results <- metric_set(accuracy, roc_auc, f_meas)(penguin_results, truth = species, estimate = .pred_class, .pred_Adelie, .pred_Chinstrap, .pred_Gentoo)  print(metrics_results)  # Confusion Matrix conf_mat(penguin_results, truth = species, estimate = .pred_class) |>   autoplot(type = \"heatmap\")"},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Díaz. Author, maintainer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Díaz D (2025). kerasnip: Bridge Keras Tidymodels. R package version 0.0.0.9000.","code":"@Manual{,   title = {kerasnip: A Bridge Between Keras and Tidymodels},   author = {David Díaz},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"kerasnip","dir":"","previous_headings":"","what":"A Bridge Between Keras and Tidymodels","title":"A Bridge Between Keras and Tidymodels","text":"goal kerasnip provide seamless bridge keras tidymodels ecosystems. allows dynamic creation parsnip model specifications Keras models, making fully compatible tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Bridge Between Keras and Tidymodels","text":"can install development version kerasnip GitHub :","code":"# install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\")"},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-1-building-a-sequential-mlp","dir":"","previous_headings":"Example","what":"Example 1: Building a Sequential MLP","title":"A Bridge Between Keras and Tidymodels","text":"example shows core workflow building simple, linear stack layers using create_keras_sequential_spec().","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks # The first block initializes the model. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # Subsequent blocks add layers. dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } # The final block creates the output layer. output_block <- function(model) {   model |>     layer_dense(units = 1) }  # 2. Create a spec from the layer blocks # This creates a new model function, `basic_mlp()`, in your environment. create_keras_sequential_spec(   model_name = \"basic_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     output = output_block   ),   mode = \"regression\" )  # 3. Use the generated spec to define a model. # We can set the number of dense layers (`num_dense`) and their parameters (`dense_units`). spec <- basic_mlp(   num_dense = 2,   dense_units = 64,   fit_epochs = 10,   learn_rate = 0.01 ) |>   set_engine(\"keras\")  # 4. Fit the model within a tidymodels workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf <- workflow(rec, spec)  set.seed(123) fit_obj <- fit(wf, data = mtcars)  # 5. Make predictions predict(fit_obj, new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  21.3 #> 2  21.3 #> 3  22.8 #> 4  21.4 #> 5  18.7"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-2-building-a-functional-fork-join-model","dir":"","previous_headings":"Example","what":"Example 2: Building a Functional “Fork-Join” Model","title":"A Bridge Between Keras and Tidymodels","text":"complex, non-linear architectures, use create_keras_functional_spec(). example builds model input forked two paths, concatenated.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define blocks. For the functional API, blocks are nodes in a graph. input_block <- function(input_shape) layer_input(shape = input_shape) path_block <- function(tensor, units = 16) tensor |> layer_dense(units = units) concat_block <- function(input_a, input_b) layer_concatenate(list(input_a, input_b)) output_block <- function(tensor) layer_dense(tensor, units = 1)  # 2. Create the spec. The graph is defined by block names and their arguments. create_keras_functional_spec(   model_name = \"forked_mlp\",   layer_blocks = list(     main_input = input_block,     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),     # The output block must be named 'output'.     output = inp_spec(output_block, \"concatenated\")   ),   mode = \"regression\" )  # 3. Use the new spec. Arguments are prefixed with their block name. spec <- forked_mlp(path_a_units = 16, path_b_units = 8, fit_epochs = 10) |>   set_engine(\"keras\")  # Fit and predict as usual set.seed(123) fit(spec, mpg ~ ., data = mtcars) |>   predict(new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  19.4 #> 2  19.5 #> 3  21.9 #> 4  18.6 #> 5  17.9"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-3-tuning-a-sequential-mlp-architecture","dir":"","previous_headings":"Example","what":"Example 3: Tuning a Sequential MLP Architecture","title":"A Bridge Between Keras and Tidymodels","text":"example demonstrates tune number dense layers rate final dropout layer, showcasing tune architecture block hyperparameters simultaneously.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks for a tunable MLP input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } dropout_block <- function(model, rate = 0.2) {   model |> layer_dropout(rate = rate) } output_block <- function(model) {   model |> layer_dense(units = 1) }  # 2. Create a spec from the layer blocks create_keras_sequential_spec(   model_name = \"tunable_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     dropout = dropout_block,     output = output_block   ),   mode = \"regression\" )  # 3. Define a tunable model specification tune_spec <- tunable_mlp(   num_dense = tune(),   dense_units = tune(),   num_dropout = 1,   dropout_rate = tune(),   fit_epochs = 10 ) |>   set_engine(\"keras\")  # 4. Set up and run a tuning workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf_tune <- workflow(rec, tune_spec)  # Define the tuning grid. params <- extract_parameter_set_dials(wf_tune) |>   update(     num_dense = dials::num_terms(c(1, 3)),     dense_units = dials::hidden_units(c(8, 64)),     dropout_rate = dials::dropout(c(0.1, 0.5))   ) grid <- grid_regular(params, levels = 2)  # 5. Run the tuning set.seed(456) folds <- vfold_cv(mtcars, v = 3)  tune_res <- tune_grid(   wf_tune,   resamples = folds,   grid = grid,   control = control_grid(verbose = FALSE) )  # 6. Show the best architecture show_best(tune_res, metric = \"rmse\") #> # A tibble: 5 × 7 #>   num_dense dense_units dropout_rate .metric .estimator .mean .config               #>       <int>       <int>        <dbl> <chr>   <chr>      <dbl> <chr>                 #> 1         1          64          0.1 rmse    standard    2.92 Preprocessor1_Model02 #> 2         1          64          0.5 rmse    standard    3.02 Preprocessor1_Model08 #> 3         3          64          0.1 rmse    standard    3.15 Preprocessor1_Model04 #> 4         1           8          0.1 rmse    standard    3.20 Preprocessor1_Model01 #> 5         3           8          0.1 rmse    standard    3.22 Preprocessor1_Model03"},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"Pre-compiles Keras models hyperparameter combination grid. function powerful debugging tool use running full tune::tune_grid(). allows quickly validate multiple model architectures, ensuring can successfully built compiled without time-consuming process actually fitting . helps catch common errors like incompatible layer shapes invalid argument values early.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"","code":"compile_keras_grid(spec, grid, x, y)"},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"spec parsnip model specification created create_keras_sequential_spec() create_keras_functional_spec(). grid tibble data.frame containing grid hyperparameters evaluate. row represents unique model architecture compiled. x data frame matrix predictors. used infer input_shape Keras model. y vector factor outcomes. used infer output shape default loss function Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"tibble following columns: Columns input grid. compiled_model: list-column containing compiled Keras model objects. compilation failed, element NULL. error: list-column containing NA successes character string error message failures.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"Compile Validate Keras Model Architectures function iterates row provided grid. hyperparameter combination, attempts build compile Keras model defined spec. process wrapped try-catch block gracefully handle report errors occur model instantiation compilation. output tibble mirrors input grid, additional columns containing compiled model object error message, making easy inspect architectures valid.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"","code":"if (FALSE) { # \\dontrun{ if (keras::is_keras_available()) {  # 1. Define a kerasnip model specification create_keras_sequential_spec(   model_name = \"my_mlp\",   layer_blocks = list(     input_block,     hidden_block,     output_block   ),   mode = \"classification\" )  mlp_spec <- my_mlp(   hidden_units = tune(),   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\" )  # 2. Create a hyperparameter grid # Include an invalid value (-10) to demonstrate error handling param_grid <- tibble::tibble(   hidden_units = c(32, 64, -10) )  # 3. Prepare dummy data x_train <- matrix(rnorm(100 * 10), ncol = 10) y_train <- factor(sample(0:1, 100, replace = TRUE))  # 4. Compile models over the grid compiled_grid <- compile_keras_grid(   spec = mlp_spec,   grid = param_grid,   x = x_train,   y = y_train )  print(compiled_grid)  # 5. Inspect the results # The row with `hidden_units = -10` will show an error. } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Functional API. allows creating complex, tunable architectures non-linear topologies integrate seamlessly tidymodels ecosystem.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"create_keras_functional_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"model_name character string name new model specification function (e.g., \"custom_resnet\"). valid R function name. layer_blocks named list functions function defines \"block\" (node) model graph. list names crucial define names nodes. arguments function define nodes connected. See \"Model Graph Connectivity\" section details. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., custom_resnet()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Functional API. ideal models complex, non-linear topologies, networks multiple inputs/outputs residual connections. function inspects arguments layer_blocks functions makes available tunable parameters generated model specification, prefixed block's name (e.g., dense_units). Common training parameters epochs learn_rate also added.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"model-graph-connectivity","dir":"Reference","previous_headings":"","what":"Model Graph Connectivity","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"kerasnip builds model's directed acyclic graph inspecting arguments function layer_blocks list. connection logic follows: names elements layer_blocks list define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. kerasnip automatically supply output tensors nodes calling my_block. two special requirements: Input Block: first block list treated input node. function take blocks input, can input_shape argument, supplied automatically fitting. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. key feature automatic creation num_{block_name} arguments (e.g., num_dense_path). allows control many times block repeated, making easy tune depth network. block can repeated exactly one input another block graph. new model specification function update() method created environment specified env argument.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   library(keras3)   library(parsnip)    # 1. Define block functions. These are the building blocks of our model.   # An input block that receives the data's shape automatically.   input_block <- function(input_shape) layer_input(shape = input_shape)    # A dense block with a tunable `units` parameter.   dense_block <- function(tensor, units) {     tensor |> layer_dense(units = units, activation = \"relu\")   }    # A block that adds two tensors together (for the residual connection).   add_block <- function(input_a, input_b) layer_add(list(input_a, input_b))    # An output block for regression.   output_block_reg <- function(tensor) layer_dense(tensor, units = 1)    # 2. Create the spec. The `layer_blocks` list defines the graph.   create_keras_functional_spec(     model_name = \"my_resnet_spec\",     layer_blocks = list(       # The names of list elements are the node names.       main_input = input_block,        # The argument `main_input` connects this block to the input node.       dense_path = function(main_input, units = 32) dense_block(main_input, units),        # This block's arguments connect it to the original input AND the dense layer.       add_residual = function(main_input, dense_path) add_block(main_input, dense_path),        # This block must be named 'output'. It connects to the residual add layer.       output = function(add_residual) output_block_reg(add_residual)     ),     mode = \"regression\"   )    # 3. Use the newly created specification function!   # The `dense_path_units` argument was created automatically.   model_spec <- my_resnet_spec(dense_path_units = 64, epochs = 10)    # You could also tune the number of dense layers since it has a single input:   # model_spec <- my_resnet_spec(num_dense_path = 2, dense_path_units = 32)    print(model_spec)   # tune::tunable(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Sequential API. ideal choice creating models simple, linear stack layers. models complex, non-linear topologies, see create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"create_keras_sequential_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"model_name character string name new model specification function (e.g., \"custom_cnn\"). valid R function name. layer_blocks named, ordered list functions. function defines \"block\" Keras layers. function must take Keras model object first argument return modified model. arguments function become tunable parameters final model specification. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., my_mlp()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Sequential API. function inspects arguments layer_blocks functions (ignoring special arguments like input_shape num_classes) makes available arguments generated model specification, prefixed block's name (e.g., dense_units). new model specification function update() method created environment specified env argument.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"model-architecture-sequential-api-","dir":"Reference","previous_headings":"","what":"Model Architecture (Sequential API)","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"kerasnip builds model applying functions layer_blocks order provided. function receives Keras model built previous function returns modified version. first block must initialize model (e.g., keras_model_sequential()). can accept input_shape argument, kerasnip provide automatically fitting. Subsequent blocks add layers model. final block add output layer. classification, can accept num_classes argument, provided automatically. key feature function automatic creation num_{block_name} arguments (e.g., num_hidden). allows control many times block repeated, making easy tune depth network.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) { library(keras3) library(parsnip) library(dials)  # 1. Define layer blocks for a complete model. # The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # A block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") }  # 2. Create the spec, providing blocks in the correct order. create_keras_sequential_spec( model_name = \"my_mlp\",   layer_blocks = list(     input = input_block,     hidden = hidden_block,     output = output_block   ),   mode = \"classification\" )  # 3. Use the newly created specification function! # Note the new arguments `num_hidden` and `hidden_units`. model_spec <- my_mlp(   num_hidden = 2,   hidden_units = 64,   epochs = 10,   learn_rate = 0.01 )  print(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Training History — extract_keras_history","title":"Extract Keras Training History — extract_keras_history","text":"Extracts returns training history parsnip model_fit object created kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Training History — extract_keras_history","text":"","code":"extract_keras_history(object)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Training History — extract_keras_history","text":"object model_fit object produced kerasnip specification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Training History — extract_keras_history","text":"keras_training_history object. can call plot() object visualize learning curves.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Keras Training History — extract_keras_history","text":"Extract Keras Training History history object contains metrics recorded model training, loss accuracy, epoch. highly useful visualizing training process diagnosing issues like overfitting. returned object can plotted directly.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"Extracts returns underlying Keras model object parsnip model_fit object created kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"","code":"extract_keras_model(object)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"object model_fit object produced kerasnip specification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"raw Keras model object (keras_model).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"Extract Raw Keras Model Kerasnip Fit useful need work directly Keras model object tasks like inspecting layer weights, creating custom plots, passing Keras-specific functions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Valid Grid from Compilation Results — extract_valid_grid","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"helper function filters results compile_keras_grid() return new hyperparameter grid containing combinations compiled successfully.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"","code":"extract_valid_grid(compiled_grid)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"compiled_grid tibble, result call compile_keras_grid().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"tibble containing subset original grid resulted successful model compilation. compiled_model error columns removed, leaving clean grid ready tuning.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"Filter Grid Valid Hyperparameter Sets running compile_keras_grid(), can use function remove problematic hyperparameter combinations proceeding full tune::tune_grid().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"","code":"if (FALSE) { # \\dontrun{ # Continuing the example from `compile_keras_grid`:  # `compiled_grid` contains one row with an error. valid_grid <- extract_valid_grid(compiled_grid)  # `valid_grid` now only contains the rows that compiled successfully. print(valid_grid)  # This clean grid can now be passed to tune::tune_grid(). } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Fitting Engine for Functional API Models — generic_functional_fit","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"function serves internal engine fitting kerasnip models based Keras functional API. intended called directly user. function invoked parsnip::fit() kerasnip functional model specification used.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"","code":"generic_functional_fit(formula, data, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"formula formula specifying predictor outcome variables, passed parsnip::fit() call. data data frame containing training data, passed parsnip::fit() call. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"Generic Fitting Function Functional Keras Models function orchestrates three main steps model fitting process: Build Compile: calls build_and_compile_functional_model() construct Keras model architecture based provided layer_blocks hyperparameters. Process Data: preprocesses input (x) output (y) data format expected Keras. Fit Model: calls keras3::fit() compiled model processed data, passing along fitting-specific arguments (e.g., epochs, batch_size, callbacks).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"","code":"# This function is not called directly by users. # It is called internally by `parsnip::fit()`. # For example: if (FALSE) { # \\dontrun{ # create_keras_functional_spec(...) defines my_functional_model  spec <- my_functional_model(hidden_units = 128, fit_epochs = 10) |>   set_engine(\"keras\")  # This call to fit() would invoke generic_functional_fit() internally fitted_model <- fit(spec, y ~ x, data = training_data) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"function serves internal engine fitting kerasnip models based Keras sequential API. intended called directly user. function invoked parsnip::fit() kerasnip sequential model specification used.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"","code":"generic_sequential_fit(formula, data, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"formula formula specifying predictor outcome variables, passed parsnip::fit() call. data data frame containing training data, passed parsnip::fit() call. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"Generic Fitting Function Sequential Keras Models function orchestrates three main steps model fitting process: Build Compile: calls build_and_compile_sequential_model() construct Keras model architecture based provided layer_blocks hyperparameters. Process Data: preprocesses input (x) output (y) data format expected Keras. Fit Model: calls keras3::fit() compiled model processed data, passing along fitting-specific arguments (e.g., epochs, batch_size, callbacks).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"","code":"# This function is not called directly by users. # It is called internally by `parsnip::fit()`. # For example: if (FALSE) { # \\dontrun{ # create_keras_sequential_spec(...) defines my_sequential_model  spec <- my_sequential_model(hidden_1_units = 128, fit_epochs = 10) |>   set_engine(\"keras\")  # This call to fit() would invoke generic_sequential_fit() internally fitted_model <- fit(spec, y ~ x, data = training_data) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":null,"dir":"Reference","previous_headings":"","what":"Inform About Compilation Errors — inform_errors","title":"Inform About Compilation Errors — inform_errors","text":"helper function inspects results compile_keras_grid() prints formatted, easy--read summary compilation errors occurred.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inform About Compilation Errors — inform_errors","text":"","code":"inform_errors(compiled_grid, n = 10)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inform About Compilation Errors — inform_errors","text":"compiled_grid tibble, result call compile_keras_grid(). n single integer maximum number distinct errors display detail.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inform About Compilation Errors — inform_errors","text":"Invisibly returns input compiled_grid. Called side effect printing summary console.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inform About Compilation Errors — inform_errors","text":"Display Summary Compilation Errors useful interactive debugging complex tuning grids hyperparameter combinations may lead invalid Keras models.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inform About Compilation Errors — inform_errors","text":"","code":"if (FALSE) { # \\dontrun{ # Continuing the example from `compile_keras_grid`:  # `compiled_grid` contains one row with an error. # This will print a formatted summary of that error. inform_errors(compiled_grid) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remap Layer Block Arguments for Model Specification — inp_spec","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"Creates wrapper function around Keras layer block rename arguments. powerful helper defining layer_blocks create_keras_functional_spec() create_keras_sequential_spec(), allowing connect reusable blocks model graph without writing verbose anonymous functions.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"inp_spec(block, input_map)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"block function defines Keras layer set layers. first arguments input tensor(s). input_map single character string named character vector specifies rename/remap arguments block.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"new function (closure) wraps block function renamed arguments, ready used layer_blocks list.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"inp_spec() makes model definitions cleaner readable. handles metaprogramming required create new function correct argument names, preserving original block's hyperparameters default values. function supports two modes operation based input_map: Single Input Renaming: input_map single character string, wrapper function renames first argument block function provided string. common case blocks take single tensor input. Multiple Input Mapping: input_map named character vector, provides explicit mapping new argument names (names vector) original argument names block function (values vector). used blocks multiple inputs, like concatenation layer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"if (FALSE) { # \\dontrun{ # --- Example Blocks --- # A standard dense block with one input tensor and one hyperparameter. dense_block <- function(tensor, units = 16) {   tensor |> keras3::layer_dense(units = units, activation = \"relu\") }  # A block that takes two tensors as input. concat_block <- function(input_a, input_b) {   keras3::layer_concatenate(list(input_a, input_b)) }  # An output block with one input. output_block <- function(tensor) {   tensor |> keras3::layer_dense(units = 1) }  # --- Usage --- layer_blocks <- list(   main_input = keras3::layer_input,   path_a = inp_spec(dense_block, \"main_input\"),   path_b = inp_spec(dense_block, \"main_input\"),   concatenated = inp_spec(     concat_block,     c(path_a = \"input_a\", path_b = \"input_b\")   ),   output = inp_spec(output_block, \"concatenated\") ) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate a Kerasnip Model — keras_evaluate","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"function provides kera_evaluate() method model_fit objects created kerasnip. preprocesses new data format expected Keras calls keras3::evaluate() underlying model compute loss metrics.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"","code":"keras_evaluate(object, x, y = NULL, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"object model_fit object produced kerasnip specification. x data frame matrix new predictor data. y vector data frame new outcome data corresponding x. ... Additional arguments passed keras3::evaluate() (e.g., batch_size).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"named list containing evaluation results (e.g., loss, accuracy). names determined metrics model compiled .","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"Evaluate Fitted Kerasnip Model New Data","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"","code":"if (FALSE) { # \\dontrun{ if (keras::is_keras_available()) {  # 1. Define and fit a model ---- create_keras_sequential_spec(   model_name = \"my_mlp\",   layer_blocks = list(input_block, hidden_block, output_block),   mode = \"classification\" )  mlp_spec <- my_mlp(   hidden_units = 32,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\",   compile_metrics = \"accuracy\",   fit_epochs = 5 ) |> set_engine(\"keras\")  x_train <- matrix(rnorm(100 * 10), ncol = 10) y_train <- factor(sample(0:1, 100, replace = TRUE)) train_df <- data.frame(x = I(x_train), y = y_train)  fitted_mlp <- fit(mlp_spec, y ~ x, data = train_df)  # 2. Evaluate the model on new data ---- x_test <- matrix(rnorm(50 * 10), ncol = 10) y_test <- factor(sample(0:1, 50, replace = TRUE))  eval_metrics <- keras_evaluate(fitted_mlp, x_test, y_test) print(eval_metrics)  # 3. Extract the Keras model object ---- keras_model <- extract_keras_model(fitted_mlp) summary(keras_model)  # 4. Extract the training history ---- history <- extract_keras_history(fitted_mlp) plot(history) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Dynamically Discovered Keras Objects — keras_objects","title":"Dynamically Discovered Keras Objects — keras_objects","text":"exported vectors contain names optimizers, losses, metrics discovered installed keras3 package kerasnip loaded. ensures kerasnip always --date Keras version.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dynamically Discovered Keras Objects — keras_objects","text":"","code":"keras_optimizers  keras_losses  keras_metrics"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dynamically Discovered Keras Objects — keras_objects","text":"object class character length 12. object class character length 21. object class character length 32.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dynamically Discovered Keras Objects — keras_objects","text":"objects primarily used provide default values dials parameter functions, optimizer_function() loss_function_keras(). allows tab-completion IDEs validation optimizer loss names tuning models. discovery process .onLoad() scrapes keras3 namespace functions matching optimizer_*, loss_*, metric_* patterns.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Loss Functions — loss_function_keras","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"Dials Parameter Keras Loss Functions","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"","code":"loss_function_keras(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"values character vector possible loss functions. Defaults known losses (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Optimizers — optimizer_function","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"Dials Parameter Keras Optimizers","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"","code":"optimizer_function(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"values character vector possible optimizers. Defaults known optimizers (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_functional.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Predictor Input for Keras (Functional API) — process_x_functional","title":"Process Predictor Input for Keras (Functional API) — process_x_functional","text":"Preprocesses predictor data (x) format suitable Keras models built Functional API. Handles tabular data list-columns arrays (e.g., images), supporting multiple inputs.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_functional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Predictor Input for Keras (Functional API) — process_x_functional","text":"","code":"process_x_functional(x)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_functional.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Predictor Input for Keras (Functional API) — process_x_functional","text":"x data frame matrix predictors.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_functional.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Predictor Input for Keras (Functional API) — process_x_functional","text":"list containing: x_proc: processed predictor data (matrix array, list arrays). input_shape: determined input shape(s) Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Predictor Input for Keras — process_x_sequential","title":"Process Predictor Input for Keras — process_x_sequential","text":"Preprocesses predictor data (x) format suitable Keras models. Handles tabular data list-columns arrays (e.g., images).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Predictor Input for Keras — process_x_sequential","text":"","code":"process_x_sequential(x)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Predictor Input for Keras — process_x_sequential","text":"x data frame matrix predictors.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Predictor Input for Keras — process_x_sequential","text":"list containing: x_proc: processed predictor data (matrix array). input_shape: determined input shape Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_functional.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Outcome Input for Keras (Functional API) — process_y_functional","title":"Process Outcome Input for Keras (Functional API) — process_y_functional","text":"Preprocesses outcome data (y) format suitable Keras models built Functional API. Handles regression (numeric) classification (factor) outcomes, including one-hot encoding classification, supports multiple outputs.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_functional.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Outcome Input for Keras (Functional API) — process_y_functional","text":"","code":"process_y_functional(y, is_classification = NULL, class_levels = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_functional.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Outcome Input for Keras (Functional API) — process_y_functional","text":"y vector data frame outcomes. is_classification Logical, optional. TRUE, treats y classification. FALSE, treats regression. NULL (default), determined .factor(y). class_levels Character vector, optional. factor levels classification outcomes. NULL (default), determined levels(y).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_functional.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Outcome Input for Keras (Functional API) — process_y_functional","text":"list containing: y_proc: processed outcome data (matrix one-hot encoded array, list multiple outputs). is_classification: Logical, indicating y treated classification. num_classes: Integer, number classes classification, NULL. class_levels: Character vector, factor levels classification, NULL.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Outcome Input for Keras — process_y_sequential","title":"Process Outcome Input for Keras — process_y_sequential","text":"Preprocesses outcome data (y) format suitable Keras models. Handles regression (numeric) classification (factor) outcomes, including one-hot encoding classification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Outcome Input for Keras — process_y_sequential","text":"","code":"process_y_sequential(y, is_classification = NULL, class_levels = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Outcome Input for Keras — process_y_sequential","text":"y vector outcomes. is_classification Logical, optional. TRUE, treats y classification. FALSE, treats regression. NULL (default), determined .factor(y). class_levels Character vector, optional. factor levels classification outcomes. NULL (default), determined levels(y).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Outcome Input for Keras — process_y_sequential","text":"list containing: y_proc: processed outcome data (matrix one-hot encoded array). is_classification: Logical, indicating y treated classification. num_classes: Integer, number classes classification, NULL. class_levels: Character vector, factor levels classification, NULL.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Loss — register_keras_loss","title":"Register a Custom Keras Loss — register_keras_loss","text":"Allows users register custom loss function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Loss — register_keras_loss","text":"","code":"register_keras_loss(name, loss_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Loss — register_keras_loss","text":"name name register loss (character). loss_fn loss function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Loss — register_keras_loss","text":"Registered losses stored internal environment. model compiled, kerasnip first check internal registry loss matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Metric — register_keras_metric","title":"Register a Custom Keras Metric — register_keras_metric","text":"Allows users register custom metric function can used name within kerasnip model specifications.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Metric — register_keras_metric","text":"","code":"register_keras_metric(name, metric_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Metric — register_keras_metric","text":"name name register metric (character). metric_fn metric function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Metric — register_keras_metric","text":"Registered metrics stored internal environment. model compiled, kerasnip first check internal registry metric matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Optimizer — register_keras_optimizer","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Allows users register custom optimizer function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"register_keras_optimizer(name, optimizer_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"name name register optimizer (character). optimizer_fn optimizer function. return Keras optimizer object.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Registered optimizers stored internal environment. model compiled, kerasnip first check internal registry optimizer matching provided name checking keras3 package. optimizer_fn can simple function partially applied function using purrr::partial(). useful creating versions Keras optimizers specific settings.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"if (requireNamespace(\"keras3\", quietly = TRUE)) {   # Register a custom version of Adam with a different default beta_1   my_adam <- purrr::partial(keras3::optimizer_adam, beta_1 = 0.8)   register_keras_optimizer(\"my_adam\", my_adam)    # Now \"my_adam\" can be used as a string in a model spec, e.g.,   # my_model_spec(compile_optimizer = \"my_adam\") }"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function completely removes model specification previously created create_keras_sequential_spec() create_keras_functional_spec(). cleans function user's environment associated registrations within parsnip package.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"remove_keras_spec(model_name, env = parent.frame())"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"model_name character string giving name model specification function remove (e.g., \"my_mlp\"). env environment remove function update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"Invisibly returns TRUE attempting remove objects.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function essential cleanly unloading dynamically created model. performs three main actions: removes model specification function (e.g., my_mlp()) corresponding update() method specified environment. searches parsnip's internal model environment objects whose names start model_name removes . purges fit methods, argument definitions, registrations. removes model's name parsnip's master list models. function uses un-exported parsnip:::get_model_env() perform cleanup, may subject change future parsnip versions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   # First, create a dummy spec   input_block <- function(model, input_shape) keras3::keras_model_sequential(input_shape = input_shape)   dense_block <- function(model, units = 16) model |> keras3::layer_dense(units = units)   create_keras_sequential_spec(\"my_temp_model\", list(input = input_block, dense = dense_block), \"regression\")    # Check it exists in the environment and in parsnip   exists(\"my_temp_model\")   \"my_temp_model\" %in% parsnip::show_engines(\"my_temp_model\")$model    # Now remove it   remove_keras_spec(\"my_temp_model\")    # Check it's gone   !exists(\"my_temp_model\")   !\"my_temp_model\" %in% parsnip::show_engines(NULL)$model } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/step_collapse.html","id":null,"dir":"Reference","previous_headings":"","what":"Collapse Predictors into a single list-column — step_collapse","title":"Collapse Predictors into a single list-column — step_collapse","text":"step_collapse() creates specification recipe step convert group predictors single list-column. useful custom models need predictors different format.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/step_collapse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collapse Predictors into a single list-column — step_collapse","text":"","code":"step_collapse(   recipe,   ...,   role = \"predictor\",   trained = FALSE,   columns = NULL,   new_col = \"predictor_matrix\",   skip = FALSE,   id = recipes::rand_id(\"collapse\") )"},{"path":"https://davidrsch.github.io/kerasnip/reference/step_collapse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collapse Predictors into a single list-column — step_collapse","text":"recipe recipe object. step added sequence operations recipe. ... One selector functions choose variables affected step. See [selections()] details. tidy method, currently used. role model terms created step, analysis role assigned?. default, new columns used predictors. trained logical indicate quantities preprocessing estimated. columns character string selected variable names. NULL step trained [prep.recipe()]. new_col character string name new list-column. default \"predictor_matrix\". skip logical. step skipped recipe baked [bake.recipe()]? operations baked prep run, skipping bake run may times desirable skip processing step. id character string unique step identify .","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/step_collapse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collapse Predictors into a single list-column — step_collapse","text":"updated version recipe new step added sequence existing steps (). tidy method, tibble columns terms columns affected value type collapse.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/step_collapse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collapse Predictors into a single list-column — step_collapse","text":"","code":"library(recipes) #> Loading required package: dplyr #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union #>  #> Attaching package: ‘recipes’ #> The following object is masked from ‘package:stats’: #>  #>     step  # 2 predictors dat <- data.frame(   x1 = 1:10,   x2 = 11:20,   y = 1:10 )  rec <- recipe(y ~ ., data = dat) %>%   step_collapse(x1, x2, new_col = \"pred\") %>%   prep()  bake(rec, new_data = NULL) #> # A tibble: 10 × 2 #>        y pred          #>    <int> <list>        #>  1     1 <int [1 × 2]> #>  2     2 <int [1 × 2]> #>  3     3 <int [1 × 2]> #>  4     4 <int [1 × 2]> #>  5     5 <int [1 × 2]> #>  6     6 <int [1 × 2]> #>  7     7 <int [1 × 2]> #>  8     8 <int [1 × 2]> #>  9     9 <int [1 × 2]> #> 10    10 <int [1 × 2]>"},{"path":"https://davidrsch.github.io/kerasnip/news/index.html","id":"kerasnip-0009000","dir":"Changelog","previous_headings":"","what":"kerasnip 0.0.0.9000","title":"kerasnip 0.0.0.9000","text":"Initial development version. Added create_keras_spec() generate parsnip specifications dynamically.","code":""}]
