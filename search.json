[{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement daviddrsch@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kerasnip","title":"Contributing to kerasnip","text":"outlines propose change kerasnip. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kerasnip","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to kerasnip","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to kerasnip","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use air package apply styles. can format code automatically commenting /style PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kerasnip","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kerasnip authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://davidrsch.github.io/kerasnip/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with kerasnip","title":"Getting help with kerasnip","text":"Thanks using kerasnip. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"when-to-use-the-functional-api","dir":"Articles","previous_headings":"","what":"When to Use the Functional API","title":"Building Functional Models with kerasnip","text":"create_keras_sequential_spec() perfect models simple, linear stack layers, many advanced architectures linear. Keras Functional API designed cases. use create_keras_functional_spec() model : Multiple input multiple output layers. Shared layers different branches. Residual connections (e.g., ResNets), layer’s input added output. non-linear topology. kerasnip makes easy define architectures automatically connecting graph layer blocks.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"the-core-concept-building-a-graph","dir":"Articles","previous_headings":"","what":"The Core Concept: Building a Graph","title":"Building Functional Models with kerasnip","text":"kerasnip builds model’s graph inspecting layer_blocks provide. connection logic simple powerful: names list elements layer_blocks define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. two special requirements: Input Block: first block list treated main input node. function take blocks input. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. Let’s see action.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"example-1-a-fork-join-regression-model","dir":"Articles","previous_headings":"","what":"Example 1: A Fork-Join Regression Model","title":"Building Functional Models with kerasnip","text":"build model forks input, passes two separate dense layer paths, joins results concatenation layer producing final prediction.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-1-load-libraries","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 1: Load Libraries","title":"Building Functional Models with kerasnip","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom        1.0.9     ✔ recipes      1.3.1 ## ✔ dials        1.4.1     ✔ rsample      1.3.1 ## ✔ dplyr        1.1.4     ✔ tibble       3.3.0 ## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 ## ✔ infer        1.0.9     ✔ tune         1.3.0 ## ✔ modeldata    1.5.0     ✔ workflows    1.2.0 ## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 ## ✔ purrr        1.1.0     ✔ yardstick    1.3.2 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter()  masks stats::filter() ## ✖ dplyr::lag()     masks stats::lag() ## ✖ recipes::step()  masks stats::step() library(keras3) ##  ## Attaching package: 'keras3' ## The following object is masked from 'package:yardstick': ##  ##     get_weights # Silence the startup messages from remove_keras_spec options(kerasnip.show_removal_messages = FALSE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-2-define-layer-blocks","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 2: Define Layer Blocks","title":"Building Functional Models with kerasnip","text":"building blocks model. function represents node graph.","code":"# The input node. `input_shape` is supplied automatically by the engine. input_block <- function(input_shape) {   layer_input(shape = input_shape) }  # A generic block for a dense path. `units` will be a tunable parameter. path_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") }  # A block to join two tensors. concat_block <- function(input_a, input_b) {   layer_concatenate(list(input_a, input_b)) }  # The final output block for regression. output_block_reg <- function(tensor) {   layer_dense(tensor, units = 1) }"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-3-create-the-model-specification","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 3: Create the Model Specification","title":"Building Functional Models with kerasnip","text":"Now assemble blocks graph. use inp_spec() helper connect blocks. avoids writing verbose anonymous functions like function(main_input, units) path_block(main_input, units). inp_spec() automatically creates wrapper renames arguments blocks match node names layer_blocks list.","code":"model_name <- \"forked_reg_spec\" # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(model_name), add = TRUE)  create_keras_functional_spec(   model_name = model_name,   layer_blocks = list(     # Node names are defined by the list names     main_input = input_block,      # `inp_spec()` renames the first argument of `path_block` ('tensor')     # to 'main_input' to match the node name.     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),      # For multiple inputs, `inp_spec()` takes a named vector to map     # new argument names to the original block's argument names.     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),      # The output block takes the concatenated tensor as its input.     output = inp_spec(output_block_reg, \"concatenated\")   ),   mode = \"regression\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-4-use-and-fit-the-model","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 4: Use and Fit the Model","title":"Building Functional Models with kerasnip","text":"new function forked_reg_spec() now available. arguments (path_a_units, path_b_units) discovered automatically block definitions.","code":"# We can override the default `units` from `path_block` for each path. spec <- forked_reg_spec(   path_a_units = 16,   path_b_units = 8,   fit_epochs = 10,   fit_verbose = 0 # Suppress fitting output in vignette ) |>   set_engine(\"keras\")  print(spec) ## forked reg spec Model Specification (regression) ##  ## Main Arguments: ##   num_main_input = structure(list(), class = \"rlang_zap\") ##   num_path_a = structure(list(), class = \"rlang_zap\") ##   num_path_b = structure(list(), class = \"rlang_zap\") ##   num_concatenated = structure(list(), class = \"rlang_zap\") ##   num_output = structure(list(), class = \"rlang_zap\") ##   path_a_units = 16 ##   path_b_units = 8 ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 10 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = 0 ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras # Fit the model on the mtcars dataset rec <- recipe(mpg ~ ., data = mtcars) wf <- workflow() |>    add_recipe(rec) |>   add_model(spec)   fit_obj <- fit(wf, data = mtcars)  predict(fit_obj, new_data = mtcars[1:5, ]) ## 1/1 - 0s - 40ms/step ## # A tibble: 5 × 1 ##   .pred ##   <dbl> ## 1  52.4 ## 2  52.5 ## 3  47.8 ## 4  44.9 ## 5  70.4"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"example-2-tuning-a-functional-models-depth","dir":"Articles","previous_headings":"","what":"Example 2: Tuning a Functional Model’s Depth","title":"Building Functional Models with kerasnip","text":"key feature kerasnip ability tune depth network repeating block multiple times. block can repeated exactly one input tensor another block graph. Let’s create simple functional model tune width (units) depth (num_...).","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-1-define-blocks-and-create-spec","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 1: Define Blocks and Create Spec","title":"Building Functional Models with kerasnip","text":"model architecturally sequential, build functional API demonstrate repetition feature.","code":"dense_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") } output_block_class <- function(tensor, num_classes) {   tensor |> layer_dense(units = num_classes, activation = \"softmax\") }  model_name_tune <- \"tunable_func_mlp\" on.exit(remove_keras_spec(model_name_tune), add = TRUE)  create_keras_functional_spec(   model_name = model_name_tune,   layer_blocks = list(     main_input = input_block,     # This block has a single input ('main_input'), so it can be repeated.     dense_path = inp_spec(dense_block, \"main_input\"),     output = inp_spec(output_block_class, \"dense_path\")   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-2-set-up-and-run-tuning","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 2: Set up and Run Tuning","title":"Building Functional Models with kerasnip","text":"tune dense_path_units (width) num_dense_path (depth). num_dense_path argument created automatically dense_path repeatable block. results show tidymodels successfully trained evaluated models different numbers hidden layers, demonstrating can tune architecture network.","code":"tune_spec <- tunable_func_mlp(   dense_path_units = tune(),   num_dense_path = tune(),   fit_epochs = 5,   fit_verbose = 0 ) |>   set_engine(\"keras\")  rec <- recipe(Species ~ ., data = iris) tune_wf <- workflow() |>    add_recipe(rec) |>   add_model(tune_spec)  folds <- vfold_cv(iris, v = 2)  # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     dense_path_units = hidden_units(c(8, 32)),     num_dense_path = num_terms(c(1, 3)) # Test models with 1, 2, or 3 hidden layers   )  grid <- grid_regular(params, levels = 2) grid ## # A tibble: 4 × 2 ##   num_dense_path dense_path_units ##            <int>            <int> ## 1              1                8 ## 2              3                8 ## 3              1               32 ## 4              3               32 control <- control_grid(save_pred = FALSE, verbose = FALSE)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   control = control ) ## 3/3 - 0s - 17ms/step ## 3/3 - 0s - 8ms/step ## 3/3 - 0s - 22ms/step ## 3/3 - 0s - 8ms/step ## 3/3 - 0s - 17ms/step ## 3/3 - 0s - 8ms/step ## 3/3 - 0s - 22ms/step ## 3/3 - 0s - 8ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 8ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 22ms/step ## 3/3 - 0s - 7ms/step show_best(tune_res, metric = \"accuracy\") ## # A tibble: 4 × 8 ##   num_dense_path dense_path_units .metric .estimator  mean     n std_err .config ##            <int>            <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   ## 1              3               32 accura… multiclass 0.953     2 0.0200  Prepro… ## 2              1               32 accura… multiclass 0.673     2 0.00667 Prepro… ## 3              1                8 accura… multiclass 0.66      2 0.00667 Prepro… ## 4              3                8 accura… multiclass 0.46      2 0.127   Prepro…"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Building Functional Models with kerasnip","text":"create_keras_functional_spec() function provides powerful intuitive way define, fit, tune complex Keras models within tidymodels framework. defining model graph connected blocks, can represent nearly architecture kerasnip handles boilerplate integrating parsnip, dials, tune.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-core-idea-from-keras-layers-to-tidymodels-specs","dir":"Articles","previous_headings":"","what":"The Core Idea: From Keras Layers to Tidymodels Specs","title":"Getting Started with kerasnip","text":"keras3 package allows building deep learning models layer--layer, powerful flexible approach. However, tidymodels ecosystem designed around declarative model specifications, define model want parameters want tune, rather building imperatively. kerasnip bridges gap simple powerful concept: layer blocks. define components neural network (e.g., input block, dense block, dropout block) simple R functions. kerasnip uses blocks building materials create brand new parsnip model specification function . new function behaves just like parsnip model (e.g., rand_forest() linear_reg()), making easy integrate tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kerasnip","text":"can install development version kerasnip GitHub. also need keras3. ’ll start loading kerasnip, tidymodels keras3:","code":"install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\") pak::pak(\"rstudio/keras3\")  # Install the backend keras3::install_keras() library(kerasnip) library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── #> ✔ broom        1.0.9     ✔ recipes      1.3.1 #> ✔ dials        1.4.1     ✔ rsample      1.3.1 #> ✔ dplyr        1.1.4     ✔ tibble       3.3.0 #> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 #> ✔ infer        1.0.9     ✔ tune         1.3.0 #> ✔ modeldata    1.5.0     ✔ workflows    1.2.0 #> ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 #> ✔ purrr        1.1.0     ✔ yardstick    1.3.2 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() library(keras3) #>  #> Attaching package: 'keras3' #> The following object is masked from 'package:yardstick': #>  #>     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"a-kerasnip-mnist-example","dir":"Articles","previous_headings":"","what":"A kerasnip MNIST Example","title":"Getting Started with kerasnip","text":"Let’s replicate classic Keras introductory example, training simple MLP MNIST dataset, using kerasnip workflow. demonstrate translate standard Keras model reusable, modular parsnip specification. ’re familiar Keras, ’ll recognize structure; , perfect place start. ’ll begin learning basics simple task: recognizing handwritten digits MNIST dataset. MNIST dataset contains 28×28 pixel grayscale images handwritten digits, like : image comes label indicating digit represents. example, labels images might 5, 0, 4, 1.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"preparing-the-data","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Preparing the Data","title":"Getting Started with kerasnip","text":"step identical Keras model. load MNIST dataset, reshape predictors, convert outcome factor tidymodels.","code":"mnist <- dataset_mnist() #> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz #>        0/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step 2416640/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step x_train <- mnist$train$x y_train <- mnist$train$y x_test <- mnist$test$x y_test <- mnist$test$y  # Reshape x_train <- array_reshape(x_train, c(nrow(x_train), 784)) x_test <- array_reshape(x_test, c(nrow(x_test), 784)) # Rescale x_train <- x_train / 255 x_test <- x_test / 255  # Convert outcomes to factors for tidymodels # kerasnip will handle y convertion internally using keras3::to_categorical() y_train_factor <- factor(y_train) y_test_factor <- factor(y_test)  # For tidymodels, it's best to work with data frames # Use I() to keep the matrix structure of x within the data frame train_df <- data.frame(x = I(x_train), y = y_train_factor) test_df <- data.frame(x = I(x_test), y = y_test_factor)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-standard-keras-approach-for-comparison","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"The Standard Keras Approach (for comparison)","title":"Getting Started with kerasnip","text":"diving kerasnip workflow, let’s quickly look model built using standard keras3 code. help highlight different approach kerasnip enables. code imperative: define layer add model step--step. Now, let’s see kerasnip approaches defining reusable components declarative, tidymodels-friendly workflow.","code":"# The standard Keras3 approach model <- keras_model_sequential(input_shape = 784) |>   layer_dense(units = 256, activation = \"relu\") |>   layer_dropout(rate = 0.4) |>   layer_dense(units = 128, activation = \"relu\") |>   layer_dropout(rate = 0.3) |>   layer_dense(units = 10, activation = \"softmax\")  summary(model)  model |>   compile(     loss = \"categorical_crossentropy\",     optimizer = optimizer_rmsprop(),     metrics = \"accuracy\"   )  # The model would then be trained with model |> fit(...)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"defining-the-model-with-reusable-blocks","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Defining the Model with Reusable Blocks","title":"Getting Started with kerasnip","text":"original Keras example interleaves layer_dense() layer_dropout(). kerasnip, can encapsulate pattern single, reusable block. makes overall architecture cleaner modular. Now, use create_keras_sequential_spec() generate parsnip model function.","code":"# An input block to initialize the model. # The 'model' argument is supplied implicitly by the kerasnip backend. mlp_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  # A reusable \"module\" that combines a dense layer and a dropout layer. # All arguments that should be tunable need a default value. dense_dropout_block <- function(model, units = 128, rate = 0.1) {   model |>     layer_dense(units = units, activation = \"relu\") |>     layer_dropout(rate = rate) }  # The output block for classification. mlp_output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") } create_keras_sequential_spec(   model_name = \"mnist_mlp\",   layer_blocks = list(     input = mlp_input_block,     hidden_1 = dense_dropout_block,     hidden_2 = dense_dropout_block,     output = mlp_output_block   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"building-and-fitting-the-model","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Building and Fitting the Model","title":"Getting Started with kerasnip","text":"can now use new mnist_mlp() function. Notice arguments, hidden_1_units hidden_1_rate, automatically generated kerasnip. names created combining name layer block (e.g., hidden_1) arguments block’s function (e.g., units, rate). replicate keras3 example, ’ll use hidden blocks provide parameters.","code":"mlp_spec <- mnist_mlp(   hidden_1_units = 256,   hidden_1_rate = 0.4,   hidden_2_rate = 0.3,   hidden_2_units =  128,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Fit the model mlp_fit <- fit(mlp_spec, y ~ x, data = train_df) mlp_fit |>    extract_keras_summary() #> Model: \"sequential\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense (Dense)                     │ (None, 256)              │       200,960 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout (Dropout)                 │ (None, 256)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_1 (Dense)                   │ (None, 128)              │        32,896 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_1 (Dropout)               │ (None, 128)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_2 (Dense)                   │ (None, 10)               │         1,290 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 470,294 (1.79 MB) #>  Trainable params: 235,146 (918.54 KB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 235,148 (918.55 KB) mlp_fit |>    extract_keras_summary() |>    plot(show_shapes = TRUE) mlp_fit |>    extract_keras_history() |>    plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"evaluating-model-performance","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Evaluating Model Performance","title":"Getting Started with kerasnip","text":"keras_evaluate() function provides straightforward way assess model’s performance test set, using underlying keras3::evaluate() method. returns loss metrics specified model compilation step.","code":"mlp_fit |> keras_evaluate(x_test, y_test) #> 313/313 - 0s - 1ms/step - accuracy: 0.9830 - loss: 0.0867 #> $accuracy #> [1] 0.983 #>  #> $loss #> [1] 0.08669841"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"making-predictions","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Making Predictions","title":"Getting Started with kerasnip","text":"model trained, can use standard tidymodels predict() function generate predictions new data. default, predict() parsnip classification model returns predicted class labels. get underlying probabilities class, can set type = \"prob\". returns tibble probability column 10 classes (0-9). can compare predicted class actual class images see model performing.","code":"# Predict the class for the first 5 images in the test set  class_preds <- mlp_fit |>   predict(new_data = head(test_df)) #> 1/1 - 0s - 43ms/step class_preds #> # A tibble: 6 × 1 #>   .pred_class #>   <fct>       #> 1 7           #> 2 2           #> 3 1           #> 4 0           #> 5 4           #> 6 1 # Predict probabilities for the first 5 images prob_preds <- mlp_fit |> predict(new_data = head(test_df), type = \"prob\") #> 1/1 - 0s - 22ms/step prob_preds #> # A tibble: 6 × 10 #>     .pred_0   .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6  .pred_7 #>       <dbl>     <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>    <dbl> #> 1 4.25 e-19 3.49 e-15 3.71e-12 2.68e-11 4.76 e-18 5.35e-18 3.21e-26 1   e+ 0 #> 2 6.80 e-18 1.71 e-11 1   e+ 0 2.57e-13 7.32 e-32 1.06e-19 3.10e-19 4.60e-20 #> 3 5.87 e-12 1.000e+ 0 1.58e- 8 1.93e-10 5.03 e- 8 4.93e-10 1.18e- 8 5.35e- 8 #> 4 1.000e+ 0 1.39 e-14 8.26e- 9 1.10e-10 3.25 e-11 1.60e- 8 1.38e- 7 4.56e-10 #> 5 5.59 e-12 3.01 e-13 3.91e-11 1.53e-14 1.000e+ 0 6.79e-14 6.37e-13 4.37e- 8 #> 6 1.97 e-11 1.000e+ 0 1.06e- 8 2.30e- 9 3.39 e- 6 1.02e-10 2.75e- 9 8.61e- 7 #> # ℹ 2 more variables: .pred_8 <dbl>, .pred_9 <dbl> # Combine predictions with actuals for comparison comparison <- bind_cols(   class_preds,   prob_preds ) |>   bind_cols(     head(test_df[, \"y\", drop = FALSE])   ) comparison #> # A tibble: 6 × 12 #>   .pred_class   .pred_0   .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6 #>   <fct>           <dbl>     <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl> #> 1 7           4.25 e-19 3.49 e-15 3.71e-12 2.68e-11 4.76 e-18 5.35e-18 3.21e-26 #> 2 2           6.80 e-18 1.71 e-11 1   e+ 0 2.57e-13 7.32 e-32 1.06e-19 3.10e-19 #> 3 1           5.87 e-12 1.000e+ 0 1.58e- 8 1.93e-10 5.03 e- 8 4.93e-10 1.18e- 8 #> 4 0           1.000e+ 0 1.39 e-14 8.26e- 9 1.10e-10 3.25 e-11 1.60e- 8 1.38e- 7 #> 5 4           5.59 e-12 3.01 e-13 3.91e-11 1.53e-14 1.000e+ 0 6.79e-14 6.37e-13 #> 6 1           1.97 e-11 1.000e+ 0 1.06e- 8 2.30e- 9 3.39 e- 6 1.02e-10 2.75e- 9 #> # ℹ 4 more variables: .pred_7 <dbl>, .pred_8 <dbl>, .pred_9 <dbl>, y <fct>"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"example-2-tuning-the-model-architecture","dir":"Articles","previous_headings":"","what":"Example 2: Tuning the Model Architecture","title":"Getting Started with kerasnip","text":"Now ’ll showcase main strength kerasnip: tuning network architecture . can treat number layers, parameters layers, hyperparameters optimized tune. Using mnist_mlp spec just created, let’s define tunable model. Next, define search space tunable parameters using dials. Finally, can inspect results find architecture performed best. First, summary table: Now ’ve identified best-performing hyperparameters, final step create train final model. use select_best() get top parameters, finalize_workflow() update workflow , fit() one last time full training dataset. can now inspect final, tuned model.  result shows tune tested various network depths, widths, dropout rates, successfully finding best-performing combination within search space. using kerasnip, able integrate complex architectural tuning directly standard tidymodels workflow.","code":"# Define a tunable specification # We set num_hidden_2 = 0 to disable the second hidden block for this tuning example tune_spec <- mnist_mlp(   num_hidden_1 = tune(),   hidden_1_units = tune(),   hidden_1_rate = tune(),   num_hidden_2 = 0,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Create a workflow tune_wf <- workflow(y ~ x, tune_spec) # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     num_hidden_1 = dials::num_terms(c(1, 3)),     hidden_1_units = dials::hidden_units(c(64, 256)),     hidden_1_rate = dials::dropout(c(0.2, 0.4))   ) grid <- grid_regular(params, levels = 3) grid #> # A tibble: 27 × 3 #>    num_hidden_1 hidden_1_units hidden_1_rate #>           <int>          <int>         <dbl> #>  1            1             64           0.2 #>  2            2             64           0.2 #>  3            3             64           0.2 #>  4            1            160           0.2 #>  5            2            160           0.2 #>  6            3            160           0.2 #>  7            1            256           0.2 #>  8            2            256           0.2 #>  9            3            256           0.2 #> 10            1             64           0.3 #> # ℹ 17 more rows # Using only the first 100 rows for speed. The real call should be # folds <- vfold_cv(train_df, v = 3) folds <- vfold_cv(train_df[1:100,], v = 3)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   metrics = metric_set(accuracy),   control = control_grid(save_pred = FALSE, save_workflow = TRUE) ) #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 36ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 36ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 1s - 269ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step # Show the summary table of the best models show_best(tune_res, metric = \"accuracy\") #> # A tibble: 5 × 9 #>   num_hidden_1 hidden_1_units hidden_1_rate .metric  .estimator  mean     n #>          <int>          <int>         <dbl> <chr>    <chr>      <dbl> <int> #> 1            3            160           0.3 accuracy multiclass 0.790     3 #> 2            3            160           0.2 accuracy multiclass 0.790     3 #> 3            2            256           0.3 accuracy multiclass 0.790     3 #> 4            2            256           0.2 accuracy multiclass 0.780     3 #> 5            1            256           0.3 accuracy multiclass 0.780     3 #> # ℹ 2 more variables: std_err <dbl>, .config <chr> # Select the best hyperparameters best_hps <- select_best(tune_res, metric = \"accuracy\")  # Finalize the workflow with the best hyperparameters final_wf <- finalize_workflow(tune_wf, best_hps)  # Fit the final model on the full training data final_fit <- fit(final_wf, data = train_df) # Print the model summary final_fit |>   extract_fit_parsnip() |>   extract_keras_summary() #> Model: \"sequential_82\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense_246 (Dense)                 │ (None, 160)              │       125,600 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_164 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_247 (Dense)                 │ (None, 160)              │        25,760 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_165 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_248 (Dense)                 │ (None, 160)              │        25,760 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_166 (Dropout)             │ (None, 160)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_249 (Dense)                 │ (None, 10)               │         1,610 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 357,462 (1.36 MB) #>  Trainable params: 178,730 (698.16 KB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 178,732 (698.18 KB)  # Plot the training history final_fit |>    extract_fit_parsnip() |>   extract_keras_history() |>   plot()"},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Díaz. Author, maintainer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Díaz D (2025). kerasnip: Bridge Keras Tidymodels. R package version 0.0.0.9000.","code":"@Manual{,   title = {kerasnip: A Bridge Between Keras and Tidymodels},   author = {David Díaz},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"kerasnip","dir":"","previous_headings":"","what":"A Bridge Between Keras and Tidymodels","title":"A Bridge Between Keras and Tidymodels","text":"goal kerasnip provide seamless bridge keras tidymodels ecosystems. allows dynamic creation parsnip model specifications Keras models, making fully compatible tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Bridge Between Keras and Tidymodels","text":"can install development version kerasnip GitHub :","code":"# install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\")"},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-1-building-a-sequential-mlp","dir":"","previous_headings":"Example","what":"Example 1: Building a Sequential MLP","title":"A Bridge Between Keras and Tidymodels","text":"example shows core workflow building simple, linear stack layers using create_keras_sequential_spec().","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks # The first block initializes the model. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # Subsequent blocks add layers. dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } # The final block creates the output layer. output_block <- function(model) {   model |>     layer_dense(units = 1) }  # 2. Create a spec from the layer blocks # This creates a new model function, `basic_mlp()`, in your environment. create_keras_sequential_spec(   model_name = \"basic_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     output = output_block   ),   mode = \"regression\" )  # 3. Use the generated spec to define a model. # We can set the number of dense layers (`num_dense`) and their parameters (`dense_units`). spec <- basic_mlp(   num_dense = 2,   dense_units = 64,   fit_epochs = 10,   learn_rate = 0.01 ) |>   set_engine(\"keras\")  # 4. Fit the model within a tidymodels workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf <- workflow(rec, spec)  set.seed(123) fit_obj <- fit(wf, data = mtcars)  # 5. Make predictions predict(fit_obj, new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  21.3 #> 2  21.3 #> 3  22.8 #> 4  21.4 #> 5  18.7"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-2-building-a-functional-fork-join-model","dir":"","previous_headings":"Example","what":"Example 2: Building a Functional “Fork-Join” Model","title":"A Bridge Between Keras and Tidymodels","text":"complex, non-linear architectures, use create_keras_functional_spec(). example builds model input forked two paths, concatenated.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define blocks. For the functional API, blocks are nodes in a graph. input_block <- function(input_shape) layer_input(shape = input_shape) path_block <- function(tensor, units = 16) tensor |> layer_dense(units = units) concat_block <- function(input_a, input_b) layer_concatenate(list(input_a, input_b)) output_block <- function(tensor) layer_dense(tensor, units = 1)  # 2. Create the spec. The graph is defined by block names and their arguments. create_keras_functional_spec(   model_name = \"forked_mlp\",   layer_blocks = list(     main_input = input_block,     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),     # The output block must be named 'output'.     output = inp_spec(output_block, \"concatenated\")   ),   mode = \"regression\" )  # 3. Use the new spec. Arguments are prefixed with their block name. spec <- forked_mlp(path_a_units = 16, path_b_units = 8, fit_epochs = 10) |>   set_engine(\"keras\")  # Fit and predict as usual set.seed(123) fit(spec, mpg ~ ., data = mtcars) |>   predict(new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  19.4 #> 2  19.5 #> 3  21.9 #> 4  18.6 #> 5  17.9"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-3-tuning-a-sequential-mlp-architecture","dir":"","previous_headings":"Example","what":"Example 3: Tuning a Sequential MLP Architecture","title":"A Bridge Between Keras and Tidymodels","text":"example demonstrates tune number dense layers rate final dropout layer, showcasing tune architecture block hyperparameters simultaneously.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks for a tunable MLP input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } dropout_block <- function(model, rate = 0.2) {   model |> layer_dropout(rate = rate) } output_block <- function(model) {   model |> layer_dense(units = 1) }  # 2. Create a spec from the layer blocks create_keras_sequential_spec(   model_name = \"tunable_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     dropout = dropout_block,     output = output_block   ),   mode = \"regression\" )  # 3. Define a tunable model specification tune_spec <- tunable_mlp(   num_dense = tune(),   dense_units = tune(),   num_dropout = 1,   dropout_rate = tune(),   fit_epochs = 10 ) |>   set_engine(\"keras\")  # 4. Set up and run a tuning workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf_tune <- workflow(rec, tune_spec)  # Define the tuning grid. params <- extract_parameter_set_dials(wf_tune) |>   update(     num_dense = dials::num_terms(c(1, 3)),     dense_units = dials::hidden_units(c(8, 64)),     dropout_rate = dials::dropout(c(0.1, 0.5))   ) grid <- grid_regular(params, levels = 2)  # 5. Run the tuning set.seed(456) folds <- vfold_cv(mtcars, v = 3)  tune_res <- tune_grid(   wf_tune,   resamples = folds,   grid = grid,   control = control_grid(verbose = FALSE) )  # 6. Show the best architecture show_best(tune_res, metric = \"rmse\") #> # A tibble: 5 × 7 #>   num_dense dense_units dropout_rate .metric .estimator .mean .config               #>       <int>       <int>        <dbl> <chr>   <chr>      <dbl> <chr>                 #> 1         1          64          0.1 rmse    standard    2.92 Preprocessor1_Model02 #> 2         1          64          0.5 rmse    standard    3.02 Preprocessor1_Model08 #> 3         3          64          0.1 rmse    standard    3.15 Preprocessor1_Model04 #> 4         1           8          0.1 rmse    standard    3.20 Preprocessor1_Model01 #> 5         3           8          0.1 rmse    standard    3.22 Preprocessor1_Model03"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Functional API. allows creating complex, tunable architectures non-linear topologies integrate seamlessly tidymodels ecosystem.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"create_keras_functional_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"model_name character string name new model specification function (e.g., \"custom_resnet\"). valid R function name. layer_blocks named list functions function defines \"block\" (node) model graph. list names crucial define names nodes. arguments function define nodes connected. See \"Model Graph Connectivity\" section details. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., custom_resnet()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Functional API. ideal models complex, non-linear topologies, networks multiple inputs/outputs residual connections. function inspects arguments layer_blocks functions makes available tunable parameters generated model specification, prefixed block's name (e.g., dense_units). Common training parameters epochs learn_rate also added.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"model-graph-connectivity","dir":"Reference","previous_headings":"","what":"Model Graph Connectivity","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"kerasnip builds model's directed acyclic graph inspecting arguments function layer_blocks list. connection logic follows: names elements layer_blocks list define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. kerasnip automatically supply output tensors nodes calling my_block. two special requirements: Input Block: first block list treated input node. function take blocks input, can input_shape argument, supplied automatically fitting. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. key feature automatic creation num_{block_name} arguments (e.g., num_dense_path). allows control many times block repeated, making easy tune depth network. block can repeated exactly one input another block graph. new model specification function update() method created environment specified env argument.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   library(keras3)   library(parsnip)    # 1. Define block functions. These are the building blocks of our model.   # An input block that receives the data's shape automatically.   input_block <- function(input_shape) layer_input(shape = input_shape)    # A dense block with a tunable `units` parameter.   dense_block <- function(tensor, units) {     tensor |> layer_dense(units = units, activation = \"relu\")   }    # A block that adds two tensors together (for the residual connection).   add_block <- function(input_a, input_b) layer_add(list(input_a, input_b))    # An output block for regression.   output_block_reg <- function(tensor) layer_dense(tensor, units = 1)    # 2. Create the spec. The `layer_blocks` list defines the graph.   create_keras_functional_spec(     model_name = \"my_resnet_spec\",     layer_blocks = list(       # The names of list elements are the node names.       main_input = input_block,        # The argument `main_input` connects this block to the input node.       dense_path = function(main_input, units = 32) dense_block(main_input, units),        # This block's arguments connect it to the original input AND the dense layer.       add_residual = function(main_input, dense_path) add_block(main_input, dense_path),        # This block must be named 'output'. It connects to the residual add layer.       output = function(add_residual) output_block_reg(add_residual)     ),     mode = \"regression\"   )    # 3. Use the newly created specification function!   # The `dense_path_units` argument was created automatically.   model_spec <- my_resnet_spec(dense_path_units = 64, epochs = 10)    # You could also tune the number of dense layers since it has a single input:   # model_spec <- my_resnet_spec(num_dense_path = 2, dense_path_units = 32)    print(model_spec)   # tune::tunable(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Sequential API. ideal choice creating models simple, linear stack layers. models complex, non-linear topologies, see create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"create_keras_sequential_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"model_name character string name new model specification function (e.g., \"custom_cnn\"). valid R function name. layer_blocks named, ordered list functions. function defines \"block\" Keras layers. function must take Keras model object first argument return modified model. arguments function become tunable parameters final model specification. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., my_mlp()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Sequential API. function inspects arguments layer_blocks functions (ignoring special arguments like input_shape num_classes) makes available arguments generated model specification, prefixed block's name (e.g., dense_units). new model specification function update() method created environment specified env argument.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"model-architecture-sequential-api-","dir":"Reference","previous_headings":"","what":"Model Architecture (Sequential API)","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"kerasnip builds model applying functions layer_blocks order provided. function receives Keras model built previous function returns modified version. first block must initialize model (e.g., keras_model_sequential()). can accept input_shape argument, kerasnip provide automatically fitting. Subsequent blocks add layers model. final block add output layer. classification, can accept num_classes argument, provided automatically. key feature function automatic creation num_{block_name} arguments (e.g., num_hidden). allows control many times block repeated, making easy tune depth network.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) { library(keras3) library(parsnip) library(dials)  # 1. Define layer blocks for a complete model. # The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # A block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") }  # 2. Create the spec, providing blocks in the correct order. create_keras_sequential_spec( model_name = \"my_mlp\",   layer_blocks = list(     input = input_block,     hidden = hidden_block,     output = output_block   ),   mode = \"classification\" )  # 3. Use the newly created specification function! # Note the new arguments `num_hidden` and `hidden_units`. model_spec <- my_mlp(   num_hidden = 2,   hidden_units = 64,   epochs = 10,   learn_rate = 0.01 )  print(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Training History — extract_keras_history","title":"Extract Keras Training History — extract_keras_history","text":"Extracts returns training history Keras model fitted kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Training History — extract_keras_history","text":"","code":"extract_keras_history(object)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Training History — extract_keras_history","text":"object model_fit object produced kerasnip specification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Training History — extract_keras_history","text":"keras_training_history containing training history (metrics per epoch).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Model Summary — extract_keras_summary","title":"Extract Keras Model Summary — extract_keras_summary","text":"Extracts returns summary Keras model fitted kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Model Summary — extract_keras_summary","text":"","code":"extract_keras_summary(object, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Model Summary — extract_keras_summary","text":"object model_fit object produced kerasnip specification. ... Additional arguments passed keras3::summary().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Model Summary — extract_keras_summary","text":"character vector, element line model summary.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"function internal engine fitting models generated create_keras_functional_spec(). intended called directly user.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"","code":"generic_functional_fit(x, y, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"x data frame matrix predictors. y vector outcomes. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)). epochs integer number training iterations. learn_rate double learning rate, used configure default Adam optimizer. batch_size integer number samples per gradient update. tunable parameter passed keras3::fit(). validation_split proportion training data use validation set. verbose integer verbosity fitting process (0, 1, 2).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"function performs following key steps: Argument & Data Preparation: resolves arguments passed parsnip (handling rlang_zap objects unspecified arguments) prepares x y data Keras. automatically determines input_shape x , classification, num_classes y. Dynamic Model Construction: builds Keras model graph processing layer_blocks list. Connectivity: graph connected matching argument names block function names previously defined blocks. example, block function(input_a, ...) receive output tensor block named input_a. Repetition: checks num_{block_name} arguments repeat block multiple times, creating chain identical layers. block can repeated exactly one input tensor another block. Model Compilation: compiles final Keras model. compilation arguments (optimizer, loss, metrics) can customized passing arguments prefixed compile_ (e.g., compile_loss = \"mae\"). Model Fitting: calls keras3::fit() train model prepared data.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"function internal engine fitting models generated create_keras_sequential_spec(). intended called directly user.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"","code":"generic_sequential_fit(x, y, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"x data frame matrix predictors. y vector outcomes. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)). epochs integer number training iterations. learn_rate double learning rate, used configure default Adam optimizer. batch_size integer number samples per gradient update. tunable parameter passed keras3::fit(). validation_split proportion training data use validation set. verbose integer verbosity fitting process (0, 1, 2).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"function performs following key steps: Argument & Data Preparation: resolves arguments passed parsnip (handling rlang_zap objects unspecified arguments) prepares x y data Keras. automatically determines input_shape x , classification, num_classes y. Dynamic Model Construction: builds Keras model sequentially processing layer_blocks list. first block function must initialize model, typically calling keras3::keras_model_sequential(). checks num_{block_name} arguments repeat block multiple times, creating deeper stack layers. Model Compilation: compiles final Keras model. compilation arguments (optimizer, loss, metrics) can customized passing arguments prefixed compile_ (e.g., compile_loss = \"mae\"). Model Fitting: calls keras3::fit() train model prepared data.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remap Layer Block Arguments for Model Specification — inp_spec","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"Creates wrapper function around Keras layer block rename arguments. powerful helper defining layer_blocks create_keras_functional_spec() create_keras_sequential_spec(), allowing connect reusable blocks model graph without writing verbose anonymous functions.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"inp_spec(block, input_map)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"block function defines Keras layer set layers. first arguments input tensor(s). input_map single character string named character vector specifies rename/remap arguments block.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"new function (closure) wraps block function renamed arguments, ready used layer_blocks list.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"inp_spec() makes model definitions cleaner readable. handles metaprogramming required create new function correct argument names, preserving original block's hyperparameters default values. function supports two modes operation based input_map: Single Input Renaming: input_map single character string, wrapper function renames first argument block function provided string. common case blocks take single tensor input. Multiple Input Mapping: input_map named character vector, provides explicit mapping new argument names (names vector) original argument names block function (values vector). used blocks multiple inputs, like concatenation layer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"if (FALSE) { # \\dontrun{ # --- Example Blocks --- # A standard dense block with one input tensor and one hyperparameter. dense_block <- function(tensor, units = 16) {   tensor |> keras3::layer_dense(units = units, activation = \"relu\") }  # A block that takes two tensors as input. concat_block <- function(input_a, input_b) {   keras3::layer_concatenate(list(input_a, input_b)) }  # An output block with one input. output_block <- function(tensor) {   tensor |> keras3::layer_dense(units = 1) }  # --- Usage --- layer_blocks <- list(   main_input = keras3::layer_input,   path_a = inp_spec(dense_block, \"main_input\"),   path_b = inp_spec(dense_block, \"main_input\"),   concatenated = inp_spec(     concat_block,     c(path_a = \"input_a\", path_b = \"input_b\")   ),   output = inp_spec(output_block, \"concatenated\") ) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate a Kerasnip Model — keras_evaluate","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"function provides kera_evaluate() method model_fit objects created kerasnip. preprocesses data format expected Keras calls keras3::evaluate() underlying model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"","code":"keras_evaluate(object, x, y = NULL, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"object model_fit object produced kerasnip specification. x data frame matrix predictors. y vector data frame outcomes. ... Additional arguments passed keras3::evaluate().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"list evaluation results","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Dynamically Discovered Keras Objects — keras_objects","title":"Dynamically Discovered Keras Objects — keras_objects","text":"exported vectors contain names optimizers, losses, metrics discovered installed keras3 package kerasnip loaded. ensures kerasnip always --date Keras version.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dynamically Discovered Keras Objects — keras_objects","text":"","code":"keras_optimizers  keras_losses  keras_metrics"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dynamically Discovered Keras Objects — keras_objects","text":"object class character length 12. object class character length 21. object class character length 32.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dynamically Discovered Keras Objects — keras_objects","text":"objects primarily used provide default values dials parameter functions, optimizer_function() loss_function_keras(). allows tab-completion IDEs validation optimizer loss names tuning models. discovery process .onLoad() scrapes keras3 namespace functions matching optimizer_*, loss_*, metric_* patterns.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Loss Functions — loss_function_keras","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"Dials Parameter Keras Loss Functions","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"","code":"loss_function_keras(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"values character vector possible loss functions. Defaults known losses (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Optimizers — optimizer_function","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"Dials Parameter Keras Optimizers","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"","code":"optimizer_function(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"values character vector possible optimizers. Defaults known optimizers (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Predictor Input for Keras — process_x","title":"Process Predictor Input for Keras — process_x","text":"Preprocesses predictor data (x) format suitable Keras models. Handles tabular data list-columns arrays (e.g., images).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Predictor Input for Keras — process_x","text":"","code":"process_x(x)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Predictor Input for Keras — process_x","text":"x data frame matrix predictors.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Predictor Input for Keras — process_x","text":"list containing: x_proc: processed predictor data (matrix array). input_shape: determined input shape Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Outcome Input for Keras — process_y","title":"Process Outcome Input for Keras — process_y","text":"Preprocesses outcome data (y) format suitable Keras models. Handles regression (numeric) classification (factor) outcomes, including one-hot encoding classification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Outcome Input for Keras — process_y","text":"","code":"process_y(y, is_classification = NULL, class_levels = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Outcome Input for Keras — process_y","text":"y vector outcomes. is_classification Logical, optional. TRUE, treats y classification. FALSE, treats regression. NULL (default), determined .factor(y). class_levels Character vector, optional. factor levels classification outcomes. NULL (default), determined levels(y).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Outcome Input for Keras — process_y","text":"list containing: y_proc: processed outcome data (matrix one-hot encoded array). is_classification: Logical, indicating y treated classification. num_classes: Integer, number classes classification, NULL. class_levels: Character vector, factor levels classification, NULL.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Loss — register_keras_loss","title":"Register a Custom Keras Loss — register_keras_loss","text":"Allows users register custom loss function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Loss — register_keras_loss","text":"","code":"register_keras_loss(name, loss_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Loss — register_keras_loss","text":"name name register loss (character). loss_fn loss function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Loss — register_keras_loss","text":"Registered losses stored internal environment. model compiled, kerasnip first check internal registry loss matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Metric — register_keras_metric","title":"Register a Custom Keras Metric — register_keras_metric","text":"Allows users register custom metric function can used name within kerasnip model specifications.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Metric — register_keras_metric","text":"","code":"register_keras_metric(name, metric_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Metric — register_keras_metric","text":"name name register metric (character). metric_fn metric function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Metric — register_keras_metric","text":"Registered metrics stored internal environment. model compiled, kerasnip first check internal registry metric matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Optimizer — register_keras_optimizer","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Allows users register custom optimizer function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"register_keras_optimizer(name, optimizer_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"name name register optimizer (character). optimizer_fn optimizer function. return Keras optimizer object.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Registered optimizers stored internal environment. model compiled, kerasnip first check internal registry optimizer matching provided name checking keras3 package. optimizer_fn can simple function partially applied function using purrr::partial(). useful creating versions Keras optimizers specific settings.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"if (requireNamespace(\"keras3\", quietly = TRUE)) {   # Register a custom version of Adam with a different default beta_1   my_adam <- purrr::partial(keras3::optimizer_adam, beta_1 = 0.8)   register_keras_optimizer(\"my_adam\", my_adam)    # Now \"my_adam\" can be used as a string in a model spec, e.g.,   # my_model_spec(compile_optimizer = \"my_adam\") }"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function completely removes model specification previously created create_keras_sequential_spec() create_keras_functional_spec(). cleans function user's environment associated registrations within parsnip package.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"remove_keras_spec(model_name, env = parent.frame())"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"model_name character string giving name model specification function remove (e.g., \"my_mlp\"). env environment remove function update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"Invisibly returns TRUE attempting remove objects.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function essential cleanly unloading dynamically created model. performs three main actions: removes model specification function (e.g., my_mlp()) corresponding update() method specified environment. searches parsnip's internal model environment objects whose names start model_name removes . purges fit methods, argument definitions, registrations. removes model's name parsnip's master list models. function uses un-exported parsnip:::get_model_env() perform cleanup, may subject change future parsnip versions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   # First, create a dummy spec   input_block <- function(model, input_shape) keras3::keras_model_sequential(input_shape = input_shape)   dense_block <- function(model, units = 16) model |> keras3::layer_dense(units = units)   create_keras_sequential_spec(\"my_temp_model\", list(input = input_block, dense = dense_block), \"regression\")    # Check it exists in the environment and in parsnip   exists(\"my_temp_model\")   \"my_temp_model\" %in% parsnip::show_engines(\"my_temp_model\")$model    # Now remove it   remove_keras_spec(\"my_temp_model\")    # Check it's gone   !exists(\"my_temp_model\")   !\"my_temp_model\" %in% parsnip::show_engines(NULL)$model } } # }"},{"path":"https://davidrsch.github.io/kerasnip/news/index.html","id":"kerasnip-0009000","dir":"Changelog","previous_headings":"","what":"kerasnip 0.0.0.9000","title":"kerasnip 0.0.0.9000","text":"Initial development version. Added create_keras_spec() generate parsnip specifications dynamically.","code":""}]
