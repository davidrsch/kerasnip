[{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement daviddrsch@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kerasnip","title":"Contributing to kerasnip","text":"outlines propose change kerasnip. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kerasnip","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to kerasnip","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to kerasnip","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use air package apply styles. can format code automatically commenting /style PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kerasnip","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kerasnip authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://davidrsch.github.io/kerasnip/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with kerasnip","title":"Getting help with kerasnip","text":"Thanks using kerasnip. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"when-to-use-the-functional-api","dir":"Articles","previous_headings":"","what":"When to Use the Functional API","title":"Building Functional Models with kerasnip","text":"create_keras_sequential_spec() perfect models simple, linear stack layers, many advanced architectures linear. Keras Functional API designed cases. use create_keras_functional_spec() model : Multiple input multiple output layers. Shared layers different branches. Residual connections (e.g., ResNets), layer’s input added output. non-linear topology. kerasnip makes easy define architectures automatically connecting graph layer blocks.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"the-core-concept-building-a-graph","dir":"Articles","previous_headings":"","what":"The Core Concept: Building a Graph","title":"Building Functional Models with kerasnip","text":"kerasnip builds model’s graph inspecting layer_blocks provide. connection logic simple powerful: names list elements layer_blocks define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. two special requirements: Input Block: first block list treated main input node. function take blocks input. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. Let’s see action.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"example-1-a-fork-join-regression-model","dir":"Articles","previous_headings":"","what":"Example 1: A Fork-Join Regression Model","title":"Building Functional Models with kerasnip","text":"build model forks input, passes two separate dense layer paths, joins results concatenation layer producing final prediction.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-1-load-libraries","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 1: Load Libraries","title":"Building Functional Models with kerasnip","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom        1.0.9     ✔ recipes      1.3.1 ## ✔ dials        1.4.1     ✔ rsample      1.3.1 ## ✔ dplyr        1.1.4     ✔ tibble       3.3.0 ## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 ## ✔ infer        1.0.9     ✔ tune         1.3.0 ## ✔ modeldata    1.5.0     ✔ workflows    1.2.0 ## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 ## ✔ purrr        1.1.0     ✔ yardstick    1.3.2 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter()  masks stats::filter() ## ✖ dplyr::lag()     masks stats::lag() ## ✖ recipes::step()  masks stats::step() library(keras3) ##  ## Attaching package: 'keras3' ## The following object is masked from 'package:yardstick': ##  ##     get_weights # Silence the startup messages from remove_keras_spec options(kerasnip.show_removal_messages = FALSE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-2-define-layer-blocks","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 2: Define Layer Blocks","title":"Building Functional Models with kerasnip","text":"building blocks model. function represents node graph.","code":"# The input node. `input_shape` is supplied automatically by the engine. input_block <- function(input_shape) {   layer_input(shape = input_shape) }  # A generic block for a dense path. `units` will be a tunable parameter. path_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") }  # A block to join two tensors. concat_block <- function(input_a, input_b) {   layer_concatenate(list(input_a, input_b)) }  # The final output block for regression. output_block_reg <- function(tensor) {   layer_dense(tensor, units = 1) }"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-3-create-the-model-specification","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 3: Create the Model Specification","title":"Building Functional Models with kerasnip","text":"Now assemble blocks graph. use inp_spec() helper connect blocks. avoids writing verbose anonymous functions like function(main_input, units) path_block(main_input, units). inp_spec() automatically creates wrapper renames arguments blocks match node names layer_blocks list.","code":"model_name <- \"forked_reg_spec\" # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(model_name), add = TRUE)  create_keras_functional_spec(   model_name = model_name,   layer_blocks = list(     # Node names are defined by the list names     main_input = input_block,      # `inp_spec()` renames the first argument of `path_block` ('tensor')     # to 'main_input' to match the node name.     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),      # For multiple inputs, `inp_spec()` takes a named vector to map     # new argument names to the original block's argument names.     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),      # The output block takes the concatenated tensor as its input.     output = inp_spec(output_block_reg, \"concatenated\")   ),   mode = \"regression\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-4-use-and-fit-the-model","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 4: Use and Fit the Model","title":"Building Functional Models with kerasnip","text":"new function forked_reg_spec() now available. arguments (path_a_units, path_b_units) discovered automatically block definitions.","code":"# We can override the default `units` from `path_block` for each path. spec <- forked_reg_spec(   path_a_units = 16,   path_b_units = 8,   fit_epochs = 10,   fit_verbose = 0 # Suppress fitting output in vignette ) |>   set_engine(\"keras\")  print(spec) ## forked reg spec Model Specification (regression) ##  ## Main Arguments: ##   num_main_input = structure(list(), class = \"rlang_zap\") ##   num_path_a = structure(list(), class = \"rlang_zap\") ##   num_path_b = structure(list(), class = \"rlang_zap\") ##   num_concatenated = structure(list(), class = \"rlang_zap\") ##   num_output = structure(list(), class = \"rlang_zap\") ##   path_a_units = 16 ##   path_b_units = 8 ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 10 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = 0 ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras # Fit the model on the mtcars dataset rec <- recipe(mpg ~ ., data = mtcars) wf <- workflow() |>    add_recipe(rec) |>   add_model(spec)   fit_obj <- fit(wf, data = mtcars)  predict(fit_obj, new_data = mtcars[1:5, ]) ## 1/1 - 0s - 39ms/step ## # A tibble: 5 × 1 ##   .pred ##   <dbl> ## 1  24.8 ## 2  24.9 ## 3  20.2 ## 4  24.0 ## 5  37.4"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"example-2-tuning-a-functional-models-depth","dir":"Articles","previous_headings":"","what":"Example 2: Tuning a Functional Model’s Depth","title":"Building Functional Models with kerasnip","text":"key feature kerasnip ability tune depth network repeating block multiple times. block can repeated exactly one input tensor another block graph. Let’s create simple functional model tune width (units) depth (num_...).","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-1-define-blocks-and-create-spec","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 1: Define Blocks and Create Spec","title":"Building Functional Models with kerasnip","text":"model architecturally sequential, build functional API demonstrate repetition feature.","code":"dense_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") } output_block_class <- function(tensor, num_classes) {   tensor |> layer_dense(units = num_classes, activation = \"softmax\") }  model_name_tune <- \"tunable_func_mlp\" on.exit(remove_keras_spec(model_name_tune), add = TRUE)  create_keras_functional_spec(   model_name = model_name_tune,   layer_blocks = list(     main_input = input_block,     # This block has a single input ('main_input'), so it can be repeated.     dense_path = inp_spec(dense_block, \"main_input\"),     output = inp_spec(output_block_class, \"dense_path\")   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"step-2-set-up-and-run-tuning","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 2: Set up and Run Tuning","title":"Building Functional Models with kerasnip","text":"tune dense_path_units (width) num_dense_path (depth). num_dense_path argument created automatically dense_path repeatable block. results show tidymodels successfully trained evaluated models different numbers hidden layers, demonstrating can tune architecture network.","code":"tune_spec <- tunable_func_mlp(   dense_path_units = tune(),   num_dense_path = tune(),   fit_epochs = 5,   fit_verbose = 0 ) |>   set_engine(\"keras\")  rec <- recipe(Species ~ ., data = iris) tune_wf <- workflow() |>    add_recipe(rec) |>   add_model(tune_spec)  folds <- vfold_cv(iris, v = 2)  # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     dense_path_units = hidden_units(c(8, 32)),     num_dense_path = num_terms(c(1, 3)) # Test models with 1, 2, or 3 hidden layers   )  grid <- grid_regular(params, levels = 2) grid ## # A tibble: 4 × 2 ##   num_dense_path dense_path_units ##            <int>            <int> ## 1              1                8 ## 2              3                8 ## 3              1               32 ## 4              3               32 control <- control_grid(save_pred = FALSE, verbose = FALSE)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   control = control ) ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 8ms/step show_best(tune_res, metric = \"accuracy\") ## # A tibble: 4 × 8 ##   num_dense_path dense_path_units .metric .estimator  mean     n std_err .config ##            <int>            <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   ## 1              3               32 accura… multiclass 0.933     2  0.0667 Prepro… ## 2              1               32 accura… multiclass 0.787     2  0.0667 Prepro… ## 3              1                8 accura… multiclass 0.507     2  0.173  Prepro… ## 4              3                8 accura… multiclass 0.473     2  0.18   Prepro…"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional-api.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Building Functional Models with kerasnip","text":"create_keras_functional_spec() function provides powerful intuitive way define, fit, tune complex Keras models within tidymodels framework. defining model graph connected blocks, can represent nearly architecture kerasnip handles boilerplate integrating parsnip, dials, tune.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"the-core-idea-from-keras-layers-to-tidymodels-specs","dir":"Articles","previous_headings":"","what":"The Core Idea: From Keras Layers to Tidymodels Specs","title":"Getting Started with kerasnip","text":"keras3 package allows building deep learning models layer--layer, powerful flexible approach. However, tidymodels ecosystem designed around declarative model specifications, define model want parameters want tune, rather building imperatively. kerasnip bridges gap simple powerful concept: layer blocks. define components neural network (e.g., input block, dense block, dropout block) simple R functions. kerasnip uses blocks building materials create brand new parsnip model specification function . new function behaves just like parsnip model (e.g., rand_forest() linear_reg()), making easy integrate workflows tune tune. ’ll start loading kerasnip, tidymodels keras3:","code":"library(kerasnip) library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom        1.0.9     ✔ recipes      1.3.1 ## ✔ dials        1.4.1     ✔ rsample      1.3.1 ## ✔ dplyr        1.1.4     ✔ tibble       3.3.0 ## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 ## ✔ infer        1.0.9     ✔ tune         1.3.0 ## ✔ modeldata    1.5.0     ✔ workflows    1.2.0 ## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 ## ✔ purrr        1.1.0     ✔ yardstick    1.3.2 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter()  masks stats::filter() ## ✖ dplyr::lag()     masks stats::lag() ## ✖ recipes::step()  masks stats::step() library(keras3) ##  ## Attaching package: 'keras3' ## The following object is masked from 'package:yardstick': ##  ##     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"example-1-building-and-fitting-a-basic-mlp","dir":"Articles","previous_headings":"","what":"Example 1: Building and Fitting a Basic MLP","title":"Getting Started with kerasnip","text":"Let’s start building simple Multi-Layer Perceptron (MLP) regression task using mtcars dataset.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-1-define-the-layer-blocks","dir":"Articles","previous_headings":"Example 1: Building and Fitting a Basic MLP","what":"Step 1: Define the Layer Blocks","title":"Getting Started with kerasnip","text":"need three blocks: 1. input block initialize model define input shape. kerasnip automatically pass input_shape argument fitting. 2. dense block hidden layers. ’ll give units argument can control number neurons. 3. output block final prediction. regression, typically single neuron linear activation.","code":"# 1. The input block must initialize the model.  # input_shape is passed automatically by the fit engine.  mlp_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape)  }  # 2. A block for hidden layers. units will become a tunable parameter.  mlp_dense_block <- function(model, units = 32) {   model |>     layer_dense(units = units, activation = \"relu\")  }  # 3. The output block for a regression model.  mlp_output_block <- function(model) {   model |>     layer_dense(units = 1)  }"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-2-create-the-model-specification","dir":"Articles","previous_headings":"Example 1: Building and Fitting a Basic MLP","what":"Step 2: Create the Model Specification","title":"Getting Started with kerasnip","text":"Now, use create_keras_sequential_spec() generate new model function, ’ll call basic_mlp(). provide layer blocks order assembled. function call side-effect: new function basic_mlp() now available environment! Notice arguments: kerasnipautomatically created num_dense (control number dense layers) dense_units (units argument mlp_dense_block).","code":"create_keras_sequential_spec(   model_name = \"basic_mlp\",   layer_blocks = list(     input = mlp_input_block,     dense = mlp_dense_block,     output = mlp_output_block   ),   mode = \"regression\"  )"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-3-use-the-spec-in-a-workflow","dir":"Articles","previous_headings":"Example 1: Building and Fitting a Basic MLP","what":"Step 3: Use the Spec in a Workflow","title":"Getting Started with kerasnip","text":"can now use basic_mlp() like parsnip model. Let’s define model two hidden layers, 64 units, train 50 epochs. ’ll use simple recipe normalize predictors combine model spec workflow.","code":"spec <- basic_mlp(   num_dense = 2,   dense_units = 64,   fit_epochs = 50,   learn_rate = 0.01  ) |>   set_engine(\"keras\")  print(spec) ## basic mlp Model Specification (regression) ##  ## Main Arguments: ##   num_input = structure(list(), class = \"rlang_zap\") ##   num_dense = 2 ##   num_output = structure(list(), class = \"rlang_zap\") ##   dense_units = 64 ##   learn_rate = 0.01 ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 50 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = structure(list(), class = \"rlang_zap\") ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras # Suppress verbose Keras output for the vignette  options(keras.fit_verbose = 0)    rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf <- workflow() |>   add_recipe(rec) |>   add_model(spec)  set.seed(123)  fit_obj <- fit(wf, data = mtcars)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-4-make-predictions","dir":"Articles","previous_headings":"Example 1: Building and Fitting a Basic MLP","what":"Step 4: Make Predictions","title":"Getting Started with kerasnip","text":"Predictions work just ’d expect tidymodels.","code":"predictions <- predict(fit_obj, new_data = mtcars[1:5, ]) ## 1/1 - 0s - 42ms/step print(predictions) ## # A tibble: 5 × 1 ##   .pred ##   <dbl> ## 1  19.7 ## 2  18.2 ## 3  24.4 ## 4  18.3 ## 5  17.9"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"example-2-tuning-the-model-architecture","dir":"Articles","previous_headings":"","what":"Example 2: Tuning the Model Architecture","title":"Getting Started with kerasnip","text":"real power kerasnip comes ability tune just hyperparameters (like learning rate dropout), architecture network . Let’s create complex tunable specification let tune find optimal number dense layers, number units layers, rate final dropout layer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-1-define-blocks-and-create-a-new-spec","dir":"Articles","previous_headings":"Example 2: Tuning the Model Architecture","what":"Step 1: Define Blocks and Create a New Spec","title":"Getting Started with kerasnip","text":"First, ’ll define additional block dropout create new model specification, tunable_mlp, includes .","code":"tunable_dropout_block <- function(model, rate = 0.2) {   model |>     layer_dropout(rate = rate) }  create_keras_sequential_spec(   model_name = \"tunable_mlp\",   layer_blocks = list(     input = mlp_input_block,     dense = mlp_dense_block,     dropout = tunable_dropout_block,     output = mlp_output_block   ),   mode = \"regression\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-2-define-a-tunable-specification","dir":"Articles","previous_headings":"Example 2: Tuning the Model Architecture","what":"Step 2: Define a Tunable Specification","title":"Getting Started with kerasnip","text":"use new tunable_mlp() function, passing tune() arguments want optimize. one dropout layer output.","code":"tune_spec <- tunable_mlp(   num_dense = tune(),   dense_units = tune(),   num_dropout = 1,   dropout_rate = tune(),   fit_epochs = 20 ) |>   set_engine(\"keras\")  print(tune_spec) ## tunable mlp Model Specification (regression) ##  ## Main Arguments: ##   num_input = structure(list(), class = \"rlang_zap\") ##   num_dense = tune() ##   num_dropout = 1 ##   num_output = structure(list(), class = \"rlang_zap\") ##   dense_units = tune() ##   dropout_rate = tune() ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 20 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = structure(list(), class = \"rlang_zap\") ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-3-set-up-the-tuning-grid","dir":"Articles","previous_headings":"Example 2: Tuning the Model Architecture","what":"Step 3: Set up the Tuning Grid","title":"Getting Started with kerasnip","text":"create workflow . , can use helper functions dials define search space parameters.","code":"tune_wf <- workflow() |>   add_recipe(rec) |>   add_model(tune_spec)  # Define the tuning grid.  # `num_terms()` is the dials function for `num_*` parameters. # `hidden_units()` is the dials function for `*_units` parameters. params <- extract_parameter_set_dials(tune_wf) |>   update(     num_dense = dials::num_terms(c(1, 3)),     dense_units = dials::hidden_units(c(8, 64)),     dropout_rate = dials::dropout(c(0.1, 0.5))   ) grid <- grid_regular(params, levels = 2)  print(grid) ## # A tibble: 8 × 3 ##   num_dense dense_units dropout_rate ##       <int>       <int>        <dbl> ## 1         1           8          0.1 ## 2         3           8          0.1 ## 3         1          64          0.1 ## 4         3          64          0.1 ## 5         1           8          0.5 ## 6         3           8          0.5 ## 7         1          64          0.5 ## 8         3          64          0.5"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-4-run-the-tuning","dir":"Articles","previous_headings":"Example 2: Tuning the Model Architecture","what":"Step 4: Run the Tuning","title":"Getting Started with kerasnip","text":"use tune_grid() resamples evaluate combination architectural parameters.","code":"set.seed(456)  folds <- vfold_cv(mtcars, v = 3)    # The control argument is used to prevent saving predictions, which  # can be large for Keras models.  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   control = control_grid(save_pred = FALSE)  ) ## 1/1 - 0s - 34ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 43ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 43ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 40ms/step ## 1/1 - 0s - 34ms/step ## 1/1 - 0s - 41ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 40ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 40ms/step ## 1/1 - 0s - 33ms/step ## 1/1 - 0s - 41ms/step"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"step-5-analyze-the-results","dir":"Articles","previous_headings":"Example 2: Tuning the Model Architecture","what":"Step 5: Analyze the Results","title":"Getting Started with kerasnip","text":"can now see architecture performed best. results show tune successfully tested different network depths (num_dense), widths (dense_units), dropout rates find best-performing combination. demonstrates kerasnip seamlessly integrates complex architectural tuning standard tidymodels workflow.","code":"show_best(tune_res, metric = \"rmse\") ## # A tibble: 5 × 9 ##   num_dense dense_units dropout_rate .metric .estimator  mean     n std_err ##       <int>       <int>        <dbl> <chr>   <chr>      <dbl> <int>   <dbl> ## 1         3          64          0.1 rmse    standard    4.14     3   0.236 ## 2         3          64          0.5 rmse    standard    4.49     3   0.756 ## 3         1          64          0.1 rmse    standard   10.9      3   0.472 ## 4         1          64          0.5 rmse    standard   10.9      3   1.10  ## 5         3           8          0.5 rmse    standard   15.4      3   2.86  ## # ℹ 1 more variable: .config <chr>"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting-started.html","id":"advanced-customization","dir":"Articles","previous_headings":"","what":"Advanced Customization","title":"Getting Started with kerasnip","text":"kerasnip provides clean API passing arguments directly Keras’s compile() fit() methods. Compile Arguments: Pass argument keras3::compile() prefixing compile_. example, change loss function use compile_loss = \"mae\". Fit Arguments: Pass argument keras3::fit() prefixing fit_. example, set validation split add callback, use fit_validation_split = 0.2 fit_callbacks = list(...). example using arguments specify different loss function, validation split, early stopping callback. system gives full control Keras training process keeping model specification function signature clean focused tunable parameters.","code":"adv_spec <- basic_mlp(   num_dense = 2,   dense_units = 32,   fit_epochs = 100,   # Arguments for keras3::compile()   compile_loss = \"mae\",   # Arguments for keras3::fit()   fit_validation_split = 0.2,   fit_callbacks = list(     keras3::callback_early_stopping(patience = 5)   ) ) |>   set_engine(\"keras\")  print(adv_spec) ## basic mlp Model Specification (regression) ##  ## Main Arguments: ##   num_input = structure(list(), class = \"rlang_zap\") ##   num_dense = 2 ##   num_output = structure(list(), class = \"rlang_zap\") ##   dense_units = 32 ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 100 ##   fit_callbacks = list(keras3::callback_early_stopping(patience = 5)) ##   fit_validation_split = 0.2 ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = structure(list(), class = \"rlang_zap\") ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = mae ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras"},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Díaz. Author, maintainer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Díaz D (2025). kerasnip: Bridge Keras Tidymodels. R package version 0.0.0.9000.","code":"@Manual{,   title = {kerasnip: A Bridge Between Keras and Tidymodels},   author = {David Díaz},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"kerasnip","dir":"","previous_headings":"","what":"A Bridge Between Keras and Tidymodels","title":"A Bridge Between Keras and Tidymodels","text":"goal kerasnip provide seamless bridge keras tidymodels ecosystems. allows dynamic creation parsnip model specifications Keras models, making fully compatible tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Bridge Between Keras and Tidymodels","text":"can install development version kerasnip GitHub :","code":"# install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\")"},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-1-building-a-sequential-mlp","dir":"","previous_headings":"Example","what":"Example 1: Building a Sequential MLP","title":"A Bridge Between Keras and Tidymodels","text":"example shows core workflow building simple, linear stack layers using create_keras_sequential_spec().","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks # The first block initializes the model. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # Subsequent blocks add layers. dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } # The final block creates the output layer. output_block <- function(model) {   model |>     layer_dense(units = 1) }  # 2. Create a spec from the layer blocks # This creates a new model function, `basic_mlp()`, in your environment. create_keras_sequential_spec(   model_name = \"basic_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     output = output_block   ),   mode = \"regression\" )  # 3. Use the generated spec to define a model. # We can set the number of dense layers (`num_dense`) and their parameters (`dense_units`). spec <- basic_mlp(   num_dense = 2,   dense_units = 64,   fit_epochs = 10,   learn_rate = 0.01 ) |>   set_engine(\"keras\")  # 4. Fit the model within a tidymodels workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf <- workflow(rec, spec)  set.seed(123) fit_obj <- fit(wf, data = mtcars)  # 5. Make predictions predict(fit_obj, new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  21.3 #> 2  21.3 #> 3  22.8 #> 4  21.4 #> 5  18.7"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-2-building-a-functional-fork-join-model","dir":"","previous_headings":"Example","what":"Example 2: Building a Functional “Fork-Join” Model","title":"A Bridge Between Keras and Tidymodels","text":"complex, non-linear architectures, use create_keras_functional_spec(). example builds model input forked two paths, concatenated.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define blocks. For the functional API, blocks are nodes in a graph. input_block <- function(input_shape) layer_input(shape = input_shape) path_block <- function(tensor, units = 16) tensor |> layer_dense(units = units) concat_block <- function(input_a, input_b) layer_concatenate(list(input_a, input_b)) output_block <- function(tensor) layer_dense(tensor, units = 1)  # 2. Create the spec. The graph is defined by block names and their arguments. create_keras_functional_spec(   model_name = \"forked_mlp\",   layer_blocks = list(     main_input = input_block,     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),     # The output block must be named 'output'.     output = inp_spec(output_block, \"concatenated\")   ),   mode = \"regression\" )  # 3. Use the new spec. Arguments are prefixed with their block name. spec <- forked_mlp(path_a_units = 16, path_b_units = 8, fit_epochs = 10) |>   set_engine(\"keras\")  # Fit and predict as usual set.seed(123) fit(spec, mpg ~ ., data = mtcars) |>   predict(new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  19.4 #> 2  19.5 #> 3  21.9 #> 4  18.6 #> 5  17.9"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-3-tuning-a-sequential-mlp-architecture","dir":"","previous_headings":"Example","what":"Example 3: Tuning a Sequential MLP Architecture","title":"A Bridge Between Keras and Tidymodels","text":"example demonstrates tune number dense layers rate final dropout layer, showcasing tune architecture block hyperparameters simultaneously.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks for a tunable MLP input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } dropout_block <- function(model, rate = 0.2) {   model |> layer_dropout(rate = rate) } output_block <- function(model) {   model |> layer_dense(units = 1) }  # 2. Create a spec from the layer blocks create_keras_sequential_spec(   model_name = \"tunable_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     dropout = dropout_block,     output = output_block   ),   mode = \"regression\" )  # 3. Define a tunable model specification tune_spec <- tunable_mlp(   num_dense = tune(),   dense_units = tune(),   num_dropout = 1,   dropout_rate = tune(),   fit_epochs = 10 ) |>   set_engine(\"keras\")  # 4. Set up and run a tuning workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf_tune <- workflow(rec, tune_spec)  # Define the tuning grid. params <- extract_parameter_set_dials(wf_tune) |>   update(     num_dense = dials::num_terms(c(1, 3)),     dense_units = dials::hidden_units(c(8, 64)),     dropout_rate = dials::dropout(c(0.1, 0.5))   ) grid <- grid_regular(params, levels = 2)  # 5. Run the tuning set.seed(456) folds <- vfold_cv(mtcars, v = 3)  tune_res <- tune_grid(   wf_tune,   resamples = folds,   grid = grid,   control = control_grid(verbose = FALSE) )  # 6. Show the best architecture show_best(tune_res, metric = \"rmse\") #> # A tibble: 5 × 7 #>   num_dense dense_units dropout_rate .metric .estimator .mean .config               #>       <int>       <int>        <dbl> <chr>   <chr>      <dbl> <chr>                 #> 1         1          64          0.1 rmse    standard    2.92 Preprocessor1_Model02 #> 2         1          64          0.5 rmse    standard    3.02 Preprocessor1_Model08 #> 3         3          64          0.1 rmse    standard    3.15 Preprocessor1_Model04 #> 4         1           8          0.1 rmse    standard    3.20 Preprocessor1_Model01 #> 5         3           8          0.1 rmse    standard    3.22 Preprocessor1_Model03"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Functional API. allows creating complex, tunable architectures non-linear topologies integrate seamlessly tidymodels ecosystem.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"create_keras_functional_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"model_name character string name new model specification function (e.g., \"custom_resnet\"). valid R function name. layer_blocks named list functions function defines \"block\" (node) model graph. list names crucial define names nodes. arguments function define nodes connected. See \"Model Graph Connectivity\" section details. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., custom_resnet()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Functional API. ideal models complex, non-linear topologies, networks multiple inputs/outputs residual connections. function inspects arguments layer_blocks functions makes available tunable parameters generated model specification, prefixed block's name (e.g., dense_units). Common training parameters epochs learn_rate also added.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"model-graph-connectivity","dir":"Reference","previous_headings":"","what":"Model Graph Connectivity","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"kerasnip builds model's directed acyclic graph inspecting arguments function layer_blocks list. connection logic follows: names elements layer_blocks list define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. kerasnip automatically supply output tensors nodes calling my_block. two special requirements: Input Block: first block list treated input node. function take blocks input, can input_shape argument, supplied automatically fitting. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. key feature automatic creation num_{block_name} arguments (e.g., num_dense_path). allows control many times block repeated, making easy tune depth network. block can repeated exactly one input another block graph. new model specification function update() method created environment specified env argument.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   library(keras3)   library(parsnip)    # 1. Define block functions. These are the building blocks of our model.   # An input block that receives the data's shape automatically.   input_block <- function(input_shape) layer_input(shape = input_shape)    # A dense block with a tunable `units` parameter.   dense_block <- function(tensor, units) {     tensor |> layer_dense(units = units, activation = \"relu\")   }    # A block that adds two tensors together (for the residual connection).   add_block <- function(input_a, input_b) layer_add(list(input_a, input_b))    # An output block for regression.   output_block_reg <- function(tensor) layer_dense(tensor, units = 1)    # 2. Create the spec. The `layer_blocks` list defines the graph.   create_keras_functional_spec(     model_name = \"my_resnet_spec\",     layer_blocks = list(       # The names of list elements are the node names.       main_input = input_block,        # The argument `main_input` connects this block to the input node.       dense_path = function(main_input, units = 32) dense_block(main_input, units),        # This block's arguments connect it to the original input AND the dense layer.       add_residual = function(main_input, dense_path) add_block(main_input, dense_path),        # This block must be named 'output'. It connects to the residual add layer.       output = function(add_residual) output_block_reg(add_residual)     ),     mode = \"regression\"   )    # 3. Use the newly created specification function!   # The `dense_path_units` argument was created automatically.   model_spec <- my_resnet_spec(dense_path_units = 64, epochs = 10)    # You could also tune the number of dense layers since it has a single input:   # model_spec <- my_resnet_spec(num_dense_path = 2, dense_path_units = 32)    print(model_spec)   # tune::tunable(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Sequential API. ideal choice creating models simple, linear stack layers. models complex, non-linear topologies, see create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"create_keras_sequential_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"model_name character string name new model specification function (e.g., \"custom_cnn\"). valid R function name. layer_blocks named, ordered list functions. function defines \"block\" Keras layers. function must take Keras model object first argument return modified model. arguments function become tunable parameters final model specification. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., my_mlp()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Sequential API. function inspects arguments layer_blocks functions (ignoring special arguments like input_shape num_classes) makes available arguments generated model specification, prefixed block's name (e.g., dense_units). new model specification function update() method created environment specified env argument.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"model-architecture-sequential-api-","dir":"Reference","previous_headings":"","what":"Model Architecture (Sequential API)","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"kerasnip builds model applying functions layer_blocks order provided. function receives Keras model built previous function returns modified version. first block must initialize model (e.g., keras_model_sequential()). can accept input_shape argument, kerasnip provide automatically fitting. Subsequent blocks add layers model. final block add output layer. classification, can accept num_classes argument, provided automatically. key feature function automatic creation num_{block_name} arguments (e.g., num_hidden). allows control many times block repeated, making easy tune depth network.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) { library(keras3) library(parsnip) library(dials)  # 1. Define layer blocks for a complete model. # The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # A block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") }  # 2. Create the spec, providing blocks in the correct order. create_keras_sequential_spec( model_name = \"my_mlp\",   layer_blocks = list(     input = input_block,     hidden = hidden_block,     output = output_block   ),   mode = \"classification\" )  # 3. Use the newly created specification function! # Note the new arguments `num_hidden` and `hidden_units`. model_spec <- my_mlp(   num_hidden = 2,   hidden_units = 64,   epochs = 10,   learn_rate = 0.01 )  print(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"function internal engine fitting models generated create_keras_functional_spec(). intended called directly user.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"","code":"generic_functional_fit(x, y, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"x data frame matrix predictors. y vector outcomes. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)). epochs integer number training iterations. learn_rate double learning rate, used configure default Adam optimizer. batch_size integer number samples per gradient update. tunable parameter passed keras3::fit(). validation_split proportion training data use validation set. verbose integer verbosity fitting process (0, 1, 2).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Keras Functional API Model Fitting Implementation — generic_functional_fit","text":"function performs following key steps: Argument & Data Preparation: resolves arguments passed parsnip (handling rlang_zap objects unspecified arguments) prepares x y data Keras. automatically determines input_shape x , classification, num_classes y. Dynamic Model Construction: builds Keras model graph processing layer_blocks list. Connectivity: graph connected matching argument names block function names previously defined blocks. example, block function(input_a, ...) receive output tensor block named input_a. Repetition: checks num_{block_name} arguments repeat block multiple times, creating chain identical layers. block can repeated exactly one input tensor another block. Model Compilation: compiles final Keras model. compilation arguments (optimizer, loss, metrics) can customized passing arguments prefixed compile_ (e.g., compile_loss = \"mae\"). Model Fitting: calls keras3::fit() train model prepared data.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"function internal engine fitting models generated create_keras_sequential_spec(). intended called directly user.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"","code":"generic_sequential_fit(x, y, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"x data frame matrix predictors. y vector outcomes. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)). epochs integer number training iterations. learn_rate double learning rate, used configure default Adam optimizer. batch_size integer number samples per gradient update. tunable parameter passed keras3::fit(). validation_split proportion training data use validation set. verbose integer verbosity fitting process (0, 1, 2).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generic Keras Sequential API Model Fitting Implementation — generic_sequential_fit","text":"function performs following key steps: Argument & Data Preparation: resolves arguments passed parsnip (handling rlang_zap objects unspecified arguments) prepares x y data Keras. automatically determines input_shape x , classification, num_classes y. Dynamic Model Construction: builds Keras model sequentially processing layer_blocks list. first block function must initialize model, typically calling keras3::keras_model_sequential(). checks num_{block_name} arguments repeat block multiple times, creating deeper stack layers. Model Compilation: compiles final Keras model. compilation arguments (optimizer, loss, metrics) can customized passing arguments prefixed compile_ (e.g., compile_loss = \"mae\"). Model Fitting: calls keras3::fit() train model prepared data.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remap Layer Block Arguments for Model Specification — inp_spec","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"Creates wrapper function around Keras layer block rename arguments. powerful helper defining layer_blocks create_keras_functional_spec() create_keras_sequential_spec(), allowing connect reusable blocks model graph without writing verbose anonymous functions.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"inp_spec(block, input_map)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"block function defines Keras layer set layers. first arguments input tensor(s). input_map single character string named character vector specifies rename/remap arguments block.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"new function (closure) wraps block function renamed arguments, ready used layer_blocks list.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"inp_spec() makes model definitions cleaner readable. handles metaprogramming required create new function correct argument names, preserving original block's hyperparameters default values. function supports two modes operation based input_map: Single Input Renaming: input_map single character string, wrapper function renames first argument block function provided string. common case blocks take single tensor input. Multiple Input Mapping: input_map named character vector, provides explicit mapping new argument names (names vector) original argument names block function (values vector). used blocks multiple inputs, like concatenation layer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"if (FALSE) { # \\dontrun{ # --- Example Blocks --- # A standard dense block with one input tensor and one hyperparameter. dense_block <- function(tensor, units = 16) {   tensor |> keras3::layer_dense(units = units, activation = \"relu\") }  # A block that takes two tensors as input. concat_block <- function(input_a, input_b) {   keras3::layer_concatenate(list(input_a, input_b)) }  # An output block with one input. output_block <- function(tensor) {   tensor |> keras3::layer_dense(units = 1) }  # --- Usage --- layer_blocks <- list(   main_input = keras3::layer_input,   path_a = inp_spec(dense_block, \"main_input\"),   path_b = inp_spec(dense_block, \"main_input\"),   concatenated = inp_spec(     concat_block,     c(path_a = \"input_a\", path_b = \"input_b\")   ),   output = inp_spec(output_block, \"concatenated\") ) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Dynamically Discovered Keras Objects — keras_objects","title":"Dynamically Discovered Keras Objects — keras_objects","text":"exported vectors contain names optimizers, losses, metrics discovered installed keras3 package kerasnip loaded. ensures kerasnip always --date Keras version.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dynamically Discovered Keras Objects — keras_objects","text":"","code":"keras_optimizers  keras_losses  keras_metrics"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dynamically Discovered Keras Objects — keras_objects","text":"object class character length 12. object class character length 21. object class character length 32.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dynamically Discovered Keras Objects — keras_objects","text":"objects primarily used provide default values dials parameter functions, optimizer_function() loss_function_keras(). allows tab-completion IDEs validation optimizer loss names tuning models. discovery process .onLoad() scrapes keras3 namespace functions matching optimizer_*, loss_*, metric_* patterns.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Loss Functions — loss_function_keras","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"Dials Parameter Keras Loss Functions","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"","code":"loss_function_keras(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"values character vector possible loss functions. Defaults known losses (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Optimizers — optimizer_function","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"Dials Parameter Keras Optimizers","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"","code":"optimizer_function(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"values character vector possible optimizers. Defaults known optimizers (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Loss — register_keras_loss","title":"Register a Custom Keras Loss — register_keras_loss","text":"Allows users register custom loss function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Loss — register_keras_loss","text":"","code":"register_keras_loss(name, loss_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Loss — register_keras_loss","text":"name name register loss (character). loss_fn loss function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Loss — register_keras_loss","text":"Registered losses stored internal environment. model compiled, kerasnip first check internal registry loss matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Metric — register_keras_metric","title":"Register a Custom Keras Metric — register_keras_metric","text":"Allows users register custom metric function can used name within kerasnip model specifications.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Metric — register_keras_metric","text":"","code":"register_keras_metric(name, metric_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Metric — register_keras_metric","text":"name name register metric (character). metric_fn metric function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Metric — register_keras_metric","text":"Registered metrics stored internal environment. model compiled, kerasnip first check internal registry metric matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Optimizer — register_keras_optimizer","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Allows users register custom optimizer function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"register_keras_optimizer(name, optimizer_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"name name register optimizer (character). optimizer_fn optimizer function. return Keras optimizer object.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Registered optimizers stored internal environment. model compiled, kerasnip first check internal registry optimizer matching provided name checking keras3 package. optimizer_fn can simple function partially applied function using purrr::partial(). useful creating versions Keras optimizers specific settings.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"if (requireNamespace(\"keras3\", quietly = TRUE)) {   # Register a custom version of Adam with a different default beta_1   my_adam <- purrr::partial(keras3::optimizer_adam, beta_1 = 0.8)   register_keras_optimizer(\"my_adam\", my_adam)    # Now \"my_adam\" can be used as a string in a model spec, e.g.,   # my_model_spec(compile_optimizer = \"my_adam\") }"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function completely removes model specification previously created create_keras_sequential_spec() create_keras_functional_spec(). cleans function user's environment associated registrations within parsnip package.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"remove_keras_spec(model_name, env = parent.frame())"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"model_name character string giving name model specification function remove (e.g., \"my_mlp\"). env environment remove function update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"Invisibly returns TRUE attempting remove objects.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function essential cleanly unloading dynamically created model. performs three main actions: removes model specification function (e.g., my_mlp()) corresponding update() method specified environment. searches parsnip's internal model environment objects whose names start model_name removes . purges fit methods, argument definitions, registrations. removes model's name parsnip's master list models. function uses un-exported parsnip:::get_model_env() perform cleanup, may subject change future parsnip versions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   # First, create a dummy spec   input_block <- function(model, input_shape) keras3::keras_model_sequential(input_shape = input_shape)   dense_block <- function(model, units = 16) model |> keras3::layer_dense(units = units)   create_keras_sequential_spec(\"my_temp_model\", list(input = input_block, dense = dense_block), \"regression\")    # Check it exists in the environment and in parsnip   exists(\"my_temp_model\")   \"my_temp_model\" %in% parsnip::show_engines(\"my_temp_model\")$model    # Now remove it   remove_keras_spec(\"my_temp_model\")    # Check it's gone   !exists(\"my_temp_model\")   !\"my_temp_model\" %in% parsnip::show_engines(NULL)$model } } # }"},{"path":"https://davidrsch.github.io/kerasnip/news/index.html","id":"kerasnip-0009000","dir":"Changelog","previous_headings":"","what":"kerasnip 0.0.0.9000","title":"kerasnip 0.0.0.9000","text":"Initial development version. Added create_keras_spec() generate parsnip specifications dynamically.","code":""}]
