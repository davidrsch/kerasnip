[{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement daviddrsch@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to kerasnip","title":"Contributing to kerasnip","text":"outlines propose change kerasnip. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to kerasnip","text":"Small typos grammatical errors documentation may edited directly using GitHub web interface, long changes made source file. YES: edit roxygen comment .R file R/. : edit .Rd file man/.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"prerequisites","dir":"","previous_headings":"","what":"Prerequisites","title":"Contributing to kerasnip","text":"make substantial pull request, always file issue make sure someone team agrees ’s problem. ’ve found bug, create associated issue illustrate bug minimal reprex.","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull request process","title":"Contributing to kerasnip","text":"recommend create Git branch pull request (PR). Look GitHub Actions build status making changes. README contains badges continuous integration services used package. New code follow tidyverse style guide. can use air package apply styles. can format code automatically commenting /style PR. use roxygen2, Markdown syntax, documentation. use testthat. Contributions test cases included easier accept. user-facing changes, add bullet top NEWS.md current development version header describing changes made followed GitHub username, links relevant issue(s)/PR(s).","code":""},{"path":"https://davidrsch.github.io/kerasnip/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to kerasnip","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 kerasnip authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://davidrsch.github.io/kerasnip/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with kerasnip","title":"Getting help with kerasnip","text":"Thanks using kerasnip. filing issue, places explore pieces put together make process smooth possible. Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site. Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed. right place, need file issue, please review “File issues” paragraph tidyverse contributing guidelines. Thanks help!","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"when-to-use-the-functional-api","dir":"Articles","previous_headings":"","what":"When to Use the Functional API","title":"Building Functional Models with kerasnip","text":"create_keras_sequential_spec() perfect models simple, linear stack layers, many advanced architectures linear. Keras Functional API designed cases. use create_keras_functional_spec() model : Multiple input multiple output layers. Shared layers different branches. Residual connections (e.g., ResNets), layer’s input added output. non-linear topology. kerasnip makes easy define architectures automatically connecting graph layer blocks.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"the-core-concept-building-a-graph","dir":"Articles","previous_headings":"","what":"The Core Concept: Building a Graph","title":"Building Functional Models with kerasnip","text":"kerasnip builds model’s graph inspecting layer_blocks provide. connection logic simple powerful: names list elements layer_blocks define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. two special requirements: Input Block: first block list treated main input node. function take blocks input. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. Let’s see action.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"example-1-a-fork-join-regression-model","dir":"Articles","previous_headings":"","what":"Example 1: A Fork-Join Regression Model","title":"Building Functional Models with kerasnip","text":"build model forks input, passes two separate dense layer paths, joins results concatenation layer producing final prediction.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-1-load-libraries","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 1: Load Libraries","title":"Building Functional Models with kerasnip","text":"First, load necessary packages.","code":"library(kerasnip) library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom        1.0.9     ✔ recipes      1.3.1 ## ✔ dials        1.4.1     ✔ rsample      1.3.1 ## ✔ dplyr        1.1.4     ✔ tibble       3.3.0 ## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 ## ✔ infer        1.0.9     ✔ tune         1.3.0 ## ✔ modeldata    1.5.0     ✔ workflows    1.2.0 ## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 ## ✔ purrr        1.1.0     ✔ yardstick    1.3.2 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter()  masks stats::filter() ## ✖ dplyr::lag()     masks stats::lag() ## ✖ recipes::step()  masks stats::step() library(keras3) ##  ## Attaching package: 'keras3' ## The following object is masked from 'package:yardstick': ##  ##     get_weights # Silence the startup messages from remove_keras_spec options(kerasnip.show_removal_messages = FALSE)"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-2-define-layer-blocks","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 2: Define Layer Blocks","title":"Building Functional Models with kerasnip","text":"building blocks model. function represents node graph.","code":"# The input node. `input_shape` is supplied automatically by the engine. input_block <- function(input_shape) {   layer_input(shape = input_shape) }  # A generic block for a dense path. `units` will be a tunable parameter. path_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") }  # A block to join two tensors. concat_block <- function(input_a, input_b) {   layer_concatenate(list(input_a, input_b)) }  # The final output block for regression. output_block_reg <- function(tensor) {   layer_dense(tensor, units = 1) }"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-3-create-the-model-specification","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 3: Create the Model Specification","title":"Building Functional Models with kerasnip","text":"Now assemble blocks graph. use inp_spec() helper connect blocks. avoids writing verbose anonymous functions like function(main_input, units) path_block(main_input, units). inp_spec() automatically creates wrapper renames arguments blocks match node names layer_blocks list.","code":"model_name <- \"forked_reg_spec\" # Clean up the spec when the vignette is done knitting on.exit(remove_keras_spec(model_name), add = TRUE)  create_keras_functional_spec(   model_name = model_name,   layer_blocks = list(     # Node names are defined by the list names     main_input = input_block,      # `inp_spec()` renames the first argument of `path_block` ('tensor')     # to 'main_input' to match the node name.     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),      # For multiple inputs, `inp_spec()` takes a named vector to map     # new argument names to the original block's argument names.     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),      # The output block takes the concatenated tensor as its input.     output = inp_spec(output_block_reg, \"concatenated\")   ),   mode = \"regression\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-4-use-and-fit-the-model","dir":"Articles","previous_headings":"Example 1: A Fork-Join Regression Model","what":"Step 4: Use and Fit the Model","title":"Building Functional Models with kerasnip","text":"new function forked_reg_spec() now available. arguments (path_a_units, path_b_units) discovered automatically block definitions.","code":"# We can override the default `units` from `path_block` for each path. spec <- forked_reg_spec(   path_a_units = 16,   path_b_units = 8,   fit_epochs = 10,   fit_verbose = 0 # Suppress fitting output in vignette ) |>   set_engine(\"keras\")  print(spec) ## forked reg spec Model Specification (regression) ##  ## Main Arguments: ##   num_main_input = structure(list(), class = \"rlang_zap\") ##   num_path_a = structure(list(), class = \"rlang_zap\") ##   num_path_b = structure(list(), class = \"rlang_zap\") ##   num_concatenated = structure(list(), class = \"rlang_zap\") ##   num_output = structure(list(), class = \"rlang_zap\") ##   path_a_units = 16 ##   path_b_units = 8 ##   learn_rate = structure(list(), class = \"rlang_zap\") ##   fit_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_epochs = 10 ##   fit_callbacks = structure(list(), class = \"rlang_zap\") ##   fit_validation_split = structure(list(), class = \"rlang_zap\") ##   fit_validation_data = structure(list(), class = \"rlang_zap\") ##   fit_shuffle = structure(list(), class = \"rlang_zap\") ##   fit_class_weight = structure(list(), class = \"rlang_zap\") ##   fit_sample_weight = structure(list(), class = \"rlang_zap\") ##   fit_initial_epoch = structure(list(), class = \"rlang_zap\") ##   fit_steps_per_epoch = structure(list(), class = \"rlang_zap\") ##   fit_validation_steps = structure(list(), class = \"rlang_zap\") ##   fit_validation_batch_size = structure(list(), class = \"rlang_zap\") ##   fit_validation_freq = structure(list(), class = \"rlang_zap\") ##   fit_verbose = 0 ##   fit_view_metrics = structure(list(), class = \"rlang_zap\") ##   compile_optimizer = structure(list(), class = \"rlang_zap\") ##   compile_loss = structure(list(), class = \"rlang_zap\") ##   compile_metrics = structure(list(), class = \"rlang_zap\") ##   compile_loss_weights = structure(list(), class = \"rlang_zap\") ##   compile_weighted_metrics = structure(list(), class = \"rlang_zap\") ##   compile_run_eagerly = structure(list(), class = \"rlang_zap\") ##   compile_steps_per_execution = structure(list(), class = \"rlang_zap\") ##   compile_jit_compile = structure(list(), class = \"rlang_zap\") ##   compile_auto_scale_loss = structure(list(), class = \"rlang_zap\") ##  ## Computational engine: keras # Fit the model on the mtcars dataset rec <- recipe(mpg ~ ., data = mtcars) wf <- workflow() |>    add_recipe(rec) |>   add_model(spec)   fit_obj <- fit(wf, data = mtcars)  predict(fit_obj, new_data = mtcars[1:5, ]) ## 1/1 - 0s - 40ms/step ## # A tibble: 5 × 1 ##      .pred ##      <dbl> ## 1  0.158   ## 2  0.00697 ## 3 -0.416   ## 4  6.79    ## 5 10.4"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"example-2-tuning-a-functional-models-depth","dir":"Articles","previous_headings":"","what":"Example 2: Tuning a Functional Model’s Depth","title":"Building Functional Models with kerasnip","text":"key feature kerasnip ability tune depth network repeating block multiple times. block can repeated exactly one input tensor another block graph. Let’s create simple functional model tune width (units) depth (num_...).","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-1-define-blocks-and-create-spec","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 1: Define Blocks and Create Spec","title":"Building Functional Models with kerasnip","text":"model architecturally sequential, build functional API demonstrate repetition feature.","code":"dense_block <- function(tensor, units = 16) {   tensor |> layer_dense(units = units, activation = \"relu\") } output_block_class <- function(tensor, num_classes) {   tensor |> layer_dense(units = num_classes, activation = \"softmax\") }  model_name_tune <- \"tunable_func_mlp\" on.exit(remove_keras_spec(model_name_tune), add = TRUE)  create_keras_functional_spec(   model_name = model_name_tune,   layer_blocks = list(     main_input = input_block,     # This block has a single input ('main_input'), so it can be repeated.     dense_path = inp_spec(dense_block, \"main_input\"),     output = inp_spec(output_block_class, \"dense_path\")   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"step-2-set-up-and-run-tuning","dir":"Articles","previous_headings":"Example 2: Tuning a Functional Model’s Depth","what":"Step 2: Set up and Run Tuning","title":"Building Functional Models with kerasnip","text":"tune dense_path_units (width) num_dense_path (depth). num_dense_path argument created automatically dense_path repeatable block. results show tidymodels successfully trained evaluated models different numbers hidden layers, demonstrating can tune architecture network.","code":"tune_spec <- tunable_func_mlp(   dense_path_units = tune(),   num_dense_path = tune(),   fit_epochs = 5,   fit_verbose = 0 ) |>   set_engine(\"keras\")  rec <- recipe(Species ~ ., data = iris) tune_wf <- workflow() |>    add_recipe(rec) |>   add_model(tune_spec)  folds <- vfold_cv(iris, v = 2)  # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     dense_path_units = hidden_units(c(8, 32)),     num_dense_path = num_terms(c(1, 3)) # Test models with 1, 2, or 3 hidden layers   )  grid <- grid_regular(params, levels = 2) grid ## # A tibble: 4 × 2 ##   num_dense_path dense_path_units ##            <int>            <int> ## 1              1                8 ## 2              3                8 ## 3              1               32 ## 4              3               32 control <- control_grid(save_pred = FALSE, verbose = FALSE)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   control = control ) ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 22ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 16ms/step ## 3/3 - 0s - 7ms/step ## 3/3 - 0s - 21ms/step ## 3/3 - 0s - 7ms/step show_best(tune_res, metric = \"accuracy\") ## # A tibble: 4 × 8 ##   num_dense_path dense_path_units .metric .estimator  mean     n std_err .config ##            <int>            <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>   ## 1              3               32 accura… multiclass 0.96      2 0.0267  Prepro… ## 2              1               32 accura… multiclass 0.673     2 0.00667 Prepro… ## 3              3                8 accura… multiclass 0.587     2 0.267   Prepro… ## 4              1                8 accura… multiclass 0.56      2 0.0933  Prepro…"},{"path":"https://davidrsch.github.io/kerasnip/articles/functional_api.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Building Functional Models with kerasnip","text":"create_keras_functional_spec() function provides powerful intuitive way define, fit, tune complex Keras models within tidymodels framework. defining model graph connected blocks, can represent nearly architecture kerasnip handles boilerplate integrating parsnip, dials, tune.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-core-idea-from-keras-layers-to-tidymodels-specs","dir":"Articles","previous_headings":"","what":"The Core Idea: From Keras Layers to Tidymodels Specs","title":"Getting Started with kerasnip","text":"keras3 package allows building deep learning models layer--layer, powerful flexible approach. However, tidymodels ecosystem designed around declarative model specifications, define model want parameters want tune, rather building imperatively. kerasnip bridges gap simple powerful concept: layer blocks. define components neural network (e.g., input block, dense block, dropout block) simple R functions. kerasnip uses blocks building materials create brand new parsnip model specification function . new function behaves just like parsnip model (e.g., rand_forest() linear_reg()), making easy integrate tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started with kerasnip","text":"can install development version kerasnip GitHub. also need keras3. ’ll start loading kerasnip, tidymodels keras3:","code":"install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\") pak::pak(\"rstudio/keras3\")  # Install the backend keras3::install_keras() library(kerasnip) library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── #> ✔ broom        1.0.9     ✔ recipes      1.3.1 #> ✔ dials        1.4.1     ✔ rsample      1.3.1 #> ✔ dplyr        1.1.4     ✔ tibble       3.3.0 #> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 #> ✔ infer        1.0.9     ✔ tune         1.3.0 #> ✔ modeldata    1.5.0     ✔ workflows    1.2.0 #> ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 #> ✔ purrr        1.1.0     ✔ yardstick    1.3.2 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() library(keras3) #>  #> Attaching package: 'keras3' #> The following object is masked from 'package:yardstick': #>  #>     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"a-kerasnip-mnist-example","dir":"Articles","previous_headings":"","what":"A kerasnip MNIST Example","title":"Getting Started with kerasnip","text":"Let’s replicate classic Keras introductory example, training simple MLP MNIST dataset, using kerasnip workflow. demonstrate translate standard Keras model reusable, modular parsnip specification. ’re familiar Keras, ’ll recognize structure; , perfect place start. ’ll begin learning basics simple task: recognizing handwritten digits MNIST dataset. MNIST dataset contains 28×28 pixel grayscale images handwritten digits, like : image comes label indicating digit represents. example, labels images might 5, 0, 4, 1.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"preparing-the-data","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Preparing the Data","title":"Getting Started with kerasnip","text":"step identical Keras model. load MNIST dataset, reshape predictors, convert outcome factor tidymodels.","code":"mnist <- dataset_mnist() #> Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz #>        0/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step 7127040/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step x_train <- mnist$train$x y_train <- mnist$train$y x_test <- mnist$test$x y_test <- mnist$test$y  # Reshape x_train <- array_reshape(x_train, c(nrow(x_train), 784)) x_test <- array_reshape(x_test, c(nrow(x_test), 784)) # Rescale x_train <- x_train / 255 x_test <- x_test / 255  # Convert outcomes to factors for tidymodels # kerasnip will handle y convertion internally using keras3::to_categorical() y_train_factor <- factor(y_train) y_test_factor <- factor(y_test)  # For tidymodels, it's best to work with data frames # Use I() to keep the matrix structure of x within the data frame train_df <- data.frame(x = I(x_train), y = y_train_factor) test_df <- data.frame(x = I(x_test), y = y_test_factor)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"the-standard-keras-approach-for-comparison","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"The Standard Keras Approach (for comparison)","title":"Getting Started with kerasnip","text":"diving kerasnip workflow, let’s quickly look model built using standard keras3 code. help highlight different approach kerasnip enables. code imperative: define layer add model step--step. Now, let’s see kerasnip approaches defining reusable components declarative, tidymodels-friendly workflow.","code":"# The standard Keras3 approach model <- keras_model_sequential(input_shape = 784) |>   layer_dense(units = 256, activation = \"relu\") |>   layer_dropout(rate = 0.4) |>   layer_dense(units = 128, activation = \"relu\") |>   layer_dropout(rate = 0.3) |>   layer_dense(units = 10, activation = \"softmax\")  summary(model)  model |>   compile(     loss = \"categorical_crossentropy\",     optimizer = optimizer_rmsprop(),     metrics = \"accuracy\"   )  # The model would then be trained with model |> fit(...)"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"defining-the-model-with-reusable-blocks","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Defining the Model with Reusable Blocks","title":"Getting Started with kerasnip","text":"original Keras example interleaves layer_dense() layer_dropout(). kerasnip, can encapsulate pattern single, reusable block. makes overall architecture cleaner modular. Now, use create_keras_sequential_spec() generate parsnip model function.","code":"# An input block to initialize the model. # The 'model' argument is supplied implicitly by the kerasnip backend. mlp_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  # A reusable \"module\" that combines a dense layer and a dropout layer. # All arguments that should be tunable need a default value. dense_dropout_block <- function(model, units = 128, rate = 0.1) {   model |>     layer_dense(units = units, activation = \"relu\") |>     layer_dropout(rate = rate) }  # The output block for classification. mlp_output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") } create_keras_sequential_spec(   model_name = \"mnist_mlp\",   layer_blocks = list(     input = mlp_input_block,     hidden_1 = dense_dropout_block,     hidden_2 = dense_dropout_block,     output = mlp_output_block   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"building-and-fitting-the-model","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Building and Fitting the Model","title":"Getting Started with kerasnip","text":"can now use new mnist_mlp() function. Notice arguments, hidden_1_units hidden_1_rate, automatically generated kerasnip. names created combining name layer block (e.g., hidden_1) arguments block’s function (e.g., units, rate). replicate keras3 example, ’ll use hidden blocks provide parameters.","code":"mlp_spec <- mnist_mlp(   hidden_1_units = 256,   hidden_1_rate = 0.4,   hidden_2_rate = 0.3,   hidden_2_units =  128,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Fit the model mlp_fit <- fit(mlp_spec, y ~ x, data = train_df) mlp_fit |>    extract_keras_model() |>    summary() #> Model: \"sequential\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense (Dense)                     │ (None, 256)              │       200,960 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout (Dropout)                 │ (None, 256)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_1 (Dense)                   │ (None, 128)              │        32,896 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_1 (Dropout)               │ (None, 128)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_2 (Dense)                   │ (None, 10)               │         1,290 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 470,294 (1.79 MB) #>  Trainable params: 235,146 (918.54 KB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 235,148 (918.55 KB) mlp_fit |>    extract_keras_model() |>    plot(show_shapes = TRUE) mlp_fit |>    extract_keras_history() |>    plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"evaluating-model-performance","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Evaluating Model Performance","title":"Getting Started with kerasnip","text":"keras_evaluate() function provides straightforward way assess model’s performance test set, using underlying keras3::evaluate() method. returns loss metrics specified model compilation step.","code":"mlp_fit |> keras_evaluate(x_test, y_test) #> 313/313 - 0s - 1ms/step - accuracy: 0.9825 - loss: 0.0860 #> $accuracy #> [1] 0.9825 #>  #> $loss #> [1] 0.08597153"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"making-predictions","dir":"Articles","previous_headings":"A kerasnip MNIST Example","what":"Making Predictions","title":"Getting Started with kerasnip","text":"model trained, can use standard tidymodels predict() function generate predictions new data. default, predict() parsnip classification model returns predicted class labels. get underlying probabilities class, can set type = \"prob\". returns tibble probability column 10 classes (0-9). can compare predicted class actual class images see model performing.","code":"# Predict the class for the first 5 images in the test set  class_preds <- mlp_fit |>   predict(new_data = head(select(test_df, x))) #> 1/1 - 0s - 41ms/step class_preds #> # A tibble: 6 × 1 #>   .pred_class #>   <fct>       #> 1 7           #> 2 2           #> 3 1           #> 4 0           #> 5 4           #> 6 1 # Predict probabilities for the first 5 images prob_preds <- mlp_fit |> predict(new_data = head(select(test_df, x)), type = \"prob\") #> 1/1 - 0s - 21ms/step prob_preds #> # A tibble: 6 × 10 #>     .pred_0  .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6  .pred_7 #>       <dbl>    <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl>    <dbl> #> 1 1.42 e-19 3.72e-15 2.46e-13 2.21e-12 2.50 e-19 6.75e-16 3.79e-25 1   e+ 0 #> 2 2.12 e-19 1.16e-11 1   e+ 0 7.30e-13 4.08 e-27 1.46e-19 9.08e-22 9.40e-20 #> 3 2.24 e-13 1   e+ 0 2.28e-10 6.13e-13 3.06 e- 8 9.36e-11 7.24e-11 3.07e- 8 #> 4 1.000e+ 0 1.45e-14 3.35e- 9 1.01e-12 1.15 e-11 2.78e- 9 4.07e- 7 5.76e-11 #> 5 1.87 e-11 1.18e-11 1.81e-10 8.77e-15 1.000e+ 0 1.85e-12 1.45e-11 4.38e- 8 #> 6 1.54 e-14 1   e+ 0 1.45e-12 2.17e-14 2.36 e- 8 9.72e-13 1.06e-12 5.24e- 8 #> # ℹ 2 more variables: .pred_8 <dbl>, .pred_9 <dbl> # Combine predictions with actuals for comparison comparison <- bind_cols(   class_preds,   prob_preds ) |>   bind_cols(     head(test_df[, \"y\", drop = FALSE])   ) comparison #> # A tibble: 6 × 12 #>   .pred_class   .pred_0  .pred_1  .pred_2  .pred_3   .pred_4  .pred_5  .pred_6 #>   <fct>           <dbl>    <dbl>    <dbl>    <dbl>     <dbl>    <dbl>    <dbl> #> 1 7           1.42 e-19 3.72e-15 2.46e-13 2.21e-12 2.50 e-19 6.75e-16 3.79e-25 #> 2 2           2.12 e-19 1.16e-11 1   e+ 0 7.30e-13 4.08 e-27 1.46e-19 9.08e-22 #> 3 1           2.24 e-13 1   e+ 0 2.28e-10 6.13e-13 3.06 e- 8 9.36e-11 7.24e-11 #> 4 0           1.000e+ 0 1.45e-14 3.35e- 9 1.01e-12 1.15 e-11 2.78e- 9 4.07e- 7 #> 5 4           1.87 e-11 1.18e-11 1.81e-10 8.77e-15 1.000e+ 0 1.85e-12 1.45e-11 #> 6 1           1.54 e-14 1   e+ 0 1.45e-12 2.17e-14 2.36 e- 8 9.72e-13 1.06e-12 #> # ℹ 4 more variables: .pred_7 <dbl>, .pred_8 <dbl>, .pred_9 <dbl>, y <fct>"},{"path":"https://davidrsch.github.io/kerasnip/articles/getting_started.html","id":"example-2-tuning-the-model-architecture","dir":"Articles","previous_headings":"","what":"Example 2: Tuning the Model Architecture","title":"Getting Started with kerasnip","text":"Now ’ll showcase main strength kerasnip: tuning network architecture . can treat number layers, parameters layers, hyperparameters optimized tune. Using mnist_mlp spec just created, let’s define tunable model. Next, define search space tunable parameters using dials. Finally, can inspect results find architecture performed best. First, summary table: Now ’ve identified best-performing hyperparameters, final step create train final model. use select_best() get top parameters, finalize_workflow() update workflow , fit() one last time full training dataset. can now inspect final, tuned model.  result shows tune tested various network depths, widths, dropout rates, successfully finding best-performing combination within search space. using kerasnip, able integrate complex architectural tuning directly standard tidymodels workflow.","code":"# Define a tunable specification # We set num_hidden_2 = 0 to disable the second hidden block for this tuning example tune_spec <- mnist_mlp(   num_hidden_1 = tune(),   hidden_1_units = tune(),   hidden_1_rate = tune(),   num_hidden_2 = 0,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = optimizer_rmsprop(),   compile_metrics = c(\"accuracy\"),   fit_epochs = 30,   fit_batch_size = 128,   fit_validation_split = 0.2 ) |>   set_engine(\"keras\")  # Create a workflow tune_wf <- workflow(y ~ x, tune_spec) # Define the tuning grid params <- extract_parameter_set_dials(tune_wf) |>   update(     num_hidden_1 = dials::num_terms(c(1, 3)),     hidden_1_units = dials::hidden_units(c(64, 256)),     hidden_1_rate = dials::dropout(c(0.2, 0.4))   ) grid <- grid_regular(params, levels = 3) grid #> # A tibble: 27 × 3 #>    num_hidden_1 hidden_1_units hidden_1_rate #>           <int>          <int>         <dbl> #>  1            1             64           0.2 #>  2            2             64           0.2 #>  3            3             64           0.2 #>  4            1            160           0.2 #>  5            2            160           0.2 #>  6            3            160           0.2 #>  7            1            256           0.2 #>  8            2            256           0.2 #>  9            3            256           0.2 #> 10            1             64           0.3 #> # ℹ 17 more rows # Using only the first 100 rows for speed. The real call should be # folds <- vfold_cv(train_df, v = 3) folds <- vfold_cv(train_df[1:100,], v = 3)  tune_res <- tune_grid(   tune_wf,   resamples = folds,   grid = grid,   metrics = metric_set(accuracy),   control = control_grid(save_pred = FALSE, save_workflow = TRUE) ) #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 32ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 36ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 27ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 36ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 219ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 37ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 28ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 26ms/step #> 2/2 - 0s - 31ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 24ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 35ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 34ms/step #> 2/2 - 0s - 36ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 30ms/step #> 2/2 - 0s - 33ms/step #> 2/2 - 0s - 25ms/step #> 2/2 - 0s - 29ms/step #> 2/2 - 0s - 34ms/step # Show the summary table of the best models show_best(tune_res, metric = \"accuracy\") #> # A tibble: 5 × 9 #>   num_hidden_1 hidden_1_units hidden_1_rate .metric  .estimator  mean     n #>          <int>          <int>         <dbl> <chr>    <chr>      <dbl> <int> #> 1            2            256         0.3   accuracy multiclass 0.810     3 #> 2            2            256         0.2   accuracy multiclass 0.790     3 #> 3            1             64         0.2   accuracy multiclass 0.790     3 #> 4            1            256         0.2   accuracy multiclass 0.780     3 #> 5            1            160         0.400 accuracy multiclass 0.780     3 #> # ℹ 2 more variables: std_err <dbl>, .config <chr> # Select the best hyperparameters best_hps <- select_best(tune_res, metric = \"accuracy\")  # Finalize the workflow with the best hyperparameters final_wf <- finalize_workflow(tune_wf, best_hps)  # Fit the final model on the full training data final_fit <- fit(final_wf, data = train_df) # Print the model summary final_fit |>   extract_fit_parsnip() |>   extract_keras_model() |>    summary() #> Model: \"sequential_82\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ dense_246 (Dense)                 │ (None, 256)              │       200,960 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_164 (Dropout)             │ (None, 256)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_247 (Dense)                 │ (None, 256)              │        65,792 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dropout_165 (Dropout)             │ (None, 256)              │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense_248 (Dense)                 │ (None, 10)               │         2,570 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 538,646 (2.05 MB) #>  Trainable params: 269,322 (1.03 MB) #>  Non-trainable params: 0 (0.00 B) #>  Optimizer params: 269,324 (1.03 MB)  # Plot the training history final_fit |>    extract_fit_parsnip() |>   extract_keras_history() |>   plot()"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"The Sequential Model with kerasnip","text":"vignette provides comprehensive guide using kerasnip define sequential Keras models within tidymodels ecosystem. kerasnip bridges gap imperative, layer--layer construction Keras models declarative, specification-based approach tidymodels. , focus create_keras_sequential_spec(), ideal models layers form plain stack, layer exactly one input tensor one output tensor.","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"The Sequential Model with kerasnip","text":"’ll start loading necessary packages:","code":"library(kerasnip) library(tidymodels) #> ── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ── #> ✔ broom        1.0.9     ✔ recipes      1.3.1 #> ✔ dials        1.4.1     ✔ rsample      1.3.1 #> ✔ dplyr        1.1.4     ✔ tibble       3.3.0 #> ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1 #> ✔ infer        1.0.9     ✔ tune         1.3.0 #> ✔ modeldata    1.5.0     ✔ workflows    1.2.0 #> ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1 #> ✔ purrr        1.1.0     ✔ yardstick    1.3.2 #> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── #> ✖ purrr::discard() masks scales::discard() #> ✖ dplyr::filter()  masks stats::filter() #> ✖ dplyr::lag()     masks stats::lag() #> ✖ recipes::step()  masks stats::step() library(keras3) #>  #> Attaching package: 'keras3' #> The following object is masked from 'package:yardstick': #>  #>     get_weights"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"when-to-use-create_keras_sequential_spec","dir":"Articles","previous_headings":"","what":"When to use create_keras_sequential_spec()","title":"The Sequential Model with kerasnip","text":"Sequential model Keras appropriate plain stack layers layer exactly one input tensor one output tensor. kerasnip’s create_keras_sequential_spec() function designed define models tidymodels-compatible way. Instead building model layer--layer imperatively, define named, ordered list R functions called layer_blocks. layer_block function takes Keras model object first argument returns modified model. kerasnip uses blocks construct full Keras Sequential model. models complex, non-linear topologies (e.g., multiple inputs/outputs, residual connections, multi-branch models), use create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"creating-a-kerasnip-sequential-model-specification","dir":"Articles","previous_headings":"","what":"Creating a kerasnip Sequential Model Specification","title":"The Sequential Model with kerasnip","text":"Let’s define simple sequential model three dense layers. First, define layer_blocks: Now, use create_keras_sequential_spec() generate parsnip model specification function. ’ll name model my_simple_mlp.","code":"# The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) }  # A reusable block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32, activation = \"relu\") {   model |> layer_dense(units = units, activation = activation) }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes, activation = \"softmax\") {   model |> layer_dense(units = num_classes, activation = activation) } create_keras_sequential_spec(   model_name = \"my_simple_mlp\",   layer_blocks = list(     input = input_block,     hidden_1 = hidden_block,     hidden_2 = hidden_block,     output = output_block   ),   mode = \"classification\" )"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"a-common-debugging-workflow-compile_keras_grid","dir":"Articles","previous_headings":"","what":"A common debugging workflow: compile_keras_grid()","title":"The Sequential Model with kerasnip","text":"original Keras guide, common workflow incrementally add layers call summary() inspect architecture. kerasnip, model defined declaratively, can’t inspect layer--layer way. However, kerasnip provides powerful equivalent: compile_keras_grid(). function checks layer_blocks define valid Keras model returns compiled model structure, without running full training cycle. perfect debugging architecture. Let’s see action CNN architecture:","code":"# Define CNN layer blocks cnn_input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } cnn_conv_block <- function(model, filters = 32, kernel_size = 3, activation = \"relu\") {   model |> layer_conv_2d(filters = filters, kernel_size = kernel_size, activation = activation) } cnn_pool_block <- function(model, pool_size = 2) {   model |> layer_max_pooling_2d(pool_size = pool_size) } cnn_flatten_block <- function(model) {   model |> layer_flatten() } cnn_output_block <- function(model, num_classes, activation = \"softmax\") {   model |> layer_dense(units = num_classes, activation = activation) }  # Create the kerasnip spec function create_keras_sequential_spec(   model_name = \"my_cnn\",   layer_blocks = list(     input = cnn_input_block,     conv1 = cnn_conv_block,     pool1 = cnn_pool_block,     flatten = cnn_flatten_block,     output = cnn_output_block   ),   mode = \"classification\" )  # Create a spec instance for a 28x28x1 image cnn_spec <- my_cnn(   conv1_filters = 32, conv1_kernel_size = 5,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\" )  # Prepare dummy data with the correct shape. # We create a list of 28x28x1 arrays. x_dummy_list <- lapply(1:10, function(i) array(runif(28*28*1), dim = c(28, 28, 1))) x_dummy_df <- tibble::tibble(x = x_dummy_list) y_dummy <- factor(sample(0:9, 10, replace = TRUE), levels = 0:9) y_dummy_df <- tibble::tibble(y = y_dummy)   # Use compile_keras_grid to get the model summary compilation_results <- compile_keras_grid(   spec = cnn_spec,   grid = tibble::tibble(),    x = x_dummy_df,   y = y_dummy_df )  # Print the summary summary(compilation_results$compiled_model[[1]]) #> Model: \"sequential\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ #> ┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ #> │ conv2d (Conv2D)                   │ (None, 24, 24, 32)       │           832 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ max_pooling2d (MaxPooling2D)      │ (None, 12, 12, 32)       │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ flatten (Flatten)                 │ (None, 4608)             │             0 │ #> ├───────────────────────────────────┼──────────────────────────┼───────────────┤ #> │ dense (Dense)                     │ (None, 10)               │        46,090 │ #> └───────────────────────────────────┴──────────────────────────┴───────────────┘ #>  Total params: 46,922 (183.29 KB) #>  Trainable params: 46,922 (183.29 KB) #>  Non-trainable params: 0 (0.00 B)"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"feature-extraction-with-a-sequential-model","dir":"Articles","previous_headings":"","what":"Feature Extraction with a Sequential Model","title":"The Sequential Model with kerasnip","text":"Sequential model built, behaves like Functional API model. means every layer input output attribute. kerasnip, can get access underlying model structure using compile_keras_grid(). allows us create new model outputs values intermediate layers.","code":"# We can reuse the compilation results from the previous chunk keras_model_obj <- compilation_results$compiled_model[[1]]  # Create a new Keras model for feature extraction feature_extractor <- keras_model(   inputs = keras_model_obj$inputs,   outputs = lapply(keras_model_obj$layers, function(x) x$output) )  # Call the feature extractor on a dummy input tensor x_tensor <- op_ones(c(1, 28, 28, 1)) features <- feature_extractor(x_tensor)  # Print the shapes of the extracted feature maps lapply(features, dim) #> [[1]] #> [1]  1 24 24 32 #>  #> [[2]] #> [1]  1 12 12 32 #>  #> [[3]] #> [1]    1 4608 #>  #> [[4]] #> [1]  1 10"},{"path":"https://davidrsch.github.io/kerasnip/articles/sequential_model.html","id":"transfer-learning-with-a-sequential-model","dir":"Articles","previous_headings":"","what":"Transfer Learning with a Sequential Model","title":"The Sequential Model with kerasnip","text":"Transfer learning consists freezing bottom layers model training top layers. common blueprint use Sequential model stack pre-trained model freshly initialized classification layers. kerasnip supports allowing layer_block contain pre-trained model.","code":"# Define a block that incorporates a pre-trained base # This block creates a new sequential model and adds the pre-trained, # frozen base model as its first layer. pretrained_base_block <- function(model, input_shape) {   base_model <- application_xception(     weights = \"imagenet\",     include_top = FALSE,     pooling = \"avg\",     input_shape = input_shape   )   # Freeze the weights of the pre-trained base   freeze_weights(base_model)      # The block must return a sequential model   keras_model_sequential(input_shape = input_shape) |>     base_model() }  # Define a new classification head. This block will be appended to the # sequential model returned by the previous block. classification_head_block <- function(model, num_classes) {   model |>     layer_dense(units = 1000, activation = \"relu\") |>      layer_dense(units = num_classes, activation = \"softmax\") }  # Create a new kerasnip spec with the pre-trained base and new head create_keras_sequential_spec(   model_name = \"transfer_cnn\",   layer_blocks = list(     base = pretrained_base_block,     head = classification_head_block   ),   mode = \"classification\" )  # Create a spec instance transfer_spec <- transfer_cnn(   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\" )  # Prepare dummy data for a 224x224x3 image x_dummy_tl_list <- lapply(1:10, function(i) array(runif(224*224*3), dim = c(224, 224, 3))) x_dummy_tl_df <- tibble::tibble(x = x_dummy_tl_list) y_dummy_tl <- factor(sample(0:9, 10, replace = TRUE), levels = 0:9) y_dummy_tl_df <- tibble::tibble(y = y_dummy_tl)   # Use compile_keras_grid to inspect the model and trainable parameters compilation_results_tl <- compile_keras_grid(   spec = transfer_spec,   grid = tibble::tibble(),   x = x_dummy_tl_df,   y = y_dummy_tl_df ) #> Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5 #>        0/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step11509760/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step29065216/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step44720128/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step51585024/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step68247552/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step83683744/83683744 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step  # Print the summary to verify that the base model's parameters are non-trainable summary(compilation_results_tl$compiled_model[[1]]) #> Model: \"sequential_2\" #> ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━┓ #> ┃ Layer (type)                  ┃ Output Shape           ┃     Param # ┃ Trai… ┃ #> ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━┩ #> │ xception (Functional)         │ (None, 2048)           │  20,861,480 │   N   │ #> ├───────────────────────────────┼────────────────────────┼─────────────┼───────┤ #> │ dense_2 (Dense)               │ (None, 1000)           │   2,049,000 │   Y   │ #> ├───────────────────────────────┼────────────────────────┼─────────────┼───────┤ #> │ dense_3 (Dense)               │ (None, 10)             │      10,010 │   Y   │ #> └───────────────────────────────┴────────────────────────┴─────────────┴───────┘ #>  Total params: 22,920,490 (87.43 MB) #>  Trainable params: 2,059,010 (7.85 MB) #>  Non-trainable params: 20,861,480 (79.58 MB)"},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Díaz. Author, maintainer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Díaz D (2025). kerasnip: Bridge Keras Tidymodels. R package version 0.0.0.9000.","code":"@Manual{,   title = {kerasnip: A Bridge Between Keras and Tidymodels},   author = {David Díaz},   year = {2025},   note = {R package version 0.0.0.9000}, }"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"kerasnip","dir":"","previous_headings":"","what":"A Bridge Between Keras and Tidymodels","title":"A Bridge Between Keras and Tidymodels","text":"goal kerasnip provide seamless bridge keras tidymodels ecosystems. allows dynamic creation parsnip model specifications Keras models, making fully compatible tidymodels workflows.","code":""},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Bridge Between Keras and Tidymodels","text":"can install development version kerasnip GitHub :","code":"# install.packages(\"pak\") pak::pak(\"davidrsch/kerasnip\")"},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-1-building-a-sequential-mlp","dir":"","previous_headings":"Example","what":"Example 1: Building a Sequential MLP","title":"A Bridge Between Keras and Tidymodels","text":"example shows core workflow building simple, linear stack layers using create_keras_sequential_spec().","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks # The first block initializes the model. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # Subsequent blocks add layers. dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } # The final block creates the output layer. output_block <- function(model) {   model |>     layer_dense(units = 1) }  # 2. Create a spec from the layer blocks # This creates a new model function, `basic_mlp()`, in your environment. create_keras_sequential_spec(   model_name = \"basic_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     output = output_block   ),   mode = \"regression\" )  # 3. Use the generated spec to define a model. # We can set the number of dense layers (`num_dense`) and their parameters (`dense_units`). spec <- basic_mlp(   num_dense = 2,   dense_units = 64,   fit_epochs = 10,   learn_rate = 0.01 ) |>   set_engine(\"keras\")  # 4. Fit the model within a tidymodels workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf <- workflow(rec, spec)  set.seed(123) fit_obj <- fit(wf, data = mtcars)  # 5. Make predictions predict(fit_obj, new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  21.3 #> 2  21.3 #> 3  22.8 #> 4  21.4 #> 5  18.7"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-2-building-a-functional-fork-join-model","dir":"","previous_headings":"Example","what":"Example 2: Building a Functional “Fork-Join” Model","title":"A Bridge Between Keras and Tidymodels","text":"complex, non-linear architectures, use create_keras_functional_spec(). example builds model input forked two paths, concatenated.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define blocks. For the functional API, blocks are nodes in a graph. input_block <- function(input_shape) layer_input(shape = input_shape) path_block <- function(tensor, units = 16) tensor |> layer_dense(units = units) concat_block <- function(input_a, input_b) layer_concatenate(list(input_a, input_b)) output_block <- function(tensor) layer_dense(tensor, units = 1)  # 2. Create the spec. The graph is defined by block names and their arguments. create_keras_functional_spec(   model_name = \"forked_mlp\",   layer_blocks = list(     main_input = input_block,     path_a = inp_spec(path_block, \"main_input\"),     path_b = inp_spec(path_block, \"main_input\"),     concatenated = inp_spec(concat_block, c(path_a = \"input_a\", path_b = \"input_b\")),     # The output block must be named 'output'.     output = inp_spec(output_block, \"concatenated\")   ),   mode = \"regression\" )  # 3. Use the new spec. Arguments are prefixed with their block name. spec <- forked_mlp(path_a_units = 16, path_b_units = 8, fit_epochs = 10) |>   set_engine(\"keras\")  # Fit and predict as usual set.seed(123) fit(spec, mpg ~ ., data = mtcars) |>   predict(new_data = mtcars[1:5, ]) #> # A tibble: 5 × 1 #>   .pred #>   <dbl> #> 1  19.4 #> 2  19.5 #> 3  21.9 #> 4  18.6 #> 5  17.9"},{"path":"https://davidrsch.github.io/kerasnip/index.html","id":"example-3-tuning-a-sequential-mlp-architecture","dir":"","previous_headings":"Example","what":"Example 3: Tuning a Sequential MLP Architecture","title":"A Bridge Between Keras and Tidymodels","text":"example demonstrates tune number dense layers rate final dropout layer, showcasing tune architecture block hyperparameters simultaneously.","code":"library(kerasnip) library(tidymodels) library(keras3)  # 1. Define Keras layer blocks for a tunable MLP input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } dense_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") } dropout_block <- function(model, rate = 0.2) {   model |> layer_dropout(rate = rate) } output_block <- function(model) {   model |> layer_dense(units = 1) }  # 2. Create a spec from the layer blocks create_keras_sequential_spec(   model_name = \"tunable_mlp\",   layer_blocks = list(     input = input_block,     dense = dense_block,     dropout = dropout_block,     output = output_block   ),   mode = \"regression\" )  # 3. Define a tunable model specification tune_spec <- tunable_mlp(   num_dense = tune(),   dense_units = tune(),   num_dropout = 1,   dropout_rate = tune(),   fit_epochs = 10 ) |>   set_engine(\"keras\")  # 4. Set up and run a tuning workflow rec <- recipe(mpg ~ ., data = mtcars) |>   step_normalize(all_numeric_predictors())  wf_tune <- workflow(rec, tune_spec)  # Define the tuning grid. params <- extract_parameter_set_dials(wf_tune) |>   update(     num_dense = dials::num_terms(c(1, 3)),     dense_units = dials::hidden_units(c(8, 64)),     dropout_rate = dials::dropout(c(0.1, 0.5))   ) grid <- grid_regular(params, levels = 2)  # 5. Run the tuning set.seed(456) folds <- vfold_cv(mtcars, v = 3)  tune_res <- tune_grid(   wf_tune,   resamples = folds,   grid = grid,   control = control_grid(verbose = FALSE) )  # 6. Show the best architecture show_best(tune_res, metric = \"rmse\") #> # A tibble: 5 × 7 #>   num_dense dense_units dropout_rate .metric .estimator .mean .config               #>       <int>       <int>        <dbl> <chr>   <chr>      <dbl> <chr>                 #> 1         1          64          0.1 rmse    standard    2.92 Preprocessor1_Model02 #> 2         1          64          0.5 rmse    standard    3.02 Preprocessor1_Model08 #> 3         3          64          0.1 rmse    standard    3.15 Preprocessor1_Model04 #> 4         1           8          0.1 rmse    standard    3.20 Preprocessor1_Model01 #> 5         3           8          0.1 rmse    standard    3.22 Preprocessor1_Model03"},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"Pre-compiles Keras models hyperparameter combination grid. function powerful debugging tool use running full tune::tune_grid(). allows quickly validate multiple model architectures, ensuring can successfully built compiled without time-consuming process actually fitting . helps catch common errors like incompatible layer shapes invalid argument values early.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"","code":"compile_keras_grid(spec, grid, x, y)"},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"spec parsnip model specification created create_keras_sequential_spec() create_keras_functional_spec(). grid tibble data.frame containing grid hyperparameters evaluate. row represents unique model architecture compiled. x data frame matrix predictors. used infer input_shape Keras model. y vector factor outcomes. used infer output shape default loss function Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"tibble following columns: Columns input grid. compiled_model: list-column containing compiled Keras model objects. compilation failed, element NULL. error: list-column containing NA successes character string error message failures.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"Compile Validate Keras Model Architectures function iterates row provided grid. hyperparameter combination, attempts build compile Keras model defined spec. process wrapped try-catch block gracefully handle report errors occur model instantiation compilation. output tibble mirrors input grid, additional columns containing compiled model object error message, making easy inspect architectures valid.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/compile_keras_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compile Keras Models Over a Grid of Hyperparameters — compile_keras_grid","text":"","code":"if (FALSE) { # \\dontrun{ if (keras::is_keras_available()) {  # 1. Define a kerasnip model specification create_keras_sequential_spec(   model_name = \"my_mlp\",   layer_blocks = list(     input_block,     hidden_block,     output_block   ),   mode = \"classification\" )  mlp_spec <- my_mlp(   hidden_units = tune(),   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\" )  # 2. Create a hyperparameter grid # Include an invalid value (-10) to demonstrate error handling param_grid <- tibble::tibble(   hidden_units = c(32, 64, -10) )  # 3. Prepare dummy data x_train <- matrix(rnorm(100 * 10), ncol = 10) y_train <- factor(sample(0:1, 100, replace = TRUE))  # 4. Compile models over the grid compiled_grid <- compile_keras_grid(   spec = mlp_spec,   grid = param_grid,   x = x_train,   y = y_train )  print(compiled_grid)  # 5. Inspect the results # The row with `hidden_units = -10` will show an error. } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Functional API. allows creating complex, tunable architectures non-linear topologies integrate seamlessly tidymodels ecosystem.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"create_keras_functional_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"model_name character string name new model specification function (e.g., \"custom_resnet\"). valid R function name. layer_blocks named list functions function defines \"block\" (node) model graph. list names crucial define names nodes. arguments function define nodes connected. See \"Model Graph Connectivity\" section details. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., custom_resnet()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Functional API. ideal models complex, non-linear topologies, networks multiple inputs/outputs residual connections. function inspects arguments layer_blocks functions makes available tunable parameters generated model specification, prefixed block's name (e.g., dense_units). Common training parameters epochs learn_rate also added.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"model-graph-connectivity","dir":"Reference","previous_headings":"","what":"Model Graph Connectivity","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"kerasnip builds model's directed acyclic graph inspecting arguments function layer_blocks list. connection logic follows: names elements layer_blocks list define names nodes graph (e.g., main_input, dense_path, output). names arguments block function specify inputs. block function like my_block <- function(input_a, input_b, ...) declares needs input nodes named input_a input_b. kerasnip automatically supply output tensors nodes calling my_block. two special requirements: Input Block: first block list treated input node. function take blocks input, can input_shape argument, supplied automatically fitting. Output Block: Exactly one block must named \"output\". tensor returned block used final output Keras model. key feature automatic creation num_{block_name} arguments (e.g., num_dense_path). allows control many times block repeated, making easy tune depth network. block can repeated exactly one input another block graph. new model specification function update() method created environment specified env argument.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_functional_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Functional API Model Specification for Tidymodels — create_keras_functional_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   library(keras3)   library(parsnip)    # 1. Define block functions. These are the building blocks of our model.   # An input block that receives the data's shape automatically.   input_block <- function(input_shape) layer_input(shape = input_shape)    # A dense block with a tunable `units` parameter.   dense_block <- function(tensor, units) {     tensor |> layer_dense(units = units, activation = \"relu\")   }    # A block that adds two tensors together (for the residual connection).   add_block <- function(input_a, input_b) layer_add(list(input_a, input_b))    # An output block for regression.   output_block_reg <- function(tensor) layer_dense(tensor, units = 1)    # 2. Create the spec. The `layer_blocks` list defines the graph.   create_keras_functional_spec(     model_name = \"my_resnet_spec\",     layer_blocks = list(       # The names of list elements are the node names.       main_input = input_block,        # The argument `main_input` connects this block to the input node.       dense_path = function(main_input, units = 32) dense_block(main_input, units),        # This block's arguments connect it to the original input AND the dense layer.       add_residual = function(main_input, dense_path) add_block(main_input, dense_path),        # This block must be named 'output'. It connects to the residual add layer.       output = function(add_residual) output_block_reg(add_residual)     ),     mode = \"regression\"   )    # 3. Use the newly created specification function!   # The `dense_path_units` argument was created automatically.   model_spec <- my_resnet_spec(dense_path_units = 64, epochs = 10)    # You could also tune the number of dense layers since it has a single input:   # model_spec <- my_resnet_spec(num_dense_path = 2, dense_path_units = 32)    print(model_spec)   # tune::tunable(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function acts factory generate new parsnip model specification based user-defined blocks Keras layers using Sequential API. ideal choice creating models simple, linear stack layers. models complex, non-linear topologies, see create_keras_functional_spec().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"create_keras_sequential_spec(   model_name,   layer_blocks,   mode = c(\"regression\", \"classification\"),   ...,   env = parent.frame() )"},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"model_name character string name new model specification function (e.g., \"custom_cnn\"). valid R function name. layer_blocks named, ordered list functions. function defines \"block\" Keras layers. function must take Keras model object first argument return modified model. arguments function become tunable parameters final model specification. mode character string, either \"regression\" \"classification\". ... Reserved future use. Currently used. env environment create new model specification function associated update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"Invisibly returns NULL. primary side effect create new model specification function (e.g., my_mlp()) specified environment register model parsnip can used within tidymodels framework.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"function generates boilerplate needed create custom, tunable parsnip model specification uses Keras Sequential API. function inspects arguments layer_blocks functions (ignoring special arguments like input_shape num_classes) makes available arguments generated model specification, prefixed block's name (e.g., dense_units). new model specification function update() method created environment specified env argument.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"model-architecture-sequential-api-","dir":"Reference","previous_headings":"","what":"Model Architecture (Sequential API)","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"kerasnip builds model applying functions layer_blocks order provided. function receives Keras model built previous function returns modified version. first block must initialize model (e.g., keras_model_sequential()). can accept input_shape argument, kerasnip provide automatically fitting. Subsequent blocks add layers model. final block add output layer. classification, can accept num_classes argument, provided automatically. key feature function automatic creation num_{block_name} arguments (e.g., num_hidden). allows control many times block repeated, making easy tune depth network.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/create_keras_sequential_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Custom Keras Sequential Model Specification for Tidymodels — create_keras_sequential_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) { library(keras3) library(parsnip) library(dials)  # 1. Define layer blocks for a complete model. # The first block must initialize the model. `input_shape` is passed automatically. input_block <- function(model, input_shape) {   keras_model_sequential(input_shape = input_shape) } # A block for hidden layers. `units` will become a tunable parameter. hidden_block <- function(model, units = 32) {   model |> layer_dense(units = units, activation = \"relu\") }  # The output block. `num_classes` is passed automatically for classification. output_block <- function(model, num_classes) {   model |> layer_dense(units = num_classes, activation = \"softmax\") }  # 2. Create the spec, providing blocks in the correct order. create_keras_sequential_spec( model_name = \"my_mlp\",   layer_blocks = list(     input = input_block,     hidden = hidden_block,     output = output_block   ),   mode = \"classification\" )  # 3. Use the newly created specification function! # Note the new arguments `num_hidden` and `hidden_units`. model_spec <- my_mlp(   num_hidden = 2,   hidden_units = 64,   epochs = 10,   learn_rate = 0.01 )  print(model_spec) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Training History — extract_keras_history","title":"Extract Keras Training History — extract_keras_history","text":"Extracts returns training history parsnip model_fit object created kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Training History — extract_keras_history","text":"","code":"extract_keras_history(object)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Training History — extract_keras_history","text":"object model_fit object produced kerasnip specification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Training History — extract_keras_history","text":"keras_training_history object. can call plot() object visualize learning curves.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Keras Training History — extract_keras_history","text":"Extract Keras Training History history object contains metrics recorded model training, loss accuracy, epoch. highly useful visualizing training process diagnosing issues like overfitting. returned object can plotted directly.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"Extracts returns underlying Keras model object parsnip model_fit object created kerasnip.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"","code":"extract_keras_model(object)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"object model_fit object produced kerasnip specification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"raw Keras model object (keras_model).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_keras_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Keras Model from a Fitted Kerasnip Object — extract_keras_model","text":"Extract Raw Keras Model Kerasnip Fit useful need work directly Keras model object tasks like inspecting layer weights, creating custom plots, passing Keras-specific functions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Valid Grid from Compilation Results — extract_valid_grid","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"helper function filters results compile_keras_grid() return new hyperparameter grid containing combinations compiled successfully.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"","code":"extract_valid_grid(compiled_grid)"},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"compiled_grid tibble, result call compile_keras_grid().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"tibble containing subset original grid resulted successful model compilation. compiled_model error columns removed, leaving clean grid ready tuning.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"Filter Grid Valid Hyperparameter Sets running compile_keras_grid(), can use function remove problematic hyperparameter combinations proceeding full tune::tune_grid().","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/extract_valid_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Valid Grid from Compilation Results — extract_valid_grid","text":"","code":"if (FALSE) { # \\dontrun{ # Continuing the example from `compile_keras_grid`:  # `compiled_grid` contains one row with an error. valid_grid <- extract_valid_grid(compiled_grid)  # `valid_grid` now only contains the rows that compiled successfully. print(valid_grid)  # This clean grid can now be passed to tune::tune_grid(). } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Fitting Engine for Functional API Models — generic_functional_fit","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"function serves internal engine fitting kerasnip models based Keras functional API. intended called directly user. function invoked parsnip::fit() kerasnip functional model specification used.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"","code":"generic_functional_fit(formula, data, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"formula formula specifying predictor outcome variables, passed parsnip::fit() call. data data frame containing training data, passed parsnip::fit() call. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"Generic Fitting Function Functional Keras Models function orchestrates three main steps model fitting process: Build Compile: calls build_and_compile_functional_model() construct Keras model architecture based provided layer_blocks hyperparameters. Process Data: preprocesses input (x) output (y) data format expected Keras. Fit Model: calls keras3::fit() compiled model processed data, passing along fitting-specific arguments (e.g., epochs, batch_size, callbacks).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_functional_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal Fitting Engine for Functional API Models — generic_functional_fit","text":"","code":"# This function is not called directly by users. # It is called internally by `parsnip::fit()`. # For example: if (FALSE) { # \\dontrun{ # create_keras_functional_spec(...) defines my_functional_model  spec <- my_functional_model(hidden_units = 128, fit_epochs = 10) |>   set_engine(\"keras\")  # This call to fit() would invoke generic_functional_fit() internally fitted_model <- fit(spec, y ~ x, data = training_data) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"function serves internal engine fitting kerasnip models based Keras sequential API. intended called directly user. function invoked parsnip::fit() kerasnip sequential model specification used.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"","code":"generic_sequential_fit(formula, data, layer_blocks, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"formula formula specifying predictor outcome variables, passed parsnip::fit() call. data data frame containing training data, passed parsnip::fit() call. layer_blocks named list layer block functions. passed internally parsnip model specification. ... Additional arguments passed model specification. can include: Layer Parameters: Arguments layer blocks, prefixed block name (e.g., dense_units = 64). Architecture Parameters: Arguments control number times block repeated, format num_{block_name} (e.g., num_dense = 2). Compile Parameters: Arguments customize model compilation, prefixed compile_ (e.g., compile_loss = \"mae\", compile_optimizer = \"sgd\"). Fit Parameters: Arguments customize model fitting, prefixed fit_ (e.g., fit_callbacks = list(...), fit_class_weight = list(...)).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"list containing fitted model metadata. list stored fit slot parsnip model fit object. list contains following elements: fit: raw, fitted Keras model object. history: Keras training history object. lvl: character vector outcome factor levels (classification) NULL (regression).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"Generic Fitting Function Sequential Keras Models function orchestrates three main steps model fitting process: Build Compile: calls build_and_compile_sequential_model() construct Keras model architecture based provided layer_blocks hyperparameters. Process Data: preprocesses input (x) output (y) data format expected Keras. Fit Model: calls keras3::fit() compiled model processed data, passing along fitting-specific arguments (e.g., epochs, batch_size, callbacks).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/generic_sequential_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Internal Fitting Engine for Sequential API Models — generic_sequential_fit","text":"","code":"# This function is not called directly by users. # It is called internally by `parsnip::fit()`. # For example: if (FALSE) { # \\dontrun{ # create_keras_sequential_spec(...) defines my_sequential_model  spec <- my_sequential_model(hidden_1_units = 128, fit_epochs = 10) |>   set_engine(\"keras\")  # This call to fit() would invoke generic_sequential_fit() internally fitted_model <- fit(spec, y ~ x, data = training_data) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":null,"dir":"Reference","previous_headings":"","what":"Inform About Compilation Errors — inform_errors","title":"Inform About Compilation Errors — inform_errors","text":"helper function inspects results compile_keras_grid() prints formatted, easy--read summary compilation errors occurred.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inform About Compilation Errors — inform_errors","text":"","code":"inform_errors(compiled_grid, n = 10)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inform About Compilation Errors — inform_errors","text":"compiled_grid tibble, result call compile_keras_grid(). n single integer maximum number distinct errors display detail.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inform About Compilation Errors — inform_errors","text":"Invisibly returns input compiled_grid. Called side effect printing summary console.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inform About Compilation Errors — inform_errors","text":"Display Summary Compilation Errors useful interactive debugging complex tuning grids hyperparameter combinations may lead invalid Keras models.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inform_errors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inform About Compilation Errors — inform_errors","text":"","code":"if (FALSE) { # \\dontrun{ # Continuing the example from `compile_keras_grid`:  # `compiled_grid` contains one row with an error. # This will print a formatted summary of that error. inform_errors(compiled_grid) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remap Layer Block Arguments for Model Specification — inp_spec","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"Creates wrapper function around Keras layer block rename arguments. powerful helper defining layer_blocks create_keras_functional_spec() create_keras_sequential_spec(), allowing connect reusable blocks model graph without writing verbose anonymous functions.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"inp_spec(block, input_map)"},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"block function defines Keras layer set layers. first arguments input tensor(s). input_map single character string named character vector specifies rename/remap arguments block.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"new function (closure) wraps block function renamed arguments, ready used layer_blocks list.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"inp_spec() makes model definitions cleaner readable. handles metaprogramming required create new function correct argument names, preserving original block's hyperparameters default values. function supports two modes operation based input_map: Single Input Renaming: input_map single character string, wrapper function renames first argument block function provided string. common case blocks take single tensor input. Multiple Input Mapping: input_map named character vector, provides explicit mapping new argument names (names vector) original argument names block function (values vector). used blocks multiple inputs, like concatenation layer.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/inp_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remap Layer Block Arguments for Model Specification — inp_spec","text":"","code":"if (FALSE) { # \\dontrun{ # --- Example Blocks --- # A standard dense block with one input tensor and one hyperparameter. dense_block <- function(tensor, units = 16) {   tensor |> keras3::layer_dense(units = units, activation = \"relu\") }  # A block that takes two tensors as input. concat_block <- function(input_a, input_b) {   keras3::layer_concatenate(list(input_a, input_b)) }  # An output block with one input. output_block <- function(tensor) {   tensor |> keras3::layer_dense(units = 1) }  # --- Usage --- layer_blocks <- list(   main_input = keras3::layer_input,   path_a = inp_spec(dense_block, \"main_input\"),   path_b = inp_spec(dense_block, \"main_input\"),   concatenated = inp_spec(     concat_block,     c(path_a = \"input_a\", path_b = \"input_b\")   ),   output = inp_spec(output_block, \"concatenated\") ) } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate a Kerasnip Model — keras_evaluate","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"function provides kera_evaluate() method model_fit objects created kerasnip. preprocesses new data format expected Keras calls keras3::evaluate() underlying model compute loss metrics.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"","code":"keras_evaluate(object, x, y = NULL, ...)"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"object model_fit object produced kerasnip specification. x data frame matrix new predictor data. y vector data frame new outcome data corresponding x. ... Additional arguments passed keras3::evaluate() (e.g., batch_size).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"named list containing evaluation results (e.g., loss, accuracy). names determined metrics model compiled .","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"Evaluate Fitted Kerasnip Model New Data","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_evaluate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate a Kerasnip Model — keras_evaluate","text":"","code":"if (FALSE) { # \\dontrun{ if (keras::is_keras_available()) {  # 1. Define and fit a model ---- create_keras_sequential_spec(   model_name = \"my_mlp\",   layer_blocks = list(input_block, hidden_block, output_block),   mode = \"classification\" )  mlp_spec <- my_mlp(   hidden_units = 32,   compile_loss = \"categorical_crossentropy\",   compile_optimizer = \"adam\",   compile_metrics = \"accuracy\",   fit_epochs = 5 ) |> set_engine(\"keras\")  x_train <- matrix(rnorm(100 * 10), ncol = 10) y_train <- factor(sample(0:1, 100, replace = TRUE)) train_df <- data.frame(x = I(x_train), y = y_train)  fitted_mlp <- fit(mlp_spec, y ~ x, data = train_df)  # 2. Evaluate the model on new data ---- x_test <- matrix(rnorm(50 * 10), ncol = 10) y_test <- factor(sample(0:1, 50, replace = TRUE))  eval_metrics <- keras_evaluate(fitted_mlp, x_test, y_test) print(eval_metrics)  # 3. Extract the Keras model object ---- keras_model <- extract_keras_model(fitted_mlp) summary(keras_model)  # 4. Extract the training history ---- history <- extract_keras_history(fitted_mlp) plot(history) } } # }"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":null,"dir":"Reference","previous_headings":"","what":"Dynamically Discovered Keras Objects — keras_objects","title":"Dynamically Discovered Keras Objects — keras_objects","text":"exported vectors contain names optimizers, losses, metrics discovered installed keras3 package kerasnip loaded. ensures kerasnip always --date Keras version.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dynamically Discovered Keras Objects — keras_objects","text":"","code":"keras_optimizers  keras_losses  keras_metrics"},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dynamically Discovered Keras Objects — keras_objects","text":"object class character length 12. object class character length 21. object class character length 32.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/keras_objects.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dynamically Discovered Keras Objects — keras_objects","text":"objects primarily used provide default values dials parameter functions, optimizer_function() loss_function_keras(). allows tab-completion IDEs validation optimizer loss names tuning models. discovery process .onLoad() scrapes keras3 namespace functions matching optimizer_*, loss_*, metric_* patterns.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Loss Functions — loss_function_keras","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"Dials Parameter Keras Loss Functions","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"","code":"loss_function_keras(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/loss_function_keras.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Loss Functions — loss_function_keras","text":"values character vector possible loss functions. Defaults known losses (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Dials Parameter for Keras Optimizers — optimizer_function","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"Dials Parameter Keras Optimizers","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"","code":"optimizer_function(values = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/optimizer_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dials Parameter for Keras Optimizers — optimizer_function","text":"values character vector possible optimizers. Defaults known optimizers (keras defaults + custom registered).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Predictor Input for Keras — process_x","title":"Process Predictor Input for Keras — process_x","text":"Preprocesses predictor data (x) format suitable Keras models. Handles tabular data list-columns arrays (e.g., images).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Predictor Input for Keras — process_x","text":"","code":"process_x(x)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Predictor Input for Keras — process_x","text":"x data frame matrix predictors.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_x.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Predictor Input for Keras — process_x","text":"list containing: x_proc: processed predictor data (matrix array). input_shape: determined input shape Keras model.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Outcome Input for Keras — process_y","title":"Process Outcome Input for Keras — process_y","text":"Preprocesses outcome data (y) format suitable Keras models. Handles regression (numeric) classification (factor) outcomes, including one-hot encoding classification.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Outcome Input for Keras — process_y","text":"","code":"process_y(y, is_classification = NULL, class_levels = NULL)"},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Outcome Input for Keras — process_y","text":"y vector outcomes. is_classification Logical, optional. TRUE, treats y classification. FALSE, treats regression. NULL (default), determined .factor(y). class_levels Character vector, optional. factor levels classification outcomes. NULL (default), determined levels(y).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/process_y.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Outcome Input for Keras — process_y","text":"list containing: y_proc: processed outcome data (matrix one-hot encoded array). is_classification: Logical, indicating y treated classification. num_classes: Integer, number classes classification, NULL. class_levels: Character vector, factor levels classification, NULL.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Loss — register_keras_loss","title":"Register a Custom Keras Loss — register_keras_loss","text":"Allows users register custom loss function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Loss — register_keras_loss","text":"","code":"register_keras_loss(name, loss_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Loss — register_keras_loss","text":"name name register loss (character). loss_fn loss function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Loss — register_keras_loss","text":"Registered losses stored internal environment. model compiled, kerasnip first check internal registry loss matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Metric — register_keras_metric","title":"Register a Custom Keras Metric — register_keras_metric","text":"Allows users register custom metric function can used name within kerasnip model specifications.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Metric — register_keras_metric","text":"","code":"register_keras_metric(name, metric_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Metric — register_keras_metric","text":"name name register metric (character). metric_fn metric function.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_metric.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Metric — register_keras_metric","text":"Registered metrics stored internal environment. model compiled, kerasnip first check internal registry metric matching provided name checking keras3 package.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Custom Keras Optimizer — register_keras_optimizer","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Allows users register custom optimizer function can used name within kerasnip model specifications tuned dials.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"register_keras_optimizer(name, optimizer_fn)"},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"name name register optimizer (character). optimizer_fn optimizer function. return Keras optimizer object.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"Registered optimizers stored internal environment. model compiled, kerasnip first check internal registry optimizer matching provided name checking keras3 package. optimizer_fn can simple function partially applied function using purrr::partial(). useful creating versions Keras optimizers specific settings.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/register_keras_optimizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a Custom Keras Optimizer — register_keras_optimizer","text":"","code":"if (requireNamespace(\"keras3\", quietly = TRUE)) {   # Register a custom version of Adam with a different default beta_1   my_adam <- purrr::partial(keras3::optimizer_adam, beta_1 = 0.8)   register_keras_optimizer(\"my_adam\", my_adam)    # Now \"my_adam\" can be used as a string in a model spec, e.g.,   # my_model_spec(compile_optimizer = \"my_adam\") }"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function completely removes model specification previously created create_keras_sequential_spec() create_keras_functional_spec(). cleans function user's environment associated registrations within parsnip package.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"remove_keras_spec(model_name, env = parent.frame())"},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"model_name character string giving name model specification function remove (e.g., \"my_mlp\"). env environment remove function update() method. Defaults calling environment (parent.frame()).","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"Invisibly returns TRUE attempting remove objects.","code":""},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"function essential cleanly unloading dynamically created model. performs three main actions: removes model specification function (e.g., my_mlp()) corresponding update() method specified environment. searches parsnip's internal model environment objects whose names start model_name removes . purges fit methods, argument definitions, registrations. removes model's name parsnip's master list models. function uses un-exported parsnip:::get_model_env() perform cleanup, may subject change future parsnip versions.","code":""},{"path":[]},{"path":"https://davidrsch.github.io/kerasnip/reference/remove_keras_spec.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove a Keras Model Specification and its Registrations — remove_keras_spec","text":"","code":"if (FALSE) { # \\dontrun{ if (requireNamespace(\"keras3\", quietly = TRUE)) {   # First, create a dummy spec   input_block <- function(model, input_shape) keras3::keras_model_sequential(input_shape = input_shape)   dense_block <- function(model, units = 16) model |> keras3::layer_dense(units = units)   create_keras_sequential_spec(\"my_temp_model\", list(input = input_block, dense = dense_block), \"regression\")    # Check it exists in the environment and in parsnip   exists(\"my_temp_model\")   \"my_temp_model\" %in% parsnip::show_engines(\"my_temp_model\")$model    # Now remove it   remove_keras_spec(\"my_temp_model\")    # Check it's gone   !exists(\"my_temp_model\")   !\"my_temp_model\" %in% parsnip::show_engines(NULL)$model } } # }"},{"path":"https://davidrsch.github.io/kerasnip/news/index.html","id":"kerasnip-0009000","dir":"Changelog","previous_headings":"","what":"kerasnip 0.0.0.9000","title":"kerasnip 0.0.0.9000","text":"Initial development version. Added create_keras_spec() generate parsnip specifications dynamically.","code":""}]
